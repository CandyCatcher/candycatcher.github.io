{"meta":{"title":"杨枝甘露’s Blog","subtitle":"","description":"","author":"杨枝甘露","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"2021年面试总结的面试题","slug":"2021年面试总结的面试题","date":"2021-09-12T15:22:00.000Z","updated":"2021-09-12T15:47:18.080Z","comments":true,"path":"2021/09/12/2021年面试总结的面试题/","link":"","permalink":"http://example.com/2021/09/12/2021%E5%B9%B4%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%E7%9A%84%E9%9D%A2%E8%AF%95%E9%A2%98/","excerpt":"记录一下2021年面试时遇到的以及从网上记录的面试题。","text":"记录一下2021年面试时遇到的以及从网上记录的面试题。 Java基础基本数据类型的操作 Java虚拟机规范定义的许多规则中的一条：所有对基本类型的操作，除了某些对long类型和double类型的操作之外，都是原子级的。 目前的JVM（java虚拟机）都是将32位作为原子操作，并非64位。当线程把主存中的 long/double类型的值读到线程内存中时，可能是两次32位值的写操作，显而易见，如果几个线程同时操作，那么就可能会出现高低2个32位值出错的情况发生。 要在线程间共享long与double字段是，必须在synchronized中操作，或是声明为volatile。 NIO的类库或框架 了解反射吗？讲了反射和自定义注解 JAVA反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个属性和方法；这种动态获取对象的信息以及动态调用对象的方法的功能称为 java语言的反射机制。 Java在编译之后会生成一个class文件，反射通过字节码文件找到其类中的方法和属性等。Class对象包含类的所有信息可以通过该对象获取到构造方法，成员变量，成员方法和接口等信息。 如何通过Class获取Field/Method/Construcor Class类中包含一个ReflectionData私有类，保存了进行反射操作的基础信息。第一处是从reflectionData直接取，reflectionData是弱引用，这算是一种缓存获取；第二处是直接调用getDeclaredFields0()这个方法获取，这是一个native方法，应当是从JVM内直接获取。reflectionData对象是SoftReference类型的，说明在内存紧张时可能会被回收，不过也可以通过-XX:SoftRefLRUPolicyMSPerMB参数控制回收的时机。 什么是面向切面编程 在编译期、类加载期、运行时，动态地将代码切入到类的指定方法、指定位置上的编程思想就是面向切面的编程。一般是使用代理模式来实现AOP。 代理分为静态代理和动态代理，静态代理，顾名思义，就是你自己写代理对象，动态代理，则是在运行期，生成一个代理对象。Spring AOP就是基于动态代理的。不是所有AOP的实现都是在运行时进行织入的，因为这样效率太低了，而且只能针对方法进行AOP，无法针对构造函数、字段进行AOP。我完全可以在编译成class时就织入啊，比如AspectJ，当然AspectJ还提供了后编译器织入和类加载期织入。 编译：*.java源码文件转为 *.class二进制字节码文件的过程 类加载：将类加载进JVM运行数据区的方法区 反射可以访问类的私有成员吗？可以。setVisiable(true); 重载和重写final，static，private Java中的包装类有什么用，为什么需要包装类 在某些场合不能使用基本类型必须使用包装类，比如集合能接收的类型为Object，基本类型是无法添加进去的，还有范型也必须使用包装类。或者说，如果用基本类型表示的话，默认值为零，如果我想表示为空就没法用了，因为值类型是无法赋空值的，如果使用包装类型就可以了，因为Integer的默认值为空。 包装类型提供了一些方法，比如可以使用 Integer.parseInt() 的方法将String型的数据转换为整型 JUC包下的常见类/Java并发包下的原子工具类，能说一下么 JUC的atomic包下运用了CAS的AtomicBoolean、AtomicInteger、AtomicReference等原子变量类 JUC的locks包下的AbstractQueuedSynchronizer（AQS）以及使用AQS的ReentantLock（显式锁）、ReentrantReadWriteLock JUC下的一些同步工具类：CountDownLatch（闭锁）、Semaphore（信号量）、CyclicBarrier（栅栏）、FutureTask JUC下的一些并发容器类：ConcurrentHashMap、CopyOnWriteArrayList JUC下的一些Executor框架的相关类： 线程池的工厂类-&gt;Executors 线程池的实现类-&gt;ThreadPoolExecutor/ForkJoinPool JUC下的一些阻塞队列实现类：ArrayBlockingQueue、LinkedBlockingQueue、PriorityBlockingQueue 标量类（Scalar）：AtomicBoolean，AtomicInteger，AtomicLong，AtomicReference 数组类：AtomicIntegerArray，AtomicLongArray，AtomicReferenceArray 更新器类：AtomicLongFieldUpdater，AtomicIntegerFieldUpdater，AtomicReferenceFieldUpdater 复合变量类：AtomicMarkableReference，AtomicStampedReference final是干嘛的，用在什么地方，项目中有用到过吗常量类 happen-before的规则 如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。 两个操作之间存在happens-before关系，并不意味着一定要按照happens-before原则制定的顺序来执行。如果重排序之后的执行结果与按照happens-before关系来执行的结果一致，那么这种重排序并不非法。 下面是happens-before原则规则： 程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作； 锁定规则：一个unLock操作先行发生于后面对同一个锁额lock操作； volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作； 传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C； 线程启动规则：Thread对象的**start()**方法先行发生于此线程的每个一个动作； 线程中断规则：对线程**interrupt()**方法的调用先行发生于被中断线程的代码检测到中断事件的发生； 线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行； 对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始； 数据库数据库事务并发问题 多个事务同时访问数据库时候，会发生下列5类问题，包括3类数据读问题（脏读，不可重复读，幻读），2类数据更新问题（第一类丢失更新，第二类丢失更新）： 3类数据读问题: 脏读（dirty read） A事务读取B事务尚未提交的更改数据，并在这个数据基础上操作。如果B事务回滚，那么A事务读到的数据根本不是合法的，称为脏读。在oracle中，由于有version控制，不会出现脏读。 不可重复读（unrepeatable read） A事务读取了B事务已经提交的更改（或删除）数据。如果A事务第一次读取数据，然后B事务更改该数据并提交，A事务再次读取数据，两次读取的数据不一样。 幻读（phantom read） A事务读取了B事务已经提交的新增数据。注意和不可重复读的区别，这里是新增，不可重复读是更改（或删除）。这两种情况对策是不一样的，对于不可重复读，只需要采取行级锁防止该记录数据被更改或删除，然而对于幻读必须加表级锁，防止在这个表中新增一条数据。 2类数据更新问题: 第一类丢失更新 A事务撤销时，把已提交的B事务的数据覆盖掉。这是因为没有事务，直接把B事务覆盖掉，回滚到A事务的状态这种错误会造成非常严重的后果。 第二类丢失更新 A事务提交时，把已提交的B事务的数据覆盖掉。这种错误会造成非常严重的后果。 先来看一下第一类丢失更新，A事务撤销时，把已经提交的B事务的更新数据覆盖了。这种错误可能造成很严重的问题，通过下面的账户取款转账就可以看出来：A事务在撤销时，”不小心”将B事务已经转入账户的金额给抹去了。 再来看一下第二类丢失更新，A事务覆盖B事务已经提交的数据，造成B事务所做操作丢失： 上面的例子里由于支票转账事务覆盖了取款事务对存款余额所做的更新，导致银行最后损失了100元，相反如果转账事务先提交，那么用户账户将损失100元。 解决措施：锁机制–4种隔离级别共享锁和排它锁 为了解决并发问题，数据库系统引入锁机制。基本的封锁类型有两种: 排它锁(Exclusive locks 简记为X锁) 和 共享锁(Share locks 简记为S锁)。 排它锁又称为写锁。若事务T对数据对象A加上X锁，则只允许T读取和修改A，其它任何事务都不能再对A加任何类型的锁，直到T释放A上的锁。这就保证了其它事务在T释放A上的锁之前不能再读取和修改A。 SELECT ... FOR UPDATE; 共享锁又称为读锁。若事务T对数据对象A加上S锁，则事务T可以读A但不能修改A，**其它事务只能再对A加S锁，而不能加X锁**，直到T释放A上的S锁。这就保证了其它事务可以读A，但在T释放A上的S锁之前不能对A做任何修改。 SELECT ... LOCK IN SHARE MODE; 事务的隔离级别 Read Uncommitted【读未提交数据】：允许所有事务读取未被其他事务提交的数据修改。会导致脏读、不可重复读和幻读的问题的出现。 Read Committed【读已提交数据】：只允许事务读取已经被其他事务提交的数据修改。Oracle和sql server默认的级别，可以避免脏读，但不可重复读和幻读问题仍然会出现。 Repeatable Read【可重读】：是MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。但幻读问题未解除。 Serializable【可串行化】：最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。 为什么使用rc？ 在RR隔离级别下，存在间隙锁，导致出现死锁的几率比RC大的多。而在RC隔离级别下，不存在间隙锁，其他事务是可以插入数据 在RR隔离级别下，条件列未命中索引会锁表。而在RC隔离级别下，只锁定满足条件的那几行 rc级别可能会出现第二类更新丢失的问题，使用分布式锁来解决这个问题，锁住唯一 ID 事务在提交之前数据存在哪里？或者可以这么问MySQL如何保证redo log和binlog的数据是一致的？ MySQL/InnoDB为例，提交（commit）前当然是在内存中。在事务执行过程中，修改数据时会不断产生redo日志（write ahead log），这些日志会写入redo日志文件。 在MySQL内部，在事务提交时利用两阶段提交(内部XA的两阶段提交)保证binlog和redo log的一致性问题： 首先如果此时SQL已经成功执行(不用undo了)。此阶段InnoDB会写事务的redo log，将Redo Log写入文件，并刷入磁盘，记录上内部XA事务的ID，同时将Redo Log状态设置为Prepare。Redo Log写入成功后，再将Binlog同样刷入磁盘，记录XA事务ID。 接着commit，这个阶段分成两个步骤。第一步写binlog（先调用write()将binlog内存日志数据写入文件系统缓存，再调用fsync()将binlog文件系统缓存日志数据永久写入磁盘）；第二步完成事务的提交（commit），此时在redo log中记录此事务的提交日志，也就是增加commit 标签(并且当事务提交时会调用fsync对redo log进行刷盘；这是默认情况下的策略)。 需要注意的是，在这个过程中是以第二阶段中binlog的写入与否作为事务是否成功提交的标志。 通过上述MySQL内部XA的两阶段提交就可以解决binlog和redo log的一致性问题。数据库在上述任何阶段crash，主从库都不会产生不一致的错误。 此时的崩溃恢复过程如下： 如果数据库在记录此事务的binlog之前和过程中发生crash。数据库在恢复后认为此事务并没有成功提交，则会回滚此事务的操作(没有commit标签)。与此同时，因为在binlog中也没有此事务的记录，所以从库也不会有此事务的数据修改。 如果数据库在记录此事务的binlog之后发生crash。此时，即使是redo log中还没有记录此事务的commit 标签，数据库在恢复后也会认为此事务提交成功。也就是说，binlog中记录的事务，在恢复时都会被认为是已提交事务，会在redo log中重新写入commit标志，并完成此事务的重做（主库中有此事务的数据修改）。与此同时，因为在binlog中已3经有了此事务的记录，所有从库也会有此事务的数据修改。 undolog保存了事务发生之前的数据的一个版本。 数据页加载的三种方式 内存读取。如果该数据存在于内存中，基本上执行时间在 1ms 左右，效率还是很高的。 随机读取。如果数据没有在内存中，就需要在磁盘上对该页进行查找，将页从磁盘服务器缓冲区传输到数据库缓冲区中。整体时间预估在 10ms 左右， 这 10ms 中有 6ms 是磁盘的实际繁忙时间（包括了寻道和半圈旋转时间），有 3ms 是对可能发生的排队时间的估计值，另外还有 1ms 的传输时间 顺序读取。其实是一种批量读取的方式，因为我们请求的数据在磁盘上往往都是相邻存储的，顺序读取可以帮我们批量读取页面，这样的话，一次性加载到缓冲池中就不需要再对其他页面单独进行磁盘 I/O 操作了。如果一个磁盘的吞吐量是 40MB/S，那么对于一个 16KB 大小的页来说，一次可以顺序读取 2560（40MB/16KB）个页，相当于一个页的读取时间为 0.4ms。采用批量读取的方式，即使是从磁盘上进行读取，效率也比从内存中只单独读取一个页的效率要高。 数据库有什么log ？mysql的undolog和redolog的意义 undo log属于逻辑日志，MySQL是修改一行先上行锁，然后写undo记录到undo数据区，然后修改数据页，然后写日志到log buffer，如果这个修改出现异常，则会使用undo日志来实现回滚操作，比如说在一个事务里面有多个操作，在最后一个操作出现异常，那么需要将前面已经成功的操作进行回滚，从而实现事务的一致性。同时可以提供多版本并发控制下的读（MVCC），也就是非锁定读。 redolog实现了事务的持久性。当事务在提交时(说明操作没有出现问题)，必须先将该事务的所有操作日志写到磁盘上的 redo log file进行持久化，这也就是我们常说的 Write Ahead Log 策略。有了redo log，在数据库发生宕机时，即使内存中的数据还没来得及持久化到磁盘上，我们也可以通过redo log完成数据的恢复，这样就避免了数据的丢失。 在MySQL中，binlog记录了数据库系统所有的更新操作，主要是用来实现数据恢复和主从复制的。一方面，主从配置的MySQL集群可以利用binlog将主库中的更新操作传递到从库中，以此来实现主从数据的一致性；另一方面，数据库还可以利用binlog来进行数据的恢复。 事务的实现原理 原子性。原子性是指一个事务是一个不可分割的工作单位，其中的操作要么都做，要么都不做；如果事务中一个sql语句执行失败，则已执行的语句也必须回滚，数据库退回到事务前的状态。undo log是实现原子性的关键，再介绍一下undo log 持久性。事务一旦提交，其所作做的修改会永久保存到数据库中，此时即使系统崩溃修改的数据也不会丢失。Redo log是实现持久性的关键，再介绍下redolog 隔离性。隔离性是指，事务内部的操作与其他事务是隔离的，并发执行的各个事务之间不能互相干扰。通过两方面实现：(一个事务)写操作对(另一个事务)写操作的影响：锁机制保证隔离性；(一个事务)写操作对(另一个事务)读操作的影响：MVCC保证隔离性 一致性。一致性是指事务使得系统从一个一致的状态转换到另一个一致状态。 联合索引的结构 对于联合索引来说只不过比单值索引多了几列，而这些索引列全都出现在索引树上。对于联合索引，存储引擎会首先根据第一个索引列排序；如果第一列相等则再根据第二列排序，依次类推就构成了索引树 索引下推组合索引满足最左匹配原则，但是遇到非等值判断时匹配停止。name like ‘陈%’不是等值判断，后面的age=20就用不上组合索引了。如果没有索引下推，组合索引只能拿到name回表判断条件。用了索引下推，就算遇到非等值判断，也能继续判断条件。 覆盖索引在一棵索引树上就能获取SQL所需的所有数据，不需要回表，速度更快。比如说有组合索引(name，age)。select age from t where name = &#39;XX&#39;; b+树为什么能三层能存2000多万个，计算过程 这里我们先假设 B+ 树高为 2，即存在一个根节点和若干个叶子节点，那么这棵 B+ 树的存放总记录数为：根节点指针数 * 单个叶子节点记录行数。 我们假设主键 ID 为 bigint 类型，长度为 8 字节，而指针大小在 InnoDB 源码中设置为 6 字节，这样一共 14 字节，我们一个页中能存放多少这样的单元，其实就代表有多少指针，即 16384/14=1170。 在 MySQL中 InnoDB 页的大小默认是 16k。假设一行数据的大小是 1k，那么一个页可以存放 16 行这样的数据，也就是单个叶子节点记录行数为16。 所以一棵高度为 2 的 B+ 树，能存放 1170*16=18720 条数据记录。同样一个高度为 3 的 B+ 树可以存放： 1170*1170*16=21902400 条记录。所以在 InnoDB 中 B+ 树高度一般为 1-3 层，它就能满足千万级的数据存储。在查找数据时一次页的查找代表一次 IO，所以通过主键索引查询通常只需要 1-3 次 IO 操作即可查找到数据。 为什么 MySQL 的索引要使用 B+ 树而不是其它树形结构？比如 B 树？ 现在这个问题的复杂版本可以参考上面的问题，简单版本回答是： 因为 B 树不管叶子节点还是非叶子节点，都会保存数据，这样导致在非叶子节点中能保存的指针数量变少（有些资料也称为扇出），指针少的情况下要保存大量数据，只能增加树的高度，导致 IO 操作变多，查询性能变低。 在 InnoDB 的表空间文件中，约定 page number 为 3 的代表主键索引的根页，而在根页偏移量为 64 的地方存放了该 B+ 树的 page level。如果 page level 为 1，树高为 2，page level 为 2，则树高为 3。即 B+ 树的高度 =page level+1； b+树的叶子节点之间是单链还是双链，页与页之间，页内部呢 B+ 树中叶子节点中的数据是通过双向链表连接的。各个页之间也是通过双向链表连接的。叶子节点为，存储了关键字和行记录，在节点内部（也就是页结构的内部）记录之间是一个单向的链表 索引还有啥结构（哈希）为什么不用？ 哈希索引不是按照索引值顺序存储的，无法用于排序 所以哈希索引不支持部分索引匹配查找，因为哈希索引是使用索引列中全部内容来计算哈希值的 哈希索引只支持等值比较查询，不支持范围查询 出现很多哈希冲突时要遍历所有行指针了，并且维护操作的代价也会很高，因为要遍历对应哈希值的链表中的每一行 Innodb引擎有一个特殊的功能——自适应哈希索引。就是有些索引值使用非常频繁的话，那么会在B+树索引之上在创建一个哈希索引。 为什么不用红黑树 B树是多路树，红黑树是二叉树！红黑树一个节点只能存出一个值，B树一个节点可以存储多个值，红黑树的深度会更大,定位时 红黑树的查找次数会大一些。 给你10个数（用b+树整一个效率最高的） 问多少阶（面试官：都可以，只要效率最高） 想了想有坑，（就给面试官算了一下b和b+的放的个数） 数据库中是否会出现死锁？数据库中的死锁避免是否可用刚才说的方法来避免？死锁的四个必要条件？如何避免死锁？如何检测死锁？ 如何避免死锁： 预防死锁发生：通过对死锁产生的四个必要条件进行限制 检测与拆除死锁：这种方式使允许死锁发生，检测死锁产生，然后解除死锁 动态避免：在资源分配过程中，确保资源请求批准后系统不会进入死锁或潜在的死锁状态。如银行家算法。银行家算法就是设法保证系统动态分配资源后不进入不安全状态，以避免可能产生的死锁。即没当进程提出资源请求且系统的资源能够满足该请求时，系统将判断满足此次资源请求后系统状态是否安全，如果判断结果为安全，则给该进程分配资源，否则不分配资源，申请资源的进程将阻塞 如何检测死锁： 有四个参数： E 向量：资源总量 A 向量：资源剩余量 C 矩阵：每个进程所拥有的资源数量，每一行都代表一个进程拥有资源的数量 R 矩阵：每个进程请求的资源数量 具体是这么做的 寻找一个没有标记的进程 Pi，它所请求的资源小于等于 A。 如果找到了这样一个进程，那么将 C 矩阵的第 i 行向量加到 A 中，标记该进程，并转回 1。 如果没有这样一个进程，算法终止。 数据库死锁的避免 保持事务简短并在一个批处理中在同一数据库中并发执行多个需要长时间运行的事务时通常发生死锁。事务运行时间越长，其持有排它锁或更新锁的时间也就越长，从而堵塞了其它活动并可能导致死锁。保持事务在一个批处理中，可以最小化事务的网络通信往返量，减少完成事务可能的延迟并释放锁 使用低隔离级别确定事务是否能在更低的隔离级别上运行。执行提交读允许事务读取另一个事务已读取（未修改）的数据，而不必等待第一个事务完成。使用较低的隔离级别（例如提交读）而不使用较高的隔离级别（例如可串行读）可以缩短持有共享锁的时间，从而降低了锁定争夺（比如这次的S NK和X IK 是InnoDB引擎Repeatable Read级别才有的） 数据库死锁的出现 事务之间对资源访问顺序的交替 一个用户A 访问表A（锁住了表A），然后又访问表B；另一个用户B 访问表B（锁住了表B），然后企图访问表A；这时用户A由于用户B已经锁住表B，它必须等待用户B释放表B才能继续，同样用户B要等用户A释放表A才能继续，这就死锁就产生了。 降低锁的粒度，不要使用表锁。避免大事务，可以拆分成多个小事务，因为大事务耗时长，与其他事务发生的概率就大。调整程序，对数据库进行多表操作时，尽量按照相同的顺序进行处理并同时锁定连个资源。 并发修改同一记录 用户A查询一条纪录，然后修改该条纪录；这时用户B修改该条纪录，这时用户A的事务里锁的性质由查询的共享锁企图上升到独占锁，而用户B里的独占锁由于A有共享锁存在所以必须等A释放掉共享锁，而A由于B的独占锁而无法上升的独占锁也就不可能释放共享锁，于是出现了死锁。 使用乐观锁MVCC。为数据增加一个版本标识。读取出数据时，将此版本号一同读出，之后更新时，对此版本号加一。此时，将提交数据的版本数据与数据库表对应记录的当前版本信息进行比对，如果提交的数据版本号大于数据库表当前版本号，则予以更新，否则认为是过期数据。乐观锁机制避免了长事务中的数据库加锁开销，大大提升了大并发量下的系统整体性能表现。使用悲观锁进行控制。从操作员读出数据、开始修改直至提交修改结果的全过程，数据库记录始终处于加锁状态 通过命令 show engine Innodb status 查看事务日志 如果在事务中执行了一条没有命中索引的update语句，则执行全表扫描，把行级锁上升为表级锁，多个这样的事务执行后，就很容易产生死锁和阻塞。类似的情况还有当表中的数据量非常庞大而索引建的过少或不合适的时候，使得经常发生全表扫描，最终应用系统会越来越慢，最终发生阻塞或死锁。 使用”explain”对SQL语句进行分析，对于有全表扫描的SQL语句，建立相应的索引进行优化。 如果进程一次锁住数据库中的多条记录来避免死锁，会带来什么问题？你觉得应该怎样解 决这个问题？在这我回答了乐观锁，然后回答了乐观锁的实现原理。 如果数据库中的确发生了死锁，应该怎么解决？ 资源剥夺法。剥夺陷于死锁的进程所占用的资源，但并不撤销此进程，直至死锁解除。 进程回退法。根据数据库的检查点让所有的进程回退到足以解除死锁。rollback 进程撤销法。kill陷入死锁的进程，回收其资源并重新分配，直至死锁解除 1：查看当前的事务 1SELECT * FROM INFORMATION_SCHEMA.INNODB_TRX; 2：查看当前锁定的事务 1SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCKS; 3：查看当前等锁的事务 1SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCK_WAITS; 1kill [ trx_mysql_thread_id ] 系统重启法。结束所有进程的执行并重新启动操作系统。这种方法很简单，但先前的工作全部作废，损失很大。重启mysql服务器 添加索引会锁表但是MySQL 在 5.6 版本已经优化了这个问题。实现不锁表增加字段和索引非常简单： 1ALTER TABLE member ADD user_from smallint(1) NOT NULL, ALGORITHM=INPLACE, LOCK=NONE ALGORITHM表示算法：default默认（根据具体操作类型自动选择），inplace（不影响DML），copy创建临时表（锁表。禁止读写，进行rename），INSTANT只修改元数据（8.0新增，在修改名字等极少数情况可用） LOCK表示是否锁表：default默认，none，shared共享锁，exclusive 只要不改主键，就不会锁表。 然后在慢查询过程中，如果修改了索引，那么会进入死锁的状态。 总之，alter table的语句是很危险的(其实他的危险其实是未提交事物或者长事务导致的)，在操作之前最好确认对要操作的表没有任何进行中的操作、没有未提交事务、也没有显式事务中的报错语句。如果有alter table的维护任务，在无人监管的时候运行，最好通过lock_wait_timeout设置好超时时间，避免长时间的metedata锁等待。 heap表 HEAP表是访问数据速度最快的MySQL表，他使用保存在内存中的散列索引。但如果MySQL或者服务器重新启动，表中数据将会丢失，但是结构不会丢失。创建内存表非常的简单，只需注明 ENGINE= MEMORY 即可:CREATE TABLE tablename(columnName varchar(256) NOT NUL) ENGINE=MEMORY DEFAULT CHARSET=latin1 MAX_ROWS=100000000;注意： 当内存表中的数据大于max_heap_table_size设定的容量大小时，mysql会转换超出的数据存储到磁盘上，因此这是性能就大打折扣了，所 以我们还需要根据我们的实际情况调整max_heap_table_size，另外在建表语句中还可以通过MAX_ROWS来控制表的记录数 int 存到数据库里面一般你都用什么类型 bigint 从 -2^63 (-9223372036854775808) 到 2^63-1 (9223372036854775807) 的整型数字 int 从 -2^31 (-2,147,483,648) 到 2^31 – 1 (2,147,483,647) 的整型数字 其实当我们在选择使用int的类型的时候，不论是int(3)还是int(11)，它在数据库里面存储的都是4个字节的长度，在使用int(3)的时候如果你输入的是10，会默认给你存储位010,也就是说这个3代表的是默认的一个长度，当你不足3位时，会帮你不全，当你超过3位时，就没有任何的影响。 varchar（35）的含义；中文在varchar中占几个字符；编码有哪些4.0版本以下，varchar(35)，指的是35字节，如果存放UTF8汉字时，只能存12个（每个汉字3字节） 5.0版本以上，varchar(35)，指的是35字符，无论存放的是数字、字母还是UTF8汉字（每个汉字3字节），都可以存放35个。 UTF8编码中一个汉字（包括数字）占用3个字节=一个字符 GBK编码中一个汉字（包括数字）占用2个字节 假设VARCHAR(100)与VARCHAR(200)类型，实际存90个字符，它不会对存储端产生影响（就是实际占用硬盘是一样的）。但是，它确实会对查询产生影响，因为当MySql创建临时表（SORT，ORDER等）时，VARCHAR会转换为CHAR，转换后的CHAR的长度就是varchar的长度，在内存中的空间就变大了，在排序、统计时候需要扫描的就越多，时间就越久。 nvarchar和varchar 从存储方式上，nvarchar是按【字符】存储的，而 varchar是按【字节】存储的； 从存储量上考虑， varchar 比较节省空间，因为存储大小为字节的实际长度，而 nvarchar是双字节存储； 在使用上，如果存储内容都是英文字符而没有汉字等其他语言符号，建议使用varchar；含有汉字的使用nvarchar，因为nvarchar是使用Unicode编码，即统一的字符编码标准，会减少乱码的出现几率； 如果你做的项目可能涉及不同语言之间的转换，建议用nvarchar。 超键(super key):在关系中能唯一标识元组的属性集称为关系模式的超键。比如身份证是超键、姓名是超键、(姓名，性别)是超键、(姓名，性别，年龄)是超键 候选键(candidate key):不含有多余属性的超键称为候选键。超键里面的身份证、姓名是候选键 主键(primary key):用户选作元组标识的一个候选键程序主键。 范式 1NF 指的是数据库表中的任何属性都是原子性的，不可再分 2NF 指的数据表里的非主属性都要和这个数据表的候选键有完全依赖关系 对于（学号，课名） → 姓名，有 学号 → 姓名，存在非主属性 姓名 对码（学号，课名）的部分函数依赖 3NF 在满足 2NF 的同时，对任何非主属性都不传递依赖于候选键 学号 → 系名，同时 系名 → 系主任，所以存在非主属性系主任对于学号的传递函数依赖 Mysql的驱动程序主要帮助编程语言与 MySQL 服务端进行通信，如果连接、关闭、传输指令与数据等 MySQL和Oracle 现在使用了PostgreSQL， PostgreSQL与MySQL相比的优点， 首先是强类型的， PostgreSQL 的稳定性更强 在高并发读写，负载逼近极限下，PG的性能指标仍可以维持双曲线甚至对数曲线，到顶峰之后不再下降，而 MySQL 明显出现一个波峰后下滑 PG有大量字典、数组、bitmap 等数据类型，相比之下mysql就差很多 对于WEB应用来说，复制的特性很重要，mysql到现在也是异步复制，pgsql可以做到同步，异步，半同步复制。还有mysql的同步是基于binlog复制，类似oracle golden gate,是基于stream的复制，做到同步很困难，这种方式更加适合异地复制，pgsql的复制基于wal，可以做到同步复制。同时，pgsql还提供stream复制。 explain 执行计划看过没有？其中 type 字段都有哪些值？分别代表什么？type显示查询使用了何种类型。从最好到最差的连接类型依次为： system，const，eq_ref，ref，fulltext，ref_or_null，index_merge，unique_subquery，index_subquery，range，index，ALL 除了all之外，其他的type都可以使用到索引，除了index_merge之外，其他的type只可以用到一个索引。 system。表中只有一行数据或者是空表，这是const类型的一个特例。且只能用于myisam和memory表。如果是Innodb引擎表，type列在这个情况通常都是all或者index。 const。当主键索引或唯一索引的所有字段跟常量值比较时。 eq_ref。多表join时，对于来自前面表的每一行，在当前表中只能找到一行。这可能是除了system和const之外最好的类型。当主键或唯一非NULL索引的所有字段都被用作join联接时会使用此类型。eq_ref可用于使用’=’操作符作比较的索引列。比较的值可以是常量，也可以是使用在此表之前读取的表的列的表达式。 ref。如果每次只匹配少数行，那就是比较好的一种，使用=或&lt;=&gt;，可以是左覆盖索引或非主键或非唯一键。 fulltext。使用全文索引的时候是这个类型。要注意，全文索引的优先级很高，若全文索引和普通索引同时存在时，mysql不管代价，优先选择使用全文索引 ref_or_null。跟ref类型类似，只是增加了null值的比较。 SELECT * FROM ref_table WHERE key_column=expr OR key_column IS NULL; unique_subquery。用于where中的in形式子查询，子查询返回不重复值唯一值，可以完全替换子查询，效率更高。该类型替换了下面形式的IN子查询的ref： value IN (SELECT primary_key FROM single_table WHERE some_expr) index_subquery。该联接类型类似于unique_subquery。适用于非唯一索引，可以返回重复值。 range。索引范围查询，常见于使用 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN()或者like等运算符的查询中。 index。索引全表扫描，把索引从头到尾扫一遍。这里包含两种情况： 一种是查询使用了覆盖索引，那么它只需要扫描索引就可以获得数据，这个效率要比全表扫描要快，因为索引通常比数据表小，而且还能避免二次查询。在extra中显示Using index，反之，如果在索引上进行全表扫描，没有Using index的提示。 all 全表扫描，性能最差。 就算有主键索引什么的，select * from table 仍然是 all。 联合索引，and可以(调换顺序也是可以的)，or会失效。如果是两个独立的索引没问题 mysql的主从复制详细过程。（大意了，学的时候，只整了原理和配置过程） 当有数据来写入到master主机时，数据发生改变，master会将修改写入到bin_log日志中，同时主服务器与从服务器之间会生成一个IO，从服务器会读到bin_log中的数据，会将其中的修改sql语句在执行一次，从而达到主服务器数据发生改变时，同步到从服务器完成主从复制。 数据库中左连接是怎么做的？ Oracle 和 MySQL 都使用了嵌套循环（Nested-Loop Join）的实现方式。嵌套循环算法，需要区分驱动表和被驱动表，先访问驱动表，筛选出结果集，然后将这个结果集作为循环的基础，访问被驱动表过滤出需要的数据。 索引嵌套循环INLJ。A 的行数为 N，所以内循环个数没变也是 N，因为还是要对 N 行 A 数据进行比较。但是内循环次数被优化了。之前的 SNLJ 算法，因为没有索引，每个内循环要扫码一次 B 表。有了索引后，不需要再全表扫描 B 表，而是进行 B 表的索引查询。最终查询和比较的次数大大降低。 块嵌套循环BNLJ。假设这里 A 为驱动表，B 为被驱动表。在外层循环扫描 A 中的所有记录。扫描的时候，会把需要进行 join 用到的列都缓存到 buffer 中。buffer 中的数据有一个特点，里面的记录不需要一条一条地取出来和 B 表进行比较，而是整个 buffer 和 B 表进行批量比较。如果我们把 buffer 的空间开得很大，可以容纳下 A 表的所有记录，那么 B 表也只需要访问访问一次。 说说如何优化连接操作？ 用来进行 join 的字段要加索引，会触发 INLJ 算法，如果是主键的聚簇索引，性能最优。 如果无法使用索引，那么注意调整 join buffer 大小，适当调大些。 小结果集驱动大结果集。用数据量小的表去驱动数据量大的表，这样可以减少内循环个数，也就是被驱动表的扫描次数。 explain执行计划有哪些字段? 除了type比较重要外还有哪些?table、type、possible_keys(可能使用到的索引)、key_len(使用到的索引长度)、Extra(额外的信息说明) 如何分析一条sql的执行，explain有哪些东西type 表示 mysql 访问数据的方式，常见的有全表扫描（all）、遍历索引（index）、区间查询（range）、常量或等值查询（ref、eq_ref）、主键等值查询（const）、当表中只有一条记录时（system）。下面是效率从最好到最差的一个排序。 1system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; all key 表示查询过程实际会用到的索引名称。 rows 表示查询过程中可能需要扫描的行数，这个数据不一定准确，是mysql 抽样统计的一个数据。 Extra 表示一些额外的信息，通常会显示是否使用了索引，是否需要排序，是否会用到临时表等。 UUID作为主键对于索引写的影响/主键用uuid会有什么问题innodb 中的主键是聚簇索引，会把相邻主键的数据安放在相邻的物理存储上。如果主键不是自增，而是随机的，那么频繁的插入会使 innodb 频繁地移动磁盘块，而影响写入性能。 不容易被人看出规律（隐藏用户数量）, 散列好，做缓存字段更好处理，不用自增降低数据库消耗，也更适合做分布式系统主键，长度比 int 好，字符串做主键也不会出现 long 型导致前端 js 麻烦问题 缺点也就性能差点 为什么要用数据库连接池，几个参数介绍DBCP，c3p0，druid 因为建立一个数据库连接是一个非常耗时耗力的事，如果使用连接池，当我们需要连接数据库服务器的时候，只需去连接池中取出一条空闲的连接，而不是新建一条连接。这样我们就可以大大减少连接数据库的开销，从而提高了应用程序的性能。 maxActive 连接池支持的最大连接数，这里取值为20，表示同时最多有20个数据库连接。一般把maxActive设置成可能的并发量就行了设 0 为没有限制。 maxIdle 连接池中最多可空闲maxIdle个连接 ，这里取值为20，表示即使没有数据库连接时依然可以保持20空闲的连接，而不被清除，随时处于待命状态。设 0 为没有限制。 minIdle 连接池中最小空闲连接数，当连接数少于此值时，连接池会创建连接来补充到该值的数量 initialSize 初始化连接数目 maxWait 连接池中连接用完时,新的请求等待时间,毫秒，这里取值-1，表示无限等待，直到超时为止，也可取值9000，表示9秒后超时。超过时间会出错误信息 removeAbandoned 是否清除已经超过“removeAbandonedTimout”设置的无效连接。如果值为“true”则超过“removeAbandonedTimout”设置的无效连接将会被清除。设置此属性可以从那些没有合适关闭连接的程序中恢复数据库的连接。 removeAbandonedTimeout 活动连接的最大空闲时间,单位为秒 超过此时间的连接会被释放到连接池中,针对未被close的活动连接 minEvictableIdleTimeMillis 连接池中连接可空闲的时间,单位为毫秒 针对连接池中的连接对象 timeBetweenEvictionRunsMillis / minEvictableIdleTimeMillis 每timeBetweenEvictionRunsMillis毫秒秒检查一次连接池中空闲的连接,把空闲时间超过minEvictableIdleTimeMillis毫秒的连接断开,直到连接池中的连接数到minIdle为止. 数据库的 having 使用在哪个场景？ having语句是分组后过滤的条件，在group by之后使用，也就是如果要用having语句，必须要先有group by语句。 group by的功能是分组聚合，将多条记录变成比较少的记录，而having的功能是由多变少之后，再变少的过程。另外having后面可以跟多种运算形式，但是运算的结果只能是一个逻辑值（0或者非0的数值）。 假如数据库某个字段是String类型，读的时候用int类型去接收会有什么问题？反过来呢？ 把参数改成?partyid=565613848aaaa能正常查到565613848的数据，改成?partyid=aaaaa565613848就只能查到partyid=0的数据。mysql会从左到右开始读取 一旦遇到非数字则视作后面的所有字符值为0 无论后面是否有数字。 为什么数据量大的时候会出现慢sql？慢查询排查 分页查询方式会从数据库第一条记录开始扫描，所以越往后，查询速度越慢，而且查询的数据越多，也会拖慢总查询速度。使用子查询优化:这种方式先定位偏移位置的 id，然后往后查询，这种方式适用于 id 递增的情况。 1234-- 1327msselect * from orders_history where type=8 andid &gt;= (select id from orders_history where type=8 limit 100000,1) limit 100;-- 3710msselect * from orders_history where type=8 limit 100000,100; 分页查询 分布式ID的特点 全局唯一性。不能出现有重复的ID标识，这是基本要求 递增性。确保生成ID对于用户或业务是递增的 高可用性。确保任何时候都能生成正确的ID 高性能。在高并发的环境下依然表现良好 分布式数据库id生成中间件 苍穹数据库ID是使用Twitter的雪花算法生成的。核心是long类型占用64个bit，通过移位计算出一个long类型的ID。在苍穹系统中，第1位作为正数标志，第241位作为时间戳数量单位毫秒，第4254位作为机器ID数量，第55~64位作为每毫秒产生的序列号。 MySQL的行锁的实现原理 InnoDB行锁是通过给索引上的索引项加锁来实现的，这一点MySQL与Oracle不同，后者是通过在数据块中对相应数据行加锁来实现的。InnoDB这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁！在实际应用中，要特别注意InnoDB行锁的这一特性，不然的话，可能导致大量的锁冲突，从而影响并发性能。 锁升级锁升级（Lock Escalation）是指将当前锁的粒度加大，锁粒度：行锁 &lt; 页锁 &lt; 表锁。 在以下情况可能发生锁升级： 由一句单独的SQL语句在一个对象上持有的锁的数量超过了阈值，默认这个阈值为5000。如果是不同对象，则不会发生锁升级； 锁资源占用的内存超过了激活内存的40%时就会发生锁升级。 InnoDB根据每个事务访问的每个页对锁进行管理，采用位图的方式。因此不管一个事务锁住页中一个记录还是多个记录，其开销通常都是一致的。 B+ 树是什么结构，B+ 树的插入过程？下面以一棵 5 阶 B+ 树的插入过程，5 阶 B+ 树的节点最少 2 个 key，最多 4 个 key。 当树为空树，插入 5。只有一个关键字，叫根节点或叶子节点都是一样的。 再次插入 3 个索引关键字，8，10，15。当前节点 key 存满了，如果再插入当前节点就要进行分裂。 再插入关键字 16。可以看到，这个 B+ 树，现在满足分裂条件了。所以要进行节点分裂。 插入 16 后超过了关键字的个数限制，所以要进行分裂。在叶子结点分裂时，假设分裂出来的左结点有 2 个记录，右节点有 3 个记录，中间 key 成为索引结点中的 key，会成为一个父节点，分裂后的两个节点都指向了父结点（根结点）。 假设我们再插入 17 这个关键字。注意，节点都是有序的。 然后，我们再插入一个 18。 此时，我们发现右边的节点，满足了分裂条件，所有我们要进行分裂。当前结点的关键字个数大于5，进行分裂。分裂成两个结点，左结点2个记录，右结点3个记录，关键字16进位到父结点（索引类型）中，将当前结点的指针指向父结点。 插入若干数据后 接着在上图中插入7，结果如下图所示 当前结点的关键字个数超过4，需要分裂。左结点2个记录，右结点3个记录。分裂后关键字7进入到父结点中，将当前结点的指针指向父结点，结果如下图所示。 当前结点的关键字个数超过4，需要继续分裂。左结点2个关键字，右结点2个关键字，关键字16进入到父结点中，将当前结点指向父结点，结果如下图所示。 当前结点的关键字个数满足条件，插入结束。 以此类推，当插入的数据满足节点分裂时就会进行分裂。但是分裂后，关键字都是有序的。 根据这个插入过程，一个 B+ 树的高度，是有一个节点能存储多少关键字，也就是索引决定的。通常，一棵 MySQL 的 B+ 树，树高为 3 的话，大约能存上亿条。树的高度太高的话，查询效率会大打折扣！ 假如查询A in (), MySQL是针对N个值分别查一次索引,还是有更好的操作？ MySQL IN的原理，如何优化 mysql为什么要有最左前缀原理limit优化offset 偏大之后 limit 查找会变慢：就算有主键索引，这样也是没有用到索引的！！！！！！ 1select * from t limit 10000,10; 这句 SQL 的执行逻辑是 从数据表中读取第N条数据添加到数据集中 重复第一步直到 N = 10000 + 10 根据 offset 抛弃前面 10000 条数 返回剩余的 10 条数据 子查询——将查询落到索引上（先找到主键，然后再根据主键索引找到相应的id） 123Select * From table_name Where id in (Select id From table_name where ( user = xxx )) limit 10000, 10;select * from table_name where( user = xxx ) limit 10000,10 相比较结果是（500w条数据）：第一条花费平均耗时约为第二条的 1/3 左右。同样是较大的 offset，第一条的查询更为复杂，为什么性能反而得到了提升？基本原理就是： 子查询只用到了索引列(user设置为索引列)，没有取实际的数据，所以不涉及到磁盘IO，所以即使是比较大的 offset 查询速度也不会太差。 利用子查询的方式，把原来的基于 user 的搜索转化为基于主键（id）的搜索，主查询因为已经获得了准确的索引值，所以查询过程也相对较快。 1select * from table_name inner join (select id from table_name where (user = xxx) limit 10000,10) b on table_name.id = b.id; sql很慢的解决方案 首先通过jprofile看耗时时间长的操作，具体是不是sql引起的，具体找到哪一行代码耗时最长，是什么sql引起的，是不是因为索引设置出错、是不是有大量相同的SQL，where是否相同，也就是程序中有大量循环操作导致。 在这个操作的时候，是不是因为事务处理不当出现了锁的等待堵塞甚至死锁。通过sql语句可以查询被阻塞线程、被阻塞的sql、阻塞线程、阻塞sql通过performance_schema.data_lock_waits这个表查。 123456789101112select t1.REQUESTING_THREAD_ID as &#x27;被阻塞线程&#x27;,t2.trx_query as &#x27;被阻塞SQL&#x27;,t1.BLOCKING_THREAD_ID as &#x27;阻塞线程&#x27;,t3.trx_query as &#x27;阻塞SQL&#x27;,(UNIX_TIMESTAMP() - UNIX_TIMESTAMP(t3.trx_started)) as &#x27;阻塞时间&#x27;from( SELECT REQUESTING_THREAD_ID,REQUESTING_ENGINE_TRANSACTION_ID,BLOCKING_THREAD_ID,BLOCKING_ENGINE_TRANSACTION_ID FROM performance_schema.data_lock_waits) t1left join information_schema.innodb_trx t2 on t1.REQUESTING_ENGINE_TRANSACTION_ID=t2.trx_idleft join information_schema.innodb_trx t3 on t1.BLOCKING_ENGINE_TRANSACTION_ID=t3.trx_id 还可以通过SHOW ENGINE INNODB STATUS;查询最近依次死锁的日志。在status里面是死锁日志。LATEST DETECTED DEADLOCK后面是两个事务的信息。 然后优化一下sql。分页、过滤掉不需要的数据、如果是join的话优化一下让小表驱动大表。 mybatis&lt;foreach&gt;的作用或者说批处理的作用 批量写入mysql可以大量减少与数据库的交互，减轻数据库的压力。比之前逐个单独写入，批量写入时日志量（MySQL的binlog和innodb的事务让日志）减少，降低了日志刷盘的数据量和频率，从而提高效率。同时也能减少SQL语句解析的次数，减少网络传输的IO，性能上有一定提升。 mysql设计表时怎么去提高性能 数据库范式 ？BCNF？第四第五范式？ 联合索引最多多少个字段 ? MySQL 单张表索引的硬性限制（不能超过 64 个） SQLsql查询一个月内数据的前5的id desc+limit 查找员工表第N高的工资 如何写SQL求出中位数、平均数、众数 将daming的成绩设置成80 where 今天看不同视频数量超过100个的用户id的前三名/每一科成绩的前三名 找出所有语文考及格但是数学没有考及格的学生 先找语文的，在这里面再找数学的。注意in的使用 输出每个course的第一名，包括id,name,score,course 现根据分数找到对应的stu，然后join其他表，最后group by 一个表一千个列值为true和false，写sql 查询查有300个列值为true的行。 1select * from t where LENGTH(REPLACE(CONCAT(v2,v3,v4,v5,v6,v7,v8,v9),&#x27;false&#x27;,&#x27;&#x27;) ) = 4*3; 计算机网络TCP里面遇到滑动窗口为0会发生什么 接收端通告的窗口大小变成0，发送端会发一个1字节的段（就是下一字节的数据，没新的数据段发送的时候发一个ack）（TCP零窗口探测），强制接收端重新宣告下一个期望的字节和窗口大小。如果接收方回复窗口大小仍然为零，则发送方的探测定时器加倍。没有收到ACK时，发送探测包的最大次数之后连接超时 怎么让udp像tcp一样可靠 确认机制——UDP要想可靠，就要接收方收到UDP数据报文段之后回复确认； 超时重传——让每个包有递增的序号，接收方发现中间丢了包就要发重传请求；发送方收不到确认包就要重新发送； 滑动窗口——当网络太差时候频繁丢包，防止越丢包越重传的恶性循环，要有个发送窗口的限制，发送窗口的大小根据网络传输情况调整，调整算法要有一定自适应性。 等于说要在传输层的上一层（或者直接在应用层）实现TCP协议的可靠数据传输机制，比如使用UDP数据包+序列号，UDP数据包+时间戳等方法。 简述URL并分别说明各部分含义基本URL包含协议名、服务器名称（或IP地址）、路径和文件名。 protocol（协议）：指定使用的传输协议。http：通过 HTTP 访问该资源。 格式 HTTP:// ；资源是本地计算机上的文件。格式file:// ；ftp：通过 FTP访问资源。格式 FTP:// hostname（主机名）：是指存放资源的服务器的域名系统 (DNS) 主机名或 IP 地址。 port（端口号）：整数，可选，省略时使用方案的默认端口，各种传输协议都有默认的端口号，如http的默认端口为80。 path（路径）：由零或多个“/”符号隔开的字符串，一般用来表示主机上的一个目录或文件地址。 ?xx=xxx这是问号传参，在HTTP事务中，问号传参是客户端把信息传递给服务器的一种方式（也有可能是跳转到某一个页面，把参数值传递给页面用来标识的）。#xxx这是哈希值，哈希值一般都是跟用户端服务器交互没啥关系，主要用于页面中的锚点定位和HASH路由切换。 http报文格式HTTP请求报文 请求方法——常见的GET/POST 还有GET， POST， PUT， DELETE 请求的URL HTTP协议及版本 报文头 User-Agent：产生请求的浏览器类型。 Accept：客户端可识别的内容类型列表。 Host：请求的主机名，允许多个域名同处一个IP地址，即虚拟主机。 请求数据 HTTP响应报文 报文协议以及版本 状态码 响应头 Server : web服务器软件名称；Content-Type : 请求的与实体对应的MIME信息；Date : 请求发送的日期和时间 响应体。数据、HTML代码或者JS代码等 HTTP如何实现缓存，怎样告诉浏览器这个可以被缓存以及缓存时间 服务端缓存又分为 代理服务器缓存 和 反向代理服务器缓存（也叫网关缓存，比如Nginx反向代理、Squid等），其实广泛使用的 CDN 也是一种服务端缓存，目的都是让用户的请求走”捷径“，并且都是缓存图片、文件等静态资源。 在客户端的话缓存一般指的是浏览器缓存，目的就是加速各种静态资源的访问。 浏览器缓存控制机制有两种：HTML Meta标签 vs. HTTP头信息 使用HTML Meta 标签，在HTML页面的节点中加入标签，在meta标签中可以设定网页的到期时间 12345&lt;!--用于设定网页的到期时间，一旦过期则必须到服务器上重新调用。需要注意的是必须使用GMT时间格式--&gt;&lt;meta http-equiv=&quot;Expires&quot; contect=&quot;Mon,12 May 2001 00:20:00 GMT&quot;&gt;&lt;!--用于设定禁止浏览器从本地机的缓存中调阅页面内容，设定后一旦离开网页就无法从Cache中再调出--&gt;&lt;meta http-equiv=&quot;Pragma&quot; contect=&quot;no-cache&quot;&gt; 使用HTTP头信息控制缓存，浏览器第一次向web服务器请求，请求响应后，协商缓存，比如 Expires策略，Expires是Web服务器响应消息头字段，在响应http请求时告诉浏览器在过期时间前浏览器可以直接从浏览器缓存取数据，而无需再次请求。 Cache-control策略，Cache-Control与Expires的作用一致，都是指明当前资源的有效期，控制浏览器是否直接从浏览器缓存取数据。只不过Cache-Control的选择更多，设置更细致，如果同时设置的话，其优先级高于Expires。 max-age指示客户机可以接收生存期不大于指定时间（以秒为单位）的响应。 min-fresh指示客户机可以接收响应时间小于当前时间加上指定时间的响应。 max-stale指示客户机可以接收超出超时期间的响应消息。如果指定max-stale消息的值，那么客户机可以接收超出超时期指定值之内的响应消息。 Public指示响应可被任何缓存区缓存。 Private指示对于单个用户的整个或部分响应消息，不能被共享缓存处理。这允许服务器仅仅描述当用户的部分响应消息，此响应消息对于其他用户的请求无效。 no-cache指示请求或响应消息不能缓存，该选项并不是说可以设置”不缓存“，容易望文生义~ no-store用于防止重要的信息被无意的发布。在请求消息中发送将使得请求和响应消息都不使用缓存，完全不存下來。 Last-Modified/If-Modified-Since： Last-Modified：标示这个响应资源的最后修改时间。web服务器在响应请求时，告诉浏览器资源的最后修改时间。2 If-Modified-Since：当资源过期时（使用Cache-Control标识的max-age），发现资源具有Last-Modified声明，则再次向web服务器请求时带上头 If-Modified-Since，表示请求时间。web服务器收到请求后发现有头If-Modified-Since 则与被请求资源的最后修改时间进行比对。若最后修改时间较新，说明资源又被改动过，则响应整片资源内容（写在响应消息包体内），HTTP 200；若最后修改时间较旧，说明资源无新修改，则响应HTTP 304 (无需包体，节省浏览)，告知浏览器继续使用所保存的cache Etag/If-None-Match： Etag：web服务器响应请求时，告诉浏览器当前资源在服务器的唯一标识（生成规则由服务器决定）。Apache中，ETag的值，默认是对文件的索引节（INode），大小（Size）和最后修改时间（MTime）进行Hash后得到的 If-None-Match：当资源过期时（使用Cache-Control标识的max-age），发现资源具有Etage声明，则再次向web服务器请求时带上头If-None-Match （Etag的值）。web服务器收到请求后发现有头If-None-Match 则与被请求资源的相应校验串进行比对，决定返回200或304 http tcp ip的联系 HTTP属于应用层，应用层为操作系统或网络应用程序提供访问网络服务的接口。该层的数据放在TCP数据包的数据部分，该层定义了一个很重要的协议——Http协议，一般的Web开发都是基于应用层的开发。之所以叫应用层是因为这些是我们直接接触到的具体应用，比如用于网页浏览的HTTP，用于传输文件的FTP，或者远程登录的Telnet和SSH。应用层需要交换数据的时候，比如你需要访问一个网页，浏览器会先生成HTTP的数据包，然后交给传输层去建立连接。 TCP是传输层，它通过流量控制，超时重传等机制来保证主机间连接的可靠性。TCP使用不同的端口来标记不同的应用发起的连接。为了避免在网络层产生分片，TCP会通过协商MSS，Path MTU Discovery的方式来对数据包进行分段交给网络层。 **IP是网络层，负责处理不同的网络之间发送的数据包，在网络中进行通信的两个计算机之间可能会经过很多个数据链路，也可能还要经过很多通信子网。IP的任务就是选择合适的网间路由和交换结点，确保数据的及时传送**。它不需要考虑具体应用或者端口的信息。它的主要工作是分片（fragmentation）和重组（reassembly）数据包（TCP需要通过重传来保证可靠性，所以不依赖网络层的分片，主要是UDP使用）以及根据IP地址做路由（route）。当然，寄送过程中可能会被中间某跳丢了，或者复制了，或者有的慢有的快，IP层不解决，留给TCP解决。 get和post的区别GET和POST本质上就是TCP链接，并无差别。但是由于HTTP的规定和浏览器/服务器的限制，导致他们在应用过程中体现出一些不同。 传输方式不同：GET参数通过URL传递，参数直接暴露在URL上，POST放在Request body中 传输数据大小不同：相对于GET方式，浏览器或操作系统对POST可传较大量的数据 传输过程不同：对于GET方式的请求，浏览器会把http header和data一并发送出去，服务器响应200（返回数据）；而对于POST，浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200 ok（返回数据）。 传输含义不同：get表达的是一种幂等的，只读的，纯粹的操作，即它除了返回结果不应该会产生其它副作用（如写数据库），因此绝大部分get请求（通常超过90%）都直接被CDN缓存了，这能大大减少web服务器的负担。 而post所表达的语义是非幂等的，有副作用的操作，所以必须交由web服务器处理。把所有get请求换成post，意味着主干网络上的所有CDN都废掉了，web服务器要处理的请求数量将成百上千倍地增加。 GET请求会被浏览器主动cache，而POST不会，除非手动设置 GET请求参数会被完整保留在浏览器历史记录里，而POST中的参数不会被保留 https与http的区别 HTTPS是HTTP协议的安全版本，HTTP协议的数据传输是明文的，是不安全的，HTTPS使用了SSL/TLS协议进行了加密处理。 http和https使用连接方式不同，默认端口也不一样，http是80，https是443。 使用 HTTPS 协议需要申请 CA 证书，一般免费证书较少，因而需要一定费用。证书颁发机构如：Symantec、Comodo、DigiCert 和 GlobalSign 等。 HTTP 页面响应速度比 HTTPS 快，这个很好理解，由于加了一层安全层，建立连接的过程更复杂，也要交换更多的数据，难免影响速度。 由于 HTTPS 是建构在 SSL / TLS 之上的 HTTP 协议，所以，要比 HTTP 更耗费服务器资源。 SSL运行在TCP/IP层之上、应用层之下，为应用程序提供加密数据通道，它采用了RC4、MD5以及RSA等加密算法。 TLS是SSL的标准化后的产物。安全传输层协议（TLS）用于在两个通信应用程序之间提供保密性和数据完整性。该协议由两层组成： TLS 记录协议（TLS Record）和 TLS 握手协议（TLS Handshake）。较低的层为 TLS 记录协议，位于某个可靠的传输协议（例如 TCP）上面。 详细讲述一下ssl 客户端给出支持SSL协议版本号、一个客户端随机数(第一个随机数)、客户端支持的加密方法等信息； 服务器收到信息后，确认双方使用的加密方法，并返回数字证书，一个服务器生成的随机数(第二个随机数)等信息； 客户端确认数字证书的有效性，然后生成一个新的随机数(第三个随机数)，然后使用数字证书中的公钥，加密这个随机数，发给服务器（只有用私钥才能解密）。 服务器使用自己的私钥，获取客户端发来的随机数； 客户端和服务器通过约定的加密方法(通常是AES算法)，使用前面三个随机数，作为后续传输数据使用的对称密钥 对称加密和非对称加密 加密和解密过程不同。对称加密过程和解密过程使用的同一个密钥，加密过程相当于用原文+密钥可以传输出密文，同时解密过程用密文-密钥可以推导出原文。但非对称加密采用了两个密钥，一般使用公钥进行加密，使用私钥进行解密。 加密解密速度不同。对称加密解密的速度比较快，适合数据比较长时的使用。非对称加密和解密花费的时间长、速度相对较慢，只适合对少量数据的使用。 对称加密的过程中无法确保密钥被安全传递，密文在传输过程中是可能被第三方截获的，如果密码本也被第三方截获，则传输的密码信息将被第三方破获，安全性相对较低。 非对称加密算法中私钥是基于不同的算法生成不同的随机数，私钥通过一定的加密算法推导出公钥，但私钥到公钥的推导过程是单向的，也就是说公钥无法反推导出私钥。所以安全性较高。 具体的加密算法对称加密、非对称加密、数字签名、数字证书等 常见的非对称加密算法：RSARSA加密是一种非对称加密。可以在不直接传递密钥的情况下，完成解密。这能够确保信息的安全性，避免了直接传递密钥所造成的被破解的风险。是由一对密钥来进行加解密的过程，分别称为公钥和私钥。两者之间有数学相关，该加密算法的原理就是对一极大整数做因数分解的困难性来保证安全性。通常个人保存私钥，公钥是公开的（可能同时多人持有）。 RSA签名的过程如下： A生成一对密钥（公钥和私钥），私钥不公开，A自己保留。公钥为公开的，任何人可以获取。 A传递自己的公钥给B，B用A的公钥对消息进行加密。 A接收到B加密的消息，利用A自己的私钥对消息进行解密。 在这个过程中，只有2次传递过程，第一次是A传递公钥给B，第二次是B传递加密消息给A，即使都被敌方截获，也没有危险性，因为只有A的私钥才能对消息进行解密，防止了消息内容的泄露。 常见的对称加密算法：DES、AES，DES 和 AES 区别DES是较旧的算法，而AES是高级算法，它比DES更快，更安全。 HTTPS 证书是啥？加密内容？ 可以伪造证书吗？中间人攻击能预防吗？ 客户端在使用HTTPS方式与Web服务器通信时有以下几个步骤，如图所示。（先用非对称加密获取秘钥，用对称加密进行通信） 第一步：首先，当客户端开启一个新的浏览器第一次去访问服务器的时候，会先让客户端安装一个数字证书，这个数字证书里包含的主要信息就是CA机构的公钥。 第二步：服务器发送来了CA机构颁发给自己的数字证书，客户端通过第一步中已经得到的公钥解密CA用私钥加密的Hash-a(这个过程就是非对称加密)，然后再用传递过来的HASH算法生成一个Hash-b，如果Hash-a === Hash-b就说明认证通过，确实是服务器发过来的。 怎么实现跨域访问？CORS是一个W3C标准，全称是”跨域资源共享”（Cross-origin resource sharing）。 实现原理：对于简单请求，浏览器直接发出CORS请求。具体来说，就是在头信息之中，增加一个Origin字段。如果Origin指定的源，不在许可范围内，服务器会返回一个正常的HTTP回应 Tcp握手为什么两次不行，三次握手时候第三次ack信号没有到达服务端会发生什么情况。四次握手呢？ 由于Server没有收到ACK确认，因此会重发之前的SYN+ACK（默认重发五次，之后自动关闭连接），Client收到后会重新传ACK给Server；如果Client向服务器发送数据，服务器会以RST包响应。 队头阻塞 tcp http的角度，分别讲一下TCP的： 队头阻塞（head-of-line blocking）发生在一个TCP分节丢失，导致其后续分节不按序到达接收端的时候，那么这个分节将被接收端一直保持直到丢失的分节被发送端重传并到达接收端为止。该后续分节的延迟递送确保接收应用进程能够按照发送端的发送顺序接收数据。这种为了达到完全有序而引入的延迟机制非常有用，但也有不利之处。 ＨTTP的： http队头阻塞和TCP队头阻塞完全不是一回事。http1.x采用长连接(Connection:keep-alive)，可以在一个TCP请求上，发送多个http请求。管道化，请求可以并行发出，但是响应必须串行返回。后一个响应必须在前一个响应之后。原因是，没有序号标明顺序，只能串行接收。 管道化请求的致命弱点: 会造成队头阻塞，前一个响应未及时返回，后面的响应被阻塞 请求必须是幂等请求，不能修改资源。因为，意外中断时候，客户端需要把未收到响应的请求重发，非幂等请求，会造成资源破坏。 由于这个原因，目前大部分浏览器和Web服务器，都关闭了管道化，采用非管道化模式。无论是非管道化还是管道化，都会造成队头阻塞(请求阻塞)。 UDP 伪首部多大，有啥，有啥用？ UDP和TCP的伪首部只用于计算校验和。32位源IP地址、32位目的IP地址、8位协议、16位UDP长度。 目的是让UDP两次检查数据是否已经正确到达目的地。第一次，通过伪首部的IP地址检验，UDP可以确认该数据报是不是发送给本机IP地址的；第二，通过伪首部的协议字段检验，UDP可以确认IP有没有把不应该传给UDP而应该传给别的高层的数据报传给了UDP。从这一点上，伪首部的作用其实很大。 发送方将UDP伪首部、首部、数据每16位一组进行二进制反码求和，再将求和结果求反码，填入校验和字段。接收方收到UDP报文后，生成伪首部，将伪首部、首部、数据每16位一组进行二进制反码求和，若求和结果全为1则无差错传输，否则丢弃。 如果接收端TCP接收到来自对端的重复数据， 可以根据序列号判断数据是否重复，从而丢弃重复数据 滑动窗口的接收缓存和什么有关系？TCP 连接是由内核维护的，内核会为每个连接建立内存缓冲区： 如果连接的内存配置过小，就无法充分使用网络带宽，TCP 传输效率就会降低； 如果连接的内存配置过大，很容易把服务器资源耗尽，这样就会导致新连接无法建立； 下文代码基于3.2.12内核，主要源文件为：net/ipv4/tcp_input.c。 10G内存能不能用8G做接收缓存，最大能设多大 对于linux，所有的TCP/IP参数都位于/proc/sys/net目录下（请注意，对/proc/sys/net目录下内容的修改都是临时的，任何修改在系统重启后都会丢失），例如下面这些重要的参数： 参数（路径+文件） 描述 默认值 优化值 /proc/sys/net/core/rmem_default 默认的TCP数据接收窗口大小（字节）。 212992 256960 /proc/sys/net/core/rmem_max 最大的TCP数据接收窗口（字节）。 212992 513920 /proc/sys/net/core/wmem_default 默认的TCP数据发送窗口大小（字节）。 229376 256960 /proc/sys/net/core/wmem_max 最大的TCP数据发送窗口（字节）。 131071 513920 滑动窗口分哪几个区域？已经发送并收到确认，已经发送未收到确认，允许发送但尚未发送，不允许发送 拥塞控制慢开始和拥塞避免：拥塞窗口从1开始按照指数增长，达到慢开始门限值每次加1，出现拥塞以后门限值设置为当前拥塞窗口大小/2，拥塞窗口从1开始重复这个过程 快重传和快恢复：发送方收到连续三个重复确认就重传接收方没有收到的报文段，而不用等待重传计时器到期，然后把门限值减半，执行拥塞避免 发送窗口和拥塞窗口 发送窗口 swnd 和接收窗口 rwnd 是约等于的关系，那么由于入了拥塞窗口的概念后，此时发送窗口的值是swnd = min(cwnd, rwnd)，也就是拥塞窗口和接收窗口中的最小值。 拥塞控制对移动端、打游戏，游戏动作同步有什么影响游戏里面很多都是用UDP，你了解吗。为什么直播要用UDP–》直播和视频网站一般用TCP，因为对画面质量有要求，且允许缓冲等待；而微信视频聊天一般用udp，因为不希望画面卡顿，可以接受画面不太清晰 王者荣耀用UDP会有什么问题（我答的丢包） 但是它实际不会出现这个问题，为什么（我答应用层加了处理逻辑） csrf 预防，http/dns 劫持 分片和分段区别 TCP分段的原因是TCP最大分段大小MSS的限制，IP分片的原因是最大传输单元MTU的大小限制。 由于一直有MSS&lt;=MTU，因此也就不需要在网络层进行IP分片了。因此TCP报文段很少会发生IP分片的情况。若数据过大，只会在传输层进行数据分段，到了IP层就不用分片。我们常提到的 IP分片是由于UDP传输协议造成的，因为UDP传输协议并未限定传输数据报的大小。 IP分片由网络层完成，也在网络层进行重组；TCP分段是在传输层完成，并在传输层进行重组 总之，UDP不会分段，就由IP来分片。TCP会分段，当然就不用IP来分片了 MTU（最大传输单元）。MTU是链路层中的网络对数据帧的一个限制，以以太网为例，MTU为1500个字节。一个IP数据报在以太网中传输，如果它的长度大于该MTU值，就要进行分片传输，使得每片数据报的长度小于MTU。分片传输的IP数据报不一定按序到达，但IP首部中的信息能让这些数据报片按序组装 。IP数据报的分片与重组是在网络层进完成的。 MSS（最大分段大小）。MSS是TCP里的一个概念（首部的选项字段中）。MSS是TCP数据包每次能够传输的最大数据分段，TCP报文段的长度大于MSS时，要进行分段传输。TCP协议在建立连接的时候通常要协商双方的MSS值，每一方都有用于通告它期望接收的MSS选项（MSS选项只出现在SYN报文段中，即TCP三次握手的前两次）。Internet上标准的MTU为576，那么如果不设置，则MSS的默认值就为536个字节。很多时候，MSS的值最好取512的倍数。TCP报文段的分段与重组是在运输层完成的。 为什么要避免UDP数据报分片呢？在网络编程中，我们要避免出现IP分片，那么为什么要避免呢？ 简单来说，就是一个消息，只能被封装为一个udp报文，这就是一个整体消息，这个消息被拆解为多个udp报文了，因为udp报文间是没有任何联系的，这个消息实质上就失去意义了。从这个层面上讲的udp报文不能分片。 但如果这个消息被tcp报文携带，那么这个消息可以分为多个tcp报文，tcp报文间是有联系的，每个tcp报文都有seq号，而且每个tcp报文都属于某一个连接，属于同一连接上的多个tcp报文间经过seq号可以排序组装，最终形成那个消息。 对于UDP包，我们需要在应用层去限制每个包的大小，一般不要超过1472字节，即以太网MTU（1500—UDP首部（8）—IP首部（20）。 用过哪些linux命令？如查看内存使用、网络情况？free 命令 是Linux系统中最简单和最常用的内存查看命令， 示例如下: 123456789$ free -m total used free shared buff/cache availableMem: 7822 321 324 377 7175 6795Swap: 4096 0 4095$ free -h total used free shared buff/cache availableMem: 7.6G 322M 324M 377M 7.0G 6.6GSwap: 4.0G 724K 4.0G 使用 top 命令 top 命令一般用于查看进程的CPU和内存使用情况；当然也会报告内存总量，以及内存使用情况，所以可用来监控物理内存的使用情况。在输出信息的顶部展示了汇总信息。 示例输出： 1234567891011top - 15:20:30 up 6:57, 5 users, load average: 0.64, 0.44, 0.33Tasks: 265 total, 1 running, 263 sleeping, 0 stopped, 1 zombie%Cpu(s): 7.8 us, 2.4 sy, 0.0 ni, 88.9 id, 0.9 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem: 8167848 total, 6642360 used, 1525488 free, 1026876 buffersKiB Swap: 1998844 total, 0 used, 1998844 free, 2138148 cachedPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 2986 enlighte 20 0 584m 42m 26m S 14.3 0.5 0:44.27 yakuake 1305 root 20 0 448m 68m 39m S 5.0 0.9 3:33.98 Xorg 7701 enlighte 20 0 424m 17m 10m S 4.0 0.2 0:00.12 kio_thumbnail nload nload 命令可以查看各个网络设备的当前网络速率，也会展示流经设备的总流量。 ss/netstat ss 和 netstat 是查看活动链接/监听端口的常用命令。ss 是 netstat 的替代，性能更好，建议使用。 ss 是 iproute2util 包的一部分，因此在大多数系统上默认安装，也可通过yum install -y iproute安装。netstat 来自 net-tools 包，新版系统上需要自行安装：yum install -y net-tools。 下图是用ss查看tcp连接的输出： iftop iftop 是一款实时流量监控工具，可以查看每个连接的实时速率。 iftop 来自EPEL软件库，安装命令是：yum install -y epel-release &amp;&amp; yum install -y iftop。iftop -nN -i eth0实时查看eth0网卡的各个连接和网速： TCP的粘包？粘包怎么解决？UDP会粘包吗？ 发送端为了将多个数据包更有效的发到接收端，使用了优化方法（Nagle算法），将多次间隔较小且数据量小的数据，合并成一个大的数据块，然后进行封包。但是这样的话接收端就难以将多个包分辨了。 UDP不存在粘包问题，是由于UDP发送的时候，没有经过Negal算法优化，不会将多个小包合并一次发送出去。 另外，在UDP协议的接收端，采用了链式结构来记录每一个到达的UDP包，这样接收端应用程序一次recv只能从socket接收缓冲区中读出一个数据包。 发送端给每个数据包添加包首部，首部中应该至少包含数据包的长度，这样接收端在接收到数据后，通过读取包首部的长度字段，便知道每一个数据包的实际长度了。 发送端将每个数据包封装为固定长度（不够的可以通过补0填充），这样接收端每次从接收缓冲区中读取固定长度的数据就自然而然的把每个数据包拆分开来。 可以在数据包之间设置边界，如添加特殊符号，这样，接收端通过这个边界就可以将不同的数据包拆分开。 TCP面向什么传输，UDP面向什么传输？TCP是面向连接的协议。UDP是一个简单的面向数据报的传输层协议。 显示URL到浏览器整个过程浏览器打开一个网站的过程中会经历哪些网络处理，DNS的具体过程是啥 面试官问每次都要域名解析吗？使用本地缓存。 TCP报文 16位的源端口号、16位的目标端口号 32位序号 32位确认序号 4位首部长度、6位保留位、6位状态位、16位滑动窗口的大小 16位校验和、16位紧急指针 紧急URG,当设置为有效时（URG=1）,表示该标志位有效，告诉操作系统有紧急数据要传送，而不要按原来的排列顺序来传送。 确认ACK，仅当ACK为1时，确认字段有效，为0时，确认字段无效 推送PSH，两个进程在进行交互式通信时，一个进程键入一个命令希望另一个进程立即收到该进程的响应，将PSH置为1，TCP使用推送操作，发送方发送一个报文段，接收方收到TCP推送的报文段时，立即向前交付接受应用程序，不等整个缓冲区满了才向上交付。 复位RST,RST为1时，表明TCP连接中出现了严重差错，必须释放链接，需要去重新建立链接。也可以拒绝非法报文和拒绝打开链接。 同步SYN，连接建立时同步序号。 终止FIN，释放连接 TCP的四次挥手，为什么有2MSL的等待时间，出现大量TIME_WAIT的情况以及解决方法 在Client发送出最后的ACK回复，但该ACK可能丢失。Server如果没有收到ACK，将不断重复发送FIN片段。所以Client不能立即关闭，它必须确认Server接收到了该ACK。Client会在发送出ACK之后进入到TIME_WAIT状态。Client会设置一个计时器，等待2MSL的时间。如果在该时间内再次收到FIN，那么Client会重发ACK并再次等待2MSL。所谓的2MSL是两倍的MSL(Maximum Segment Lifetime)。MSL指一个片段在网络中最大的存活时间，2MSL就是一个发送和一个回复所需的最大时间。如果直到2MSL，Client都没有再次收到FIN，那么Client推断ACK已经被成功接收，则结束TCP连接。 在高并发短连接的TCP服务器上，当服务器处理完请求后立刻主动正常关闭连接。这个场景下会出现大量socket处于TIME_WAIT状态。如果客户端的并发量持续很高，此时部分客户端就会显示连接不上。 服务内核参数调整：可以先看看 ip_local_port_range 能否还能再设置大一些，以便可以使用开启更多的端口（如何修改可以参考这里）。 增加服务器：既然一台机器已经无法承载更多的连接了，加服务器是最快捷和合理的方案。 尽量不要让服务器端成为主动关闭连接的一方：设置服务器端的 KeepAlive ，尽可能不让服务器端主动关闭连接，而是让客户端连接，这样就不会出现 TIME_WAIT 过多的问题。 短连接表示“业务处理+传输数据的时间 远远小于 TIMEWAIT超时的时间”的连接。 在 Linux 系统中，MSL 被定义成 30 秒， 2MSL 就是 60 秒 Session、Cookie、Token 由于http的无状态性，为了使某个域名下的所有网页能够共享某些数据，session和cookie出现了。客户端访问服务器的流程如下 首先，客户端会发送一个http请求到服务器端。 服务器端接受客户端请求后，建立一个session，并发送一个http响应到客户端，这个响应头，其中就包含Set-Cookie头部。该头部包含了sessionId。Set-Cookie格式如下，具体请看Cookie详解Set-Cookie: value[; expires=date][; domain=domain][; path=path][; secure] 在客户端发起的第二次请求，假如服务器给了set-Cookie，浏览器会自动在请求头中添加cookie 服务器接收请求，分解cookie，验证信息，核对成功后返回response给客户端 session存储于服务器，可以理解为一个状态列表，拥有一个唯一识别符号sessionId，通常存放于cookie中。服务器收到cookie后解析出sessionId，再去session列表中查找，才能找到相应session。依赖cookie cookie类似一个令牌，装有sessionId，存储在客户端，浏览器通常会自动添加。另外，cookie只是实现session的其中一种方案。虽然是最常用的，但并不是唯一的方法。禁用cookie后还有其他方法存储，比如放在url中。 token 的认证流程与cookie很相似 用户登录，成功后服务器返回Token给客户端。 客户端收到数据后保存在客户端 客户端再次访问服务器，将token放入headers中 服务器端采用filter过滤器校验。校验成功则返回请求数据，校验失败则返回错误码 token也类似一个令牌，无状态，用户信息都被加密到token中，服务器收到token后解密就可知道是哪个用户。需要开发者手动添加。token在客户端一般存放于localStorage，cookie，或sessionStorage中。在服务器一般存于数据库中。 Cookies在HTTP请求的哪个位置，以及Cookies的格式每个HTTP请求和响应都会带有相应的头部信息。Cookie：当前页面设置的任何Cookie。 格式就是键值对。 1Set-Cookie: &quot;name=value;domain=.domain.com;path=/;expires=Sat, 11 Jun 2016 11:29:42 GMT;HttpOnly;secure&quot; Session是存储在服务器上，如何存储的？存储方式的区别Session的服务端服务实现由很多种，常见有磁盘文件保存（PHP默认方式，会阻塞）、MySQL（Discuz! 的自己的实现方式）、Memcached（性能较高的一种实现）、Redis。 tcp和udp区别tcp为什么可靠序列号，确认重发，超时重传，三握四挥，流量控制，拥塞控制 序列号：给发送的每一个包进行编号 校验和：TCP报文段首部和数据的校验和，如果收到的校验和有差错，TCP将丢弃这个报文段和不确认收到此报文段 流量控制：TCP连接的每一方都有固定大小的缓冲空间，TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据。TCP的流量控制协议是可变大小的滑动窗口协议 拥塞控制：当网络拥塞时，调整数据的发送的大小 超时重传：当TCP发出一个段后，将启动一个定时器，等待目的端口确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段 半连接攻击以及如何解决半连接攻击 半连接攻击是一种攻击协议栈的攻击方式，坦白说就是攻击主机的一种攻击方式。通过将主机的资源消耗殆尽，从而导致应用层的程序无资源可用，导致无法运行。在正常情况下，客户端连接服务端需要通过三次握手，首先客户端构造一个SYN连接数据包发送至服务端，自身进入SYN_SEND状态，当服务端收到客户端的SYN包之后，为其分配内存核心内存，并将其放置在半连接队列中。半连接就是通过不断地构造客户端的SYN连接数据包发向服务端，等到服务端的半连接队列满的时候，后续的正常用户的连接请求将会被丢弃，从而无法连接到服务端。此为半连接攻击方式。根据服务端的半连接队列的大小，不同主机的抵抗这种SYN攻击的能力也是不一样。 可以通过拓展半连接队列的大小，来进行补救，但缺点是，不能无限制的增加，这样会耗费过多的服务端资源，导致服务端性能地下。这种方式几乎不可取。现主要通syn cookie或者syn中继机制来防范半连接攻，部位半连接分配核心内存的方式来防范。 调整半连接队列阀值（正对不同操作系统带参数），缩短timeout，syn cookie,设置可疑队列，syn中继 全连接攻击以及解决全连接攻击 全连接攻击是通过消费服务端进程数和连接数，只连接而不进行发送数据的一种攻击方式。当客户端连接到服务端，仅仅只是连接，此时服务端会为每一个连接创建一个进程来处理客户端发送的数据。但是客户端只是连接而不发送数据，此时服务端会一直阻塞在recv或者read的状态，如此一来，多个连接，服务端的每个连接都是出于阻塞状态从而导致服务端的崩溃。 可以通过不为全连接分配进程处理的方式来防范全连接攻击，具体的情况是当收到数据之后，在为其分配一个处理线程。具体的处理方式在accept返回之前是不分配处理线程的。直到接收相关的数据之后才为之提供一个处理过程。 Connection=keep-alive干嘛的 HTTP每次请求都会执行TCP连接握手操作，比较消耗资源而且性能也不是很好，所以通过客户端告知服务端建立长连接方式，进行长连接复用。客户端的下次请求即可使用该长连接进行发送。HTTP1.0协议要求客户端增加Connection=keep-alive表明长连接请求，服务端也需要返回加Connection=keep-alive表示支持，然后双方建立长连接，客户端可复用该条连接请求。http1.1默认使用加Connection=keep-alive的长连接模式，默认的连接时长可以在服务器端设置不会让它长时间连接防止无效资源占用。长连接可以保证客户端可进行连接复用，但不能使服务器端主动向客户端发送请求(复用的是底层TCP的Socket连接，但需要和TCP的keep-alive区分开) session 和 cookie 是什么，有什么区别 Session是在服务端保存用来跟踪用户的状态，这个数据可以保存在集群、数据库、文件中；Session的生命周期是指一个终端用户与交互系统进行通信的时间间隔，通常指从注册进入系统到注销退出系统之间所经过的时间以及如果需要的话，可能还有一定的操作空间。 Cookie是客户端保存用户信息的一种机制，用来记录用户的一些信息，也是实现Session的一种方式。如果不设置过期时间，则表示这个cookie生命周期为浏览器会话期间，只要关闭浏览器窗口，cookie就消失了。Cookie其实还可以用在一些方便用户的场景下，设想你某次登陆过一个网站，下次登录的时候不想再次输入账号了，怎么办？这个信息可以写到Cookie里面，访问网站的时候，网站页面的脚本可以读取这个信息，就自动帮你把用户名给填了，能够方便一下用户。这也是Cookie名称的由来，给用户的一点甜头。 token session存储于服务器，可以理解为一个状态列表，拥有一个唯一识别符号sessionId，通常存放于cookie中。服务器收到cookie后解析出sessionId，再去session列表中查找，才能找到相应session。依赖cookie cookie类似一个令牌，装有sessionId，存储在客户端，浏览器通常会自动添加。 token也类似一个令牌，无状态，用户信息都被加密到token中，服务器收到token后解密就可知道是哪个用户。需要开发者手动添加。 jwt只是一个跨域认证的方案 token可以抵抗csrf(跨站请求伪造)，cookie+session不行 如果黑客在其他页面设置了一个链接，这个链接指向一个网站的转账系统。并且当前用户是这个网站的会员，并且处于登陆的状态(也就是客户端浏览器存在存储合法的session_id的cookie)，那么当用户点击了这个链接以后，那么客户端浏览器就会将用户的这些信息进行传递到服务端，但是这个链接具体做了什么，用户根本不知道，这也就做到了伪造了用户的身份，做了用户都不知道的事情。(转账) DNS 是基于传输层的什么协议的？ DNS区域传输的时候使用TCP协议：辅域名服务器会定时（一般3小时）向主域名服务器进行查询以便了解数据是否有变动。如有变动，会执行一次区域传送，进行数据同步。区域传送使用TCP而不是UDP，因为数据同步传送的数据量比一个请求应答的数据量要多得多。TCP是一种可靠连接，保证了数据的准确性。 域名解析时使用UDP协议：客户端向DNS服务器查询域名，一般返回的内容都不超过512字节，用UDP传输即可。不用经过三次握手，这样DNS服务器负载更低，响应更快。理论上说，客户端也可以指定向DNS服务器查询时用TCP，但事实上，很多DNS服务器进行配置的时候，仅支持UDP查询包。 tcp连接后，路由器突然断开了，需要重新连接吗根据tcp/ip协议的描述，tcp连接建立之后，如果双方没有通信，连接可以一直保存下去，假如中间路由器崩溃或者中间的某条线路断开，只要两端的主机没有被重启，连接就一直被保持着。 保活定时器一般配置的时间是2小时，即服务器每个2小时就会向客户端发送探查消息，如果收到客户端的反馈消息，则再等2个小时再发；如果等不到客户端的反馈，则再等75秒钟再发一次保活探查，这样连续发送10次，如果10次都没有收到反馈，就认为客户端已经异常断开了，此时，tcp层的程序就会向上层应用程序发送一条“连接超时”的错误反馈。 一个 10M 大小的 buffer 里存满了数据，现在要把这个 buffer 里的数据尽量发出去，可以允许部分丢包，问是用TCP好还是UDP好？为什么？一个完整的 HTTP 请求会涉及到哪些协议？HTTP协议就是基于TCP/IP协议模型来传输信息的。 链路层。也称作数据链路层或网络接口层（在第一个图中为网络接口层和硬件层），通常包括操作系统中的设备驱动程序和计算机中对应的网络接口卡。它们一起处理与电缆（或其他任何传输媒介）的物理接口细节。ARP（地址解析协议）和RARP（逆地址解析协议）是某些网络接口（如以太网和令牌环网）使用的特殊协议，用来转换IP层和网络接口层使用的地址。 网络层。处理分组在网络中的活动，例如分组的选路。在TCP/IP协议族中，网络层协议包括IP协议（网际协议），ICMP协议（Internet互联网控制报文协议），以及IGMP协议（Internet组管理协议）。 IP是一种网络层协议，提供的是一种不可靠的服务，它只是尽可能快地把分组从源结点送到目的结点，但是并不提供任何可靠性保证。同时被TCP和UDP使用。TCP和UDP的每组数据都通过端系统和每个中间路由器中的IP层在互联网中进行传输。 ICMP是IP协议的附属协议。IP层用它来与其他主机或路由器交换错误报文和其他重要信息。 IGMP是Internet组管理协议。它用来把一个UDP数据报多播到多个主机。 传输层主要为两台主机上的应用程序提供端到端的通信。在TCP/IP协议族中，有两个互不相同的传输协议：TCP（传输控制协议）和UDP（用户数据报协议）。 应用层。应用层决定了向用户提供应用服务时通信的活动。TCP/IP 协议族内预存了各类通用的应用服务。包括 HTTP，FTP（File Transfer Protocol，文件传输协议），DNS（Domain Name System，域名系统）服务。 socket的原理 syn-cookie算法 http1.1和2.0主要区别 http头部压缩怎么协商 介绍一下cc防护算法 介绍一下nginx能说多少是多少 其他问题输入Url之后发生了什么 Dns解析的全过程 Http1.1 特点，缺点 http2.0 特点，缺点 http3.0 特点，缺点 QUIC 特点，缺点 常见的拥塞控制算法 （BBR，RENO，BIC-tcp） TCP连接过程，如何实现拥塞控制？ BBR 怎么快，怎么实现，和之前的有什么区别 tcp 选项有什么 tcp 首部多大 TCP 首部的数据结构，如果不计选项字段，首部是 20 个字节。 tcp半打开和半关闭的区别 udp 存在的意义 tcp怎么计算时间，RTT和RTO ？ tcp 异常处理，什么时候有RST， Redis用分布式锁的时候主节点挂了，从节点还没有同步到数据，锁丢失了，这种情况怎么解决 setnx 锁最大的缺点就是它加锁时只作用在一个 Redis 节点上，即使 Redis 通过 Sentinel(哨岗、哨兵) 保证高可用，如果这个 master 节点由于某些原因发生了主从切换，那么就会出现锁丢失的情况，下面是个例子： 在 Redis 的 master 节点上拿到了锁； 但是这个加锁的 key 还没有同步到 slave 节点； master 故障，发生故障转移，slave 节点升级为 master节点； 上边 master 节点上的锁丢失。 有的时候甚至不单单是锁丢失这么简单，新选出来的 master 节点可以重新获取同样的锁，出现一把锁被拿两次的场景。锁被拿两次，也就不能满足安全性了… redlock算法 redis集群之间的关系，三个节点存储数据是相同的，还是不同的。存储的数据是不同的 Redis Cluster 是如何进行扩容的？redis的主从复制，超过缓冲区大小怎么办？若发生缓存溢出，则要进行全量复制。 释放锁如果不用lua脚本会出现什么问题 为了解决锁不能正常释放的问题，所以set锁的时候，需要设置过期时间。因为多进程释放锁加锁时，是无法做到原子操作，比如进程 A 执行完业务逻辑，在准备释放锁时，恰好这时候进程 A 的锁自动过期时间到了，而另一个进程 B 获得锁成功，然后 B 还没来得及执行，进程 A 就执行了 delete(key) ，释放了进程 B 的锁，因此需要配合 Lua 脚本释放锁 redis集群部署介绍，get key从发起请求到获取值的过程redis的内部一致性是怎么实现的，RDB快照的过程，为什么fork进程能够读到快照数据？全量和增量复制的区别 全量复制用于初次复制或者其他无法进行部分复制的情况，将主节点的所有数据都发送给从节点，是一个非常重型的操作，当数据量较大时，会对主从节点和网络造成很大的开销 增量复制用于处理在主从复制中因网络闪断等原因造成的数据丢失场景，当从节点再次连上主节点后，如果条件允许，主节点的复制积压缓冲区内存将这部分数据则直接发送给从节点，这样就可以保持主从节点复制的一致性。补发的这部分数据一般远远小于全量数据。，因为补发的数据远远小于全量数据，可以有效避免全量复制的过高开销，需要注意的是，如果网络中断时间过长，造成主节点没有能够完整的保存中断期间执行的写命令，则无法进行部分复制，仍使用全量复制 主节点接到psync命令后首先核对参数runId是否与自身一致，如果一致，说明之前复制的是当前主节点；之后根据参数offset在自身复制积压缓冲区查找，如果偏移量之后的数据存在缓冲区中，则对从节点发送+COUTINUE响应，表示可以进行部分复制。因为缓冲区大小固定，若发生 redis集群介绍一下 redis的集群是将一共16384个槽分给集群中节点，每个节点通过gossip协议得知其它节点的信息，并在自身的clusterState中记录了所有的节点的信息和槽数组的分配情况 Gossip协议基本思想就是：一个节点想要分享一些信息给网络中的其他的一些节点。于是，它周期性的随机选择一些节点，并把信息传递给这些节点。这些收到信息的节点接下来会做同样的事情，即把这些信息传递给其他一些随机选择的节点。一般而言，信息会周期性的传递给N个目标节点，而不只是一个。20个节点且设置fanout=4，公式结果是2.16，这只是个近似值。真实传递时，可能需要3次甚至4次循环才能让所有节点收到消息。这是因为每个节点在传播消息的时候，是随机选择N个节点的，这样的话，就有可能某个节点会被选中2次甚至更多次。 gossip 好处在于，元数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续打到所有节点上去更新，降低了压力；不好在于，元数据的更新有延时，可能导致集群中的一些操作会有一些滞后 客户端是如何访问集群的？ 客户端会先访问集群中的一个节点，如果槽命中直接访问，如果不命中，则会返回MOVED指令，并告知槽实际存在的节点，然后再去访问。(这里其实还有个迁移中的情况，如果访问的槽正在迁移，则返回ask命令，客户端会被引导去目标节点查找) 一致性 hash？那和普通hash有什么区别？ 一致性hash把hash的空间虚拟成一个圆环，key做hash落在圆环上，按顺时针查找，遇到的第一个缓存节点就命中。另外还可以通过虚拟节点避免缓存分布不均，以及某个节点挂了之后，下面的节点只需要承担一部分的流量而不会因为需要承担所有流量而挂了，然后发生雪崩 Redis分布式锁先SETNX后,未来得及设置过期时间宕机了怎么办 set key value nx ex seconds(原子操作) LUA脚本保证原子操作 某个线程在申请分布式锁的时候，为了应对极端情况，比如机器宕机，那么这个锁就一直不能被释放。一个比较好的解决方案是，申请锁的时候，预估一个程序的执行时间，然后给锁设置一个超时时间，这样，即使机器宕机，锁也能自动释放。 但是这也带来了一个问题，就是在有时候负载很高，任务执行的很慢，锁超时自动释放了任务还未执行完毕，这时候其他线程获得了锁，导致程序执行的并发问题。对这种情况的解决方案是：在获得锁之后，就开启一个守护线程，定时去查询Redis分布式锁的到期时间，如果发现将要过期了，就进行续期。 Redis内存不足时怎么样 执行淘汰策略.默认是返回错误;还有随即淘汰和最近最少使用淘汰策略(注意设置maxmemory,否则64位机器会用尽机器内存) 使用集群模式,将数据分片存储(中间件解决分片/业务keyhash实现) 减少大key产生,查询大key优化 Redis如何查看大KEY redis-cli –bigkeys 查看大Key;基于scan命令,不用担心阻塞问题.对String类型统计字节长度,对集合类型统计元素个数. debug object key 查看对应key的序列化后长度 手动执行bgsave命令,生成rdb文件,通过redis rdb tools工具分析rdb文件 memory usage key 查看对应key的内存使用(4.0+版本) 缓存雪崩缓存雪崩的情况是说，当某一时刻发生大规模的缓存失效的情况，比如你的缓存服务宕机了，会有大量的请求进来直接打到DB上面。结果就是DB 撑不住，挂掉。 使用集群缓存，保证缓存服务的高可用。这种方案就是在发生雪崩前对缓存集群实现高可用，如果是使用 Redis，可以使用 主从+哨兵 ，Redis Cluster 来避免 Redis 全盘崩溃的情况。 ehcache本地缓存 + Hystrix限流&amp;降级，避免MySQL被打死。使用 ehcache 本地缓存的目的也是考虑在 Redis Cluster 完全不可用的时候，ehcache 本地缓存还能够支撑一阵。使用 Hystrix进行限流 &amp; 降级 ，比如一秒来了5000个请求，我们可以设置假设只能有一秒 2000个请求能通过这个组件，那么其他剩余的 3000 请求就会走限流逻辑。然后去调用我们自己开发的降级组件（降级），比如设置的一些默认值呀之类的。以此来保护最后的 MySQL 不会被大量的请求给打死。 服务限流算法 限流怎么做的？令牌桶的算法实现？限流还有哪些方式？ 限流的算法有：计数器算法、漏桶算法、令牌桶算法 令牌桶算法是比较常见的限流算法之一，大概描述如下： 所有的请求在处理之前都需要拿到一个可用的令牌才会被处理 根据限流大小，设置按照一定的速率往桶里添加令牌 桶设置最大的放置令牌限制，当桶满时、新添加的令牌就被丢弃或者拒绝； 请求达到后首先要获取令牌桶中的令牌，拿着令牌才可以进行其他的业务逻辑，处理完业务逻辑之后，将令牌直接删除； 令牌桶有最低限额，当桶中的令牌达到最低限额的时候，请求处理完之后将不会删除令牌，以此保证足够的限流； 实现简单令牌桶算法，没有考虑随时间滑动的情况； 加强版：令牌桶，加上随时间滑动的要求，即：限制用户在任一连续的一小时内，不能超过5W的请求。这边提到了说将一小时分成多格，比如60格这样的，面试官点头貌似同意了，然后就实现代码了，包括协程异步更新时间窗口； 服务降级 这里有两种场景: 当下游的服务因为某种原因响应过慢，下游服务主动停掉一些不太重要的业务，释放出服务器资源，增加响应速度 当下游的服务因为某种原因不可用，上游主动调用本地的一些降级逻辑，避免卡顿，迅速返回给用户 其实应该要这么理解: 服务降级有很多种降级方式！如开关降级、限流降级、熔断降级 服务熔断属于降级方式的一种 服务熔断：当下游的服务因为某种原因突然变得不可用或响应过慢，上游服务为了保证自己整体服务的可用性，不再继续调用目标服务，直接返回，快速释放资源。如果目标服务情况好转则恢复调用。 1234567try&#123; //调用下游的helloWorld服务 xxRpc.helloWorld();&#125;catch(Exception e)&#123; //因为熔断，所以调不通 doSomething();&#125; 注意看，下游的helloWorld服务因为熔断而调不通。此时上游服务就会进入catch里头的代码块，那么catch里头执行的逻辑，你就可以理解为降级逻辑! 开关降级也是我们生产上常用的另一种降级方式！做法很简单，做个开关，然后将开关放配置中心！在配置中心更改开关，决定哪些服务进行降级。至于配置变动后，应用怎么监控到配置发生了变动，这就不是本文该讨论的范围。那么，在应用程序中部下开关的这个过程，业内也有一个名词，称为埋点！ 那接下来最关键的一个问题，哪些业务需要埋点？一般有以下方法 简化执行流程。自己梳理出核心业务流程和非核心业务流程。然后在非核心业务流程上加上开关，一旦发现系统扛不住，关掉开关，结束这些次要流程。 关闭次要功能。一个微服务下肯定有很多功能，那自己区分出主要功能和次要功能。然后次要功能加上开关，需要降级的时候，把次要功能关了吧！ 降低一致性。假设，你在业务上发现执行流程没法简化了，愁啊！也没啥次要功能可以关了，桑心啊！那只能降低一致性了，即将核心业务流程的同步改异步，将强一致性改最终一致性！ 缓存穿透正常情况下，我们去查询数据都是存在的。那么请求去查询一条压根儿数据库中根本就不存在的数据，也就是缓存和数据库都查询不到这条数据，但是请求每次都会打到数据库上面去。之所以会发生穿透，就是因为缓存中没有存储这些空数据的key。从而导致每次查询都到数据库去了。 那么我们就可以为这些key对应的值设置为null 丢到缓存里面去。后面再出现查询这个key 的请求的时候，直接返回null 。这样，就不用在到数据库中去了。(对于空数据的key有限的，重复率比较高的，我们则可以采用第一种方式进行缓存) 在缓存之前在加一层 BloomFilter ，在查询的时候先去 BloomFilter 去查询 key 是否存在，如果不存在就直接返回，存在再走查缓存 -&gt; 查 DB。(针对于一些恶意攻击，攻击带过来的大量key 是不存在的) 缓存击穿在平常高并发的系统中，大量的请求同时查询一个 key 时，此时这个key正好失效了，就会导致大量的请求都打到数据库上面去。这种现象我们称为缓存击穿。 在第一个查询数据的请求上使用一个 互斥锁来锁住它。其他的线程走到这一步拿不到锁就等着，等第一个线程查询到了数据，然后做缓存。后面的线程进来发现已经有缓存了，就直接走缓存。 Redis挂了，流量把数据库也打挂了，怎么办？这是啥？Redis 挂了，不就是缓存都没了吗？缓存都没了，不就是缓存雪崩了吗？缓存雪崩了，不就导致数据库挂了吗？一提到“缓存雪崩”这四个字，缓存穿透、缓存击穿这几兄弟，是不是就立马条件反射的出现在你的脑海里面了，还顺带着对应的几套解决方案。(穿，只是穿过了缓存。透，是直接干到底。) Redis 挂了，为什么挂了？怎么就挂了？是不是有单点问题？这不就是问你 Redis 服务的高可用吗？说到 Redis 的高可用，脑子里面必须马上蹦出来主从、哨兵和集群吧？ 这时该怎么进行恢复？站在运维人员的角度，当然优先考虑是先把 Redis 和数据库服务重新启动起来啦。但是启动之前得先做个小操作，把流量摘掉，可以先把流量拦截在入口的地方，比如简单粗暴的通过 Nginx 的配置把请求都转到一个精心设计的错误页面，就是说这么一个意思。这样做的目的是为了防止流量过大，直接把新启动的服务，启动一个打挂一个的情况出现。当 Redis 服务重新启动后，通过程序先放点已知的热点 key 进去后，系统再对外提供服务，防止缓存击穿的场景。 该怎么进行预防？服务中是不是需要考虑限流或者熔断机制，最大程度的保护程序的运行？或者我们是否应该建立多级缓存的机制，防止 Redis 挂掉之后，大批流量直接打到 MySQL 服务导致数据库的崩盘？多级缓存、限流措施、服务降级、熔断机制 Redis Sentinel 重选主的流程，客户端找不到新主怎么办？ 当客户端试图连接失效的Master时，集群也会向客户端返回新Master的地址。Master和Slave服务器切换后，Master的redis.conf、Slave的redis.conf和sentinel.conf的配置文件的内容都会发生相应的改变，即，Master主服务器的redis.conf配置文件中会多一行slaveof的配置，sentinel.conf的监控目标会随之调换。 过期策略 使用过Redis的同学应该知道，我们在设置一个key之后，可以指定这个key的过期时间。那么这个key到了过期时间就会立即被删除吗？Redis是如何删除这些过期key的呢？ 先说结论：Redis是使用定期删除+惰性删除两者配合的过期策略。 定期删除 定期删除指的是Redis默认每隔100ms就随机抽取一些设置了过期时间的key，检测这些key是否过期，如果过期了就将其删掉。因为key太多，如果全盘扫描所有的key会非常耗性能，所以是随机抽取一些key来删除。这样就有可能删除不完，需要惰性删除配合。 惰性删除 惰性删除不再是Redis去主动删除，而是在客户端在请求某个key的时候，Redis会先去检测一下这个key是否已经过期，如果没有过期则返回给客户端，如果已经过期了，那么Redis会删除这个key，不会返回给客户端。 所以惰性删除可以解决一些过期了，但没被定期删除随机抽取到的key。但有些过期的key既没有被随机抽取，也没有被客户端访问，就会一直保留在数据库，占用内存，长期下去可能会导致内存耗尽。所以Redis提供了内存淘汰机制来解决这个问题。 为什么不使用定时删除？所谓定时删除，指的是用一个定时器来负责监视key，当这个key过期就自动删除，虽然内存及时释放，但是十分消耗CPU资源，因此一般不推荐采用这一策略。 内存淘汰策略 noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。默认策略 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。 allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。 LRU和LFULRU和LFU都是内存管理的页面置换算法。主要思想都是：如果数据最近被访问过，那么将来被访问的几率也更高。 LRU，即：最近最少使用淘汰算法（Least Recently Used）。LRU是淘汰最长时间没有被使用的页面， 存在的问题：偶发性的、周期性的批量查询操作（包含冷数据）会淘汰掉大量的热点数据，导致 LRU 命中率急剧下降，缓存污染情况比较严重。 LFU，即：最不经常使用淘汰算法（Least Frequently Used）。LFU是淘汰一段时间内使用次数最少的页面。 存在的问题： 最近加入的数据总是易于被剔除（缓存末端抖动），因为他起始的频率很低。它无法对一个拥有最初高访问率之后长时间没有被访问的条目缓存负责。 为了避免早期的热点数据一直占据缓存，即LFU算法也需有一些访问时间模式的特性。但是，如果频率的时间度量是 1 小时（数据根据最近一小时内的访问次数排序），则平均每小时访问 1000 次的数据可能会比前一个小时内访问次数为 1001 的数据更优先剔除掉。 一般情况下，LFU 效率要优于 LRU，且能够避免周期性或者偶发性的操作导致缓存命中率下降的问题，但 LFU 需要记录数据的历史访问记录，一旦数据访问模式改变，LFU 需要更长时间来适用新的访问模式，即 LFU 存在历史数据影响将来数据的 缓存污染 问题。 LFU 使用计数器来记录条目被访问的频率，通过使用 LFU 缓存算法，最低访问次数的条目首先被移除，此方法并不经常使用，因为它无法对一个拥有最初高访问率之后长时间没有被访问的条目缓存负责。 持久化如何处理过期？RDB 从内存持久化数据到RDB文件：持久化key之前，会检查是否过期，过期的key不进入RDB文件。从RDB文件恢复数据到内存：数据载入内存之前，会对key先进行过期检查，如果过期，不导入内存（主库情况）。 如果 Redis 以主服务器的模式运行，那么会对 RDB 中的键进行时间检查，过期的键不会被恢复到 Redis 中。 如果 Redis 以从服务器的模式运行，那么 RDB 中所有的键都会被载入，忽略时间检查。在从服务器与主服务器进行数据同步的时候，从服务器的数据会先被清空，所以载入过期键不会有问题。 AOF 如果一个键过期了，那么不会立刻对 AOF 文件造成影响。因为 Redis 使用的是惰性删除和定期删除，只有这个键被删除了，才会往 AOF 文件中追加一条 DEL 命令。在重写 AOF 的过程中，程序会检查数据库中的键，已经过期的键不会被保存到 AOF 文件中。 对于主服务器，一个过期的键被删除了后，会向从服务器发送 DEL 命令，通知从服务器删除对应的键 从服务器接收到读取一个键的命令时，即使这个键已经过期，也不会删除，而是照常处理这个命令。 从服务器接收到主服务器的 DEL 命令后，才会删除对应的过期键。 redis扩容时 新老旧节点 数据迁移具体是怎么做的？ cannal binlog mq具体过程？怎么防止重复消费？怎么防止消息丢失？ 一致性hash一致性哈希在某节点宕机时怎么保证一致性的？ 布隆过滤器 讲述布隆过滤器的原理之前，我们先思考一下，通常你判断某个元素是否存在用的是什么？应该蛮多人回答 HashMap 吧，确实可以将值映射到 HashMap 的 Key，然后可以在 O(1) 的时间复杂度内返回结果，效率奇高。但是 HashMap 的实现也有缺点，例如存储容量占比高，考虑到负载因子的存在，通常空间是不能被用满的，而一旦你的值很多例如上亿的时候，那 HashMap 占据的内存大小就变得很可观了。 还比如说你的数据集存储在远程服务器上，本地服务接受输入，而数据集非常大不可能一次性读进内存构建 HashMap 的时候，也会存在问题。 布隆过滤器（Bloom Filter）的核心实现是一个超大的位数组和几个哈希函数。假设位数组的长度为m，哈希函数的个数为k 以上图为例，具体的操作流程：假设集合里面有3个元素{x， y， z}，哈希函数的个数为3。首先将位数组进行初始化，将里面每个位都设置位0。对于集合里面的每一个元素，将元素依次通过3个哈希函数进行映射，每次映射都会产生一个哈希值，这个值对应位数组上面的一个点，然后将位数组对应的位置标记为1。查询W元素是否存在集合中的时候，同样的方法将W通过哈希映射到位数组上的3个点。如果3个点的其中有一个点不为1，则可以判断该元素一定不存在集合中。反之，如果3个点都为1，则该元素可能存在集合中。注意：此处不能判断该元素是否一定存在集合中，可能存在一定的误判率。可以从图中可以看到：假设某个元素通过映射对应下标为4，5，6这3个点。虽然这3个点都为1，但是很明显这3个点是不同元素经过哈希得到的位置，因此这种情况说明元素虽然不在集合中，也可能对应的都是1，这是误判率存在的原因。 它有一个致命的缺点，就是不支持删除。为什么？ 假设要删除 [why]，那么就要把 1,4,8 这三个位置置为 0。但是你想啊，[jay] 也指向了位置 8 呀。如果删除 [why] ，位置 8 变成了 0，那么是不是相当于把 [jay] 也移除了？ 布隆过滤器还有一个问题：查询性能不高。 因为真实场景中过滤器中的数组长度是非常长的，经过多个不同 Hash 函数后，得到的数组下标在内存中的跨度可能会非常的大。跨度大，就是不连续。不连续，就会导致 CPU 缓存行命中率低。 布谷鸟过滤器它有两个 hash 表，记为 T1，T2。两个 hash 函数，记为 h1，h2。 当一个不存在的元素插入的时候，会先根据 h1 计算出其在 T1 表的位置，如果该位置为空则可以放进去。 如果该位置不为空，则根据 h2 计算出其在 T2 表的位置，如果该位置为空则可以放进去。 如果该位置不为空，就把当前位置上的元素踢出去，然后把当前元素放进去就行了。 也可以随机踢出两个位置中的一个，总之会有一个元素被踢出去。 支持动态的新增和删除元素。 提供了比传统布隆过滤器更高的查找性能，即使在接近满的情况下（比如空间利用率达到 95% 的时候）。 比诸如商过滤器（quotient filter，另一种过滤器）之类的替代方案更容易实现。 如果要求错误率小于3%，那么在许多实际应用中，它比布隆过滤器占用的空间更小。 Redis 和 Memcached的区别 数据操作不同。与Memcached仅支持简单的key-value结构的数据记录不同，Redis支持的数据类型要丰富得多。Memcached基本只支持简单的key-value存储，不支持枚举，不支持持久化和复制等功能。Redis支持服务器端的数据操作相比Memcached来说，拥有更多的数据结构和并支持更丰富的数据操作，支持list、set、sorted set、hash等众多数据结构，还同时提供了持久化和复制等功能。 内存管理机制不同。在Redis中，并不是所有的数据都一直存储在内存中的。当物理内存用完时，Redis可以将一些很久没用到的value交换到磁盘，重启的时候可以再次加载进行使用。 。而memcache不支持数据持久存储 集群管理不同。redis支持master-slave复制模式，memcache可以使用一致性hash做分布式 数据一致性不同。redis使用的是单线程模型，保证了数据按顺序提交。memcache需要使用cas保证数据一致性。CAS（Check and Set）是一个确保并发一致性的机制，属于“乐观锁”范畴；原理很简单：拿版本号，操作，对比版本号，如果一致就操作，不一致就放弃任何操作 这是和Memcached相比一个最大的区别。Redis只会缓存所有的key的信息，如果Redis发现内存的使用量超过了某一个阀值，将触发swap的操作，Redis根据“swappability = age*log(size_in_memory)”计算出哪些key对应的value需要swap到磁盘。然后再将这些key对应的value持久化到磁盘中，同时在内存中清除。这种特性使得Redis可以保持超过其机器本身内存大小的数据。 从内存利用率来讲，使用简单的key-value存储的话，Memcached的内存利用率更高。而如果Redis采用hash结构来做key-value存储，由于其组合式的压缩，其内存利用率会高于Memcached。 小结：Redis和Memcached哪个更好？ Redis更多场景是作为Memcached的替代者来使用，当需要除key-value之外的更多数据类型支持或存储的数据不能被剔除时，使用Redis更合适。如果只做缓存的话，Memcached已经足够应付绝大部分的需求，Redis 的出现只是提供了一个更加好的选择。总的来说，根据使用者自身的需求去选择才是最合适的。 redis常用的数据结构有哪几种，在你的项目中用过哪几种，以及在业务中使用的场景主要使用的是value为String类型的。 String：字符串类型 在用户登录之后，将用户对象转为Json字符串 保存秒杀商品信息的时候，将商品的uuid保存 保存秒杀订单，将秒杀订单转为Json字符串 判断是否已经秒杀过等等 List：列表类型。商品的关注列表；店铺的粉丝列表；商品评论（将评论信息转成json存储到list中） Set：无序集合类型。共同好友；利用唯一性，统计访问网站的所有独立ip；好友推荐时，根据tag求交集，大于某个阈值就可以推荐 ZSet：有序集合类型。销售的排行榜；带权重的消息队列 Hash：哈希表类型。存储、读取、修改用户属性 为什么要用redis做缓存？主要还是相对于mysql等关系型数据库，redis的性能更高以及redis支持高并发。 为什么redis性能高支持高并发？主要有四点 redis是存储在内存中的，是纯内存的操作 redis中存储的是键值对，内部采用了高效的数据结构 redis的是单线程模型的，减少了线程切换所带来的的开销 redis使用的是非阻塞IO，IO多路复用。吞吐能力比较大 怎么实现redis缓存的一致性？ 在我自己做的秒杀项目中，系统不是严格要求缓存和数据库必须严格一致性的，主要用redis抗住峰值流量，只是简单的给缓存设置过期时间以保证最终一致性，比如商品的详情，设置的时间为1s。 如果需要保证缓存和数据库的一致性的话：读的时候，先读缓存，缓存没有的话就去读数据库，然后取出数据后放入缓存中。更新的时候，先更新数据库，然后再删除缓存。 另外还有延时双删策略(缓存双淘汰法)+设置缓存过期时间，可以**将前面所造成的缓存脏数据，再次删除**： 先删除(淘汰)缓存 再写数据库（1和2的步骤可以互换） 休眠一段时间，再次删除(淘汰)缓存 还有一个更复杂的基于订阅binlog的异步更新缓存策略。binlog增量订阅消费+消息队列+增量数据更新到redis。 读Redis：热数据基本都在Redis 写MySQL:增删改都是操作MySQL 更新Redis数据：MySQ的数据操作binlog，来更新到Redis 这样一旦MySQL中产生了新的写入、更新、删除等操作，就可以把binlog相关的消息推送至Redis，Redis再根据binlog中的记录，对Redis进行更新。 redis的hash怎么实现的，rehash过程讲一下和JavaHashMap的rehash有什么区别？ Redis中，键值对（Key-Value Pair）存储方式是由字典（Dict）保存的，而字典底层是通过哈希表来实现的。通过哈希表中的节点保存字典中的键值对。类似Java中的HashMap，将Key通过哈希函数映射到哈希表节点位置。和HashMap不同的是，redis里面保存的是两张哈希表以便进行rehash。 1234567891011typedef struct dict &#123; // 和类型相关的处理函数 dictType *type; // 上述类型函数对应的可选参数 void *privdata; // 两张哈希表，ht[0]为原生哈希表，ht[1]为 rehash 哈希表 dictht ht[2]; // 当等于-1时表示没有在 rehash，否则表示 rehash 的下标 long rehashidx; int iterators;// 迭代器数量(暂且不谈)&#125; dict; rehash的过程： 两张哈希表，ht[0]为原生哈希表，ht[1]为 rehash 哈希表。如果是扩容操作，为 ht[1] 分配空间，ht[1] 的大小为第一个大于等于 $ht[0].used*2$ 的 $2^n$ 比当前 ht[0].used 值的二倍大的第一个 2 的整数幂；如果是缩容操作，ht[1] 的大小为第一个大于等于 $ht[0].used$ 的 $2^n$。 将 ht[0] 中的键值 Rehash 到 ht[1] 中。 当 ht[0] 全部迁移到 ht[1] 中后，释放 ht[0]，将 ht[1] 置为 ht[0]，并为 ht[1] 创建一张新表，为下次 Rehash 做准备。 渐进式rehash的过程： 为ht[1]分配空间，字典中维护一个 rehashidx，并将它置为 0，表示 Rehash 开始。 在 Rehash 期间，还可以对字典操作。程序会将 ht[0] 在 rehashidx 索引上的键值对 rehash 到 ht[1] 中，当 Rehash 完成后，将 rehashidx+1(位置变化)。当全部 rehash 完成后，将 rehashidx 置为 -1，表示 rehash 完成。 redis cluster怎么做到高可用的？ Redis Cluster可以很方便的进行横向扩容 当新的节点加入进来的时候，通过reshard（重新分片）来将已经分配给某个节点的任意数量的slot迁移给另一个节点，在Redis内部是由redis**-**trib负责执行的。 假设我们需要向集群中加入一个D节点，而此时 集群内已经有A、B、C三个节点了。此时redis**-trib会向A、B、C三个节点发送迁移出槽位的请求，同时向D节点发送准备导入槽位的请求，做好准备之后A、B、C这三个源节点就开始执行迁移，将对应的slot所对应的键值对迁移至目标节点D。最后redis-**trib会向集群中所有主节点发送槽位的变更信息。 如果Redis Cluster中的某个master节点挂了，会进行故障转移 简单来说，针对A节点，某一个节点认为A宕机了，那么此时是主观宕机。而如果集群内超过半数的节点认为A挂了， 那么此时A就会被标记为客观宕机。 一旦节点A被标记为了客观宕机，集群就会开始执行故障转移。其余正常运行的master节点会进行投票选举，从A节点的slave节点中选举出一个，将其切换成新的master对外提供服务。当某个slave获得了超过半数的master节点投票，就成功当选。当选成功之后，新的master会执行slaveof no one来让自己停止复制A节点，使自己成为master。然后将A节点所负责处理的slot，全部转移给自己，然后就会向集群发PONG消息来广播自己的最新状态。 按照一致性哈希的思想，如果某个节点挂了，那么就会沿着那个圆环，按照顺时针的顺序找到遇到的第一个Redis实例。而对于Redis Cluster，某个key它其实并不关心它最终要去到哪个节点，他只关心他最终落到哪个slot上，无论你节点怎么去迁移，最终还是只需要找到对应的slot，然后再找到slot关联的节点，最终就能够找到最终的Redis实例 遇到过redis的hotkey吗？怎么处理的？ redis集群和哨兵机制有什么区别？ 哨兵模式是基于主备模式演进而来的。一个Master可以有多个Slaves，sentinel发现master挂了后，就会从slave中重新选举一个master。 集群：集群是基于哨兵模式，为了解决单机Redis容量有限的问题，将数据按一定的规则分配到多台机器，内存/QPS不受限于单机，可受益于分布式集群高扩展性。 redis的持久化机制了解吗？在项目中是怎么做持久化的？Redis提供了两种不同的持久化方法可以将数据存储在磁盘中，一种叫快照RDB，另一种叫追加文件AOF。 在指定的时间间隔内将内存中的数据集快照写入磁盘(Snapshot)，它恢复时是将快照文件直接读到内存里。 Redis会单独创建（fork）一个子进程来进行持久化，会先将数据写入到一个临时文件中，待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件。整个过程中，主进程是不进行任何IO操作的，这就确保了极高的性能如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那RDB方式要比AOF方式更加的高效。RDB的缺点是最后一次持久化后的数据可能丢失。 AOF 会把 Redis 服务器执行的写命令记录到一个日志文件中，当服务器重启时再次执行 AOF 文件中的命令来恢复数据。 Redis每次执行，会将执行的写命令追加到 AOF 的缓冲区 aof_buf，随后根据相应的策略才真正将缓冲区的数据写入到磁盘里。并且定期对 AOF 进行重写，从而实现对写命令的压缩。 redis单线程的吗？单线程为什么还这么快？ Redis的线程模型 纯内存操作。数据存放在内存中，内存的响应时间是纳秒级别的 使用IO多路复用技术。利用 epoll/select/kqueue 等多路复用技术，在单线程的事件循环中不断去处理事件（客户端请求），最后回写响应数据到客户端。 单线程的优势。非CPU密集型任务，线程没有了线程上下文切换和访问共享资源加锁的性能损耗，而且单线程模型对程序的开发和调试非常友好。 高效的数据结构 用一种称为 SDS（Simple Dynamic String）的结构体来保存字符串。 字典类型的。Rehash和渐进式 Rehash。 Zset底层的数据结构是跳跃表 skiplist 编码转化。对于某一种类型的数据，可能有多种编码来实现 讲一讲redis的内存模型？Redis的内存占用主要可以划分为以下几个部分： 数据。这部分内存在used_memory中。Redis所有数据都是Key-Value型，每次创建Key-Value都是创建2个对象，即Key对象和Value对象。Key对象都是字符串。Value对象则包括5种类型（String，List，Hash，Set，Zset） 进程本身运行需要的内存。Redis主进程本身运行肯定需要占用内存，如代码、常量池等等。这部分内存在used_memory中。 缓冲内存。包括客户端缓冲区、复制积压缓冲区、AOF缓冲区等 客户端缓冲存储客户端连接的输入输出缓冲；复制积压缓冲用于部分复制功能；AOF缓冲区用于在进行AOF重写时，保存最近的写入命令。这部分内存由jemalloc分配，因此会统计在used_memory中。 内存碎片是Redis在分配、回收物理内存过程中产生的。例如，如果对数据的更改频繁，而且数据之间的大小相差很大，可能导致redis释放的空间在物理内存中并没有释放，但redis又无法有效利用，这就形成了内存碎片。内存碎片不会统计在used_memory中。 Redis Cluster的架构，怎么寻找对应的Key的？ 一般集群建议搭建三主三从架构，三主提供服务，三从提供备份功能。每一个节点都存有这个集群所有主节点以及从节点的信息 所有的redis节点彼此互联(PING-PONG机制)，内部使用二进制协议优化传输速度和带宽 客户端与redis节点直连，不需要中间proxy层。客户端不需要连接集群所有节点，连接集群中任何一个可用节点即可 节点的fail是通过集群中超过半数的master节点检测失效时才生效。如果有一半以上的节点去ping一个节点的时候没有回应，集群就认为这个节点宕机了，然后去连接它的备用节点。如果某个节点和所有从节点全部挂掉，我们集群就进入fail状态。还有就是如果有一半以上的主节点宕机，那么我们集群同样进入fail状态。 redis cluster并没有使用一致性哈希来计算key对应的节点，而是通过记录一张映射表 hash slots的方式。由于分层两层结构，从key到slotId这一步还是使用了哈希算法的。为了提高客户端快速找到key所在的slot，这里采用了哈希的方式，计算公式是: slotId = crc16(key) % 16384。 如何保证消息的顺序执行？Kafka了解吗？ 你为啥不用kafka来做，当时怎么考虑的？ tcp怎么保证有序传输的，讲下tcp的快速重传和拥塞机制，知不知道time_wait状态，这个状态出现在什么地方，有什么用？ 有没有了解过协程？说下协程和线程的区别？用过哪些linux命令？如查看内存使用、网络情况？ RDB的实现细节 客户端发起 BGSAVE 命令，Redis 主进程判断当前是否存在正在执行备份的子进程，如果存在则直接返回 父进程 fork 一个子进程 （fork 的过程中会造成阻塞的情况），这个过程可以使用 info stats 命令查看 latest_fork_usec 选项，查看最近一次 fork 操作消耗的时间，单位是微秒 父进程 fork 完成之后，则会返回 Background saving started 的信息提示，此时 fork 阻塞解除 fork 创建的子进程开始根据父进程的内存数据生成临时的快照文件，然后替换原文件 子进程备份完毕后向父进程发送完成信息，父进程更新统计信息 为什么bgsave不阻塞请求，那这时候如果来请求了redis如何处理的 为什么 fork 之后的子进程能够获取父进程内存中的数据？ 通过 fork 生成的父子进程会共享包括内存空间在内的资源； fork 函数是否会带来额外的性能开销，这些开销我们怎么样才可以避免？ fork 函数并不会带来明显的性能开销，对内存进行大量的拷贝，它能通过写时拷贝将拷贝内存这一工作推迟到真正需要的时候 在fork之后exec之前两个进程用的是相同的物理空间（内存区），子进程的代码段、数据段、堆栈都是指向父进程的物理空间，也就是说，两者的虚拟空间不同，但其对应的物理空间是同一个。当父子进程中有更改相应段的行为发生时，再为子进程相应的段分配物理空间。 总体来看，Redis还是读操作比较多。如果子进程存在期间，发生了大量的写操作，那可能就会出现很多的分页错误(页异常中断page-fault)，这样就得耗费不少性能在复制上。 而在rehash阶段上，写操作是无法避免的。所以Redis在fork出子进程之后，将负载因子阈值提高，尽量减少写操作，避免不必要的内存写入操作，最大限度地节约内存。 生成的快照是精确到指定时刻的内存数据，还是在某个时间段内的内存数据？Copy On Write 机制，备份的是开始那个时刻内存中的数据。 RDB的优缺点优点 生成RDB快照文件的过程是异步的，所以在持久化过程中对服务器性能影响小。 RDB文件存储的是内存数据库的快照，采用紧凑的二进制文件存储。通过RDB文件进行数据库恢复的时候速度快。 缺点 由于RDB文件是异步进行备份的，所以存在数据安全性弱的弊端：当系统发生故障导致内存数据库数据丢失的时候，从RDB文件中只能恢复创建RDB快照那一刻的数据，在最近一次创建RDB快照那一刻到服务器宕机之间的数据将永久性的丢失了。数据恢复的完整程度依赖于RDB快照创建的频率。 由于RDB快照是将整个内存数据库备份下来，所以当内存数据库很大的时候创建RDB文件需要耗费更久的时间。 AOF的实现细节 命令追加（append）：将 Redis 执行的写命令追加到 AOF 的缓冲区 aof_buf 文件写入（write）和文件同步（fsync）：AOF 根据对应的策略将 aof_buf 的数据同步到硬盘 write：为了提高文件的写入效率，当用户调用 write 函数将数据写入文件时，操作系统会先把数据写入到一个内存缓冲区里，当缓冲区被填满或超过了指定时限后，才真正将缓冲区的数据写入到磁盘里。 fsync：Redis 提供了 appendfsync 配置项来控制 AOF 缓存区的文件同步策略，appendfsync 可配置以下三种策略： appendfsync always：每执行一次命令保存一次 命令写入 aof_buf 缓冲区后立即调用系统 fsync 函数同步到 AOF 文件 appendfsync no：不保存 命令写入 aof_buf 缓冲区后调用系统 write 操作，不对 AOF 文件做 fsync 同步；同步由操作系统负责，通常同步周期为 30 秒。 appendfsync everysec：每秒钟保存一次 文件重写（rewrite）：定期对 AOF 进行重写，从而实现对写命令的压缩。 触发重写，执行bgrewriteaof命令 父进程fork子进程进行重写，fork子进程的同时父进程阻塞，fork完毕父进程继续接受指令（子进程只是父进程的快照（相当于复制了某时刻的父进程）） 子进程在创建新的aof文件的同时，父进程继续接收write指令，存储到继续存到aof_buf缓存中和aof_rewirte_buf缓存中，所以父进程继续往旧的aof文件中备份，同时也要往新的AOf文件中备份。 新的aof备份完成 同时父进程，备份的新文件创建完成 将aof_rewrite_buf缓存中的备份到新的aof文件中 新的文件替换旧的aof文件 AOF的优缺点优势 每修改同步：appendfsync always 同步持久化，每次发生数据变更会被立即记录到磁盘，性能较差但数据完整性比较好 每秒同步：appendfsync everysec 异步操作，每秒记录，如果一秒内宕机，有数据丢失 不同步：appendfsync no 从不同步 劣势 相同数据集的数据而言aof文件要远大于rdb文件，恢复速度慢于rdb aof运行效率要慢于rdb，每秒同步策略效率较好，不同步效率和rdb相同 所以如果 Redis 服务器开启了 AOF 持久化功能，那么服务器会优先使用 AOF 文件来还原数据库状态；只有在 AOF 的持久化功能处于关闭状态时，服务器才会使用使用 RDB 文件还原数据库状态。 缓存和DB之间怎么保证数据一致性缓存预留模式读操作：先读缓存，缓存没有的话读DB，然后取出数据放入缓存，最后响应数据写操作：先更新DB，再删除缓存为什么是删除而不是更新呢？原因很简单，复杂场景下缓存不单单是DB中直接取出来的值，此外更新缓存的代价是很高的，频繁更新的缓存到底会不会被频繁访问到？可能更新到缓存里面的数据都是冷数据，频繁失效，所以一般用到再去加载缓存，lazy加载的思想 先更新DB，再删除缓存的问题，如果更新DB成功，删除缓存失败会导致数据不一致所以一般是先删除缓存，再更新DB 还是有问题，A先删除了缓存，但还没更新DB，这时B过来请求数据，发现缓存没有，去请求DB拿到旧数据，然后再写到缓存，等A更新完了DB之后就会出现缓存和DB数据不一致的情况了 解决方案：更新数据时，根据数据的唯一标识路由到队列中，读取数据时，如果发现数据不再缓存中，那么把读取数据+更新缓存的操作，根据唯一标识路由之后，也发送到相应队列中。一个队列对应一个工作线程，线程串行拿到队列里的操作一一执行 带来的新问题：可能数据更新频繁，导致队列中积压了大量的更新操作，读请求长时间阻塞，所以要压测 一致性Hash的特点，虚拟节点的作用 RpcContext是在什么维度传递的？ Dubbo的远程调用怎么实现的？ Spring的单例是怎么实现的？ 为什么要单独实现一个服务治理框架？ Redis了解哪些（我把底层的SDS，渐进式hash，压缩表，raft算法，gossip协议疯狂输出） 项目 如何优化秒杀高并发 通过使用redis集群实现的分布式Seesion。 UUID是value，用户的ID是key。 用户登录成功之后给这个用户创建一个类似于sessionId的东西标示这个用户，我们称之为token写到cookie当中，将cookie存放在reids中的并传递给客户端。客户端在随后访问当中都在cookie中上传token，服务端都根据token取到用户session信息，因为是访问的redis集群，所以效率提高了 并借助redis缓存降低服务器的压力 获取商品列表、商品信息的时候先去redis中获取信息。如果redis中没有缓存的话再去查找数据库。下定单也是会保存到redis缓存中的 在进行秒杀的时候，如果redis有相应的订单的话就及时阻止。如果没有相应订单，也会现在redis缓存中预减库存 使用消息队列完成异步下单，削峰和降流 消息队列在一端承接瞬时的流量洪峰，在另一端平滑地将消息推送出去。 秒杀接口地址的隐藏 对于每一个用户生成唯一的秒杀地址，主要还是为了防刷 接口限流防刷 创建一个自定义注解+拦截器+Redis实现限流 redis如果宕机了怎么办 会抛出异常，请求不会怼到数据库 测过哪些QPS，redis的QPS多少，mysql的QPS多 用户id在秒杀的时候是通过url的方式传的，还是通过request（cookie，SSO） 用户登录时，验证用户的账户和密码。生成一个Token保存在数据库中，将Token写到Cookie中， 将用户数据保存在Session中。请求时都会带上Cookie，检查有没有登录，如果已经登录则放行 redis限流的key是怎么样的 用户的ID+request.getRequestURI 两次MD5的原理，为什么两次，第二次salt放在数据库里面，客户端如何获取到 开始做秒杀的时候有没有设定目标，有没有参考其他做的很好的。 我看你还用了RabbitMQ，简单说一下RabbitMQ的工作原理？ 点击秒杀时，先通过userId和商品的id生成一个uuid，前端将uuid组合成url通过post调用方法，再调用具体的秒杀方法，先判断url对不对；然后判断缓存中库存是否还有；接着在判断是否已经秒杀到了，也就是缓存中是否有对应的秒杀订单了；如果没有的话通过redis进行预减库存看能否预减成功：是通过redisTemplate.opsForValue().increment(realKey, -1)，这个方法使用的是redis的Incrby命令，Incrby是原子操作。(这里可能会出现线程安全的问题，预减完再进行一次判断就好了)；接着进行排队，进行异步下单。接着在消息队列的消费者一端，再次判断一下数据库中的库存是否足够，以及有没有秒杀过了，如果还是没有的话进行下订单写数据库了。下订单中还是要判断一下库存。 消息队列异步下单，时间很长怎么办？既然是异步请求，可以在提交要下单的请求成功，就可以返回了。缩短用户等待时间。在跳转到订单页面的时候，前端页面一直轮询方法查询缓存中有没有生成订单。 怎么实现限流防刷？自定义了一个注解： @AccessLimit(seconds=5, maxCount=5, needLogin=true)，5秒内点击秒杀的次数最大为5次，并且还是要登录的。实现拦截器，实现HandlerInterceptor接口，在preHandle方法中进行判断，具体的次数是对redis缓存进行incrby-1 @Retention(RUNTIME)RUNTIME代表可以被JVM执行，当需要在运行时动态获取注解信息（比如通过反射）时选择此策略 @Target(METHOD) 如果保存到缓存时出错怎么办？抛出异常。 Zookeeperzk介绍一下 ZooKeeper是一个分布式的应用程序协调服务，它是集群的管理者，监视着集群中各个节点的状态根据节点提交的反馈进行下一步操作。最终，将简单易用的接口和性能高效、功能稳定的系统提供给用户 。 Zookeeper一个最常用的使用场景就是用于担任服务生产者和服务消费者的注册中心，服务生产者将自己提供的服务注册到Zookeeper中心，服务的消费者在进行服务调用的时候先到Zookeeper中查找服务，获取到服务生产者的详细信息之后，再去调用服务生产者的内容与数据。 zk的结构、节点间的通讯协议是什么。 讲讲 ZK 的结构？ Zookeeper数据模型的结构与Unix文件系统很类似，整体上可看作是一棵树，每个节点称作一个ZNode。每个ZNode默认能存储1MB数据，每个ZNode都可以通过其路径唯一标识。 与文件系统不同的是，这些节点都可以设置关联的数据，而文件系统中只有文件节点可以存放数据而目录节点不行。 znode的内容： cZxid ：创建的事务标识 ctime：创建的时间戳 dataVersion ：节点数据的版本号，每次对节点进行修改操作（set）后，dataVersion的值都会增加1（即使设置的是相同的数据） dataLength：节点存储的数据长度，单位为 B （字节） numChildren：直接子节点的个数 pZxid：直接子节点最后更新的事务标识，子节点有变化（创建create、修改set、删除delete，rmr）时，都会更新pZxid。 cversion ：直接子节点的版本号。当子节点有变化（创建create、修改set、删除delete，rmr）时，cversion 的值就会增加1。 ephemeralOwner：当前节点是临时节点（ephemeral node ）时，这个ephemeralOwner的值是客户端持有的session id。 zookeeper选举机制如何进行选主的？从 CAP 分析下Leader选举是集群正常运行的前提，当集群启动或Leader失联后，就会进入Leader选举流程。 所有节点进入LOOKING状态 每个节点广播携带自身ID和ZXID的选票，投票推举自己为Leader 节点接收其他节点发送的选票，把选票信息和自己推举的选票进行PK（选票中ZXID大者胜出，ZXID相同，则ID大者胜出） 如果外部选票获胜，则保存此选票信息，并把选票广播出去（赞成该选票） 循环上述3-4步骤 当有选票得到超过半数节点赞成，且该选票的所有者也赞成该选票，则选举成功，该选票所有者成为Leader Leader切换为LEADING，Follower切换为FOLLOWING，Observer切换为OBSERVING状态，选举结束，进入数据同步流程。 节点间通讯用的协议 Zookeeper的核心是原子广播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议。Zab协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数Server完成了和leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和Server具有相同的系统状态。 zk是什么分布式模型（想问的CAP定理） CP。也就是保证了一致性和分区容错性 zk只有一个主节点，写性能不高，zk怎么解决的写过程 对应一个ZooKeeper集群，我们可能有多个客户端，客户端能任意连接其中一台ZooKeeper节点，但是所有的客户端都只能往leader节点上面去写数据，所有的客户端能从所有的节点上面读取数据。 如果有客户端连接的是follower节点，然后往follower上发送了写数据的请求，这个时候follower就会把这个写请求转发给leader节点处理。leader接受到写请求就会往其他节点（包括自己）同步数据，如果过半的节点接受到消息后发送回来ack消息，那么leader节点就对这条消息进行commit，commit后该消息就对用户可见了。 因为需要过半的节点发送ack后，leader才对消息进行commit，这个时候会有一个问题，如果集群越大，那么等待过半节点发送回来ack消息这个过程就需要越久，也就是说节点越多虽然会增加集群的读性能，但是会影响到集群的写性能，所以我们一般建议ZooKeeper的集群规模在3到5个节点左右。为了解决这个问题，后来的ZooKeeper中增加了一个observer 的角色，这个节点不参与投票，只是负责同步数据。比如我们leader写数据需要过半的节点发送ack响应，这个observer节点是不参与过半的数量统计的。它只是负责从leader同步数据，然后提供给客户端读取，所以引入这个角色目的就是为了增加集群读的性能，然后不影响集群的写性能。用户搭建集群的时候可以自己设置该角色。 所有的写的请求，转发给Leader，Leader采取两阶段提交的方式。 这里和Raft系统不同，Raft是master先commit，再ack 客户，最后在下一个心跳消息里面通知所有小弟们commit 分布式的paxos和raft算法了解么paxos：多个proposer发请提议（每个提议有id+value），acceptor接受最新id的提议并把之前保留的提议返回。当超过半数的acceptor返回某个提议时，此时要求value修改为proposer历史上最大值，proposer认为可以接受该提议，于是广播给每个acceptor，acceptor发现该提议和自己保存的一致，于是接受该提议并且learner同步该提议。 raft：raft要求每个节点有一个选主的时间间隔，每过一个时间间隔向master发送心跳包，当心跳失败，该节点重新发起选主，当过半节点响应时则该节点当选主机，广播状态，然后以后继续下一轮选主。 zk 一致性保证 只有超过半数节点Ack了的事务操作，才会被commit，才会最终响应到客户端。所以响应了客户端的操作，不管leader是否挂了，新leader中肯定存了这个日志，否则选举中不会获胜。 未完成半数Ack的事务操作，leader挂了，新leader可能保存这个日志，也可能没有保存这个日志。 如果新leader没有这个事务操作的日志，依赖客户端的超时重试机制，来完成这个proposal，客户端会发起重试。 如果新leader有这个uncommitted的事务操作日志，则会替代老leader继续完成这个操作 zk 事务操作有序性 ZAB协议。在整个消息广播中，Leader会将每一个事务请求转换成对应的 proposal 来进行广播，并且在广播 事务Proposal 之前，Leader服务器会首先为这个事务Proposal分配一个全局单递增的唯一ID，称之为事务ID（即zxid），由于Zab协议需要保证每一个消息的严格的顺序关系，因此必须将每一个proposal按照其zxid的先后顺序进行排序和处理。 ZAB的消息广播具体步骤 客户端发起一个写操作请求。 Leader 服务器将客户端的请求转化为事务 Proposal 提案，同时为每个 Proposal 分配一个全局的ID，即zxid。 Leader 服务器为每个 Follower 服务器分配一个单独的队列，然后将需要广播的 Proposal 依次放到队列中取，并且根据 FIFO 策略进行消息发送。 Follower 接收到 Proposal 后，会首先将其以事务日志的方式写入本地磁盘中，写入成功后向 Leader 反馈一个 Ack 响应消息。 Leader 接收到超过半数以上 Follower 的 Ack 响应消息后，即认为消息发送成功，可以发送 commit 消息。 Leader 向所有 Follower 广播 commit 消息，同时自身也会完成事务提交。Follower 接收到 commit 消息后，会将上一条事务提交。 数据同步 整个集群完成Leader选举之后，Learner（Follower和Observer的统称）回向Leader服务器进行注册。当Learner服务器向Leader服务器完成注册后，进入数据同步环节。 数据同步流程：（均以消息传递的方式进行） i. Learner向Learder注册 ii. 数据同步 iii. 同步确认 在进行数据同步前，Leader服务器会完成数据同步初始化： peerLastZxid：从learner服务器注册时发送的ACKEPOCH消息中提取lastZxid（该Learner服务器最后处理的ZXID） minCommittedLog：Leader服务器Proposal缓存队列committedLog中最小ZXID maxCommittedLog：Leader服务器Proposal缓存队列committedLog中最大ZXID Zookeeper的数据同步通常分为四类： 直接差异化同步（DIFF同步） 先回滚再差异化同步（TRUNC+DIFF同步） 仅回滚同步（TRUNC同步） 全量同步（SNAP同步） 直接差异化同步（DIFF同步） peerLastZxid介于minCommittedLog和maxCommittedLog之间 先回滚再差异化同步（TRUNC+DIFF同步） 这种场景是比较特殊的情况，简单来说就是，当Leader将事务提交到本地事务日志中后，正准备将proposal发送给其他的Follower进行投票时突然宕机，这个时候Zookeeper集群会选取出新的Leader对外服务，并且可能提交了几个事务，此后当老Leader再次上线，新Leader发现它身上有自己没有的事务，就需要回滚抹去老Leader上自己没有的事务，再让老Leader同步完自己新提交的事务，这就是TRUNC+DIFF的场景。那么就需要让该Learner服务器进行事务回滚–回滚到Leader服务器上存在的，同时也是最接近于peerLastZxid的ZXID 仅回滚同步（TRUNC同步） peerLastZxid 大于 maxCommittedLog 全量同步（SNAP同步） 场景一：peerLastZxid 小于 minCommittedLog 场景二：Leader服务器上没有Proposal缓存队列且peerLastZxid不等于lastProcessZxid zookeeper 中的节点类型 PERSISTENT-持久节点除非手动删除，否则节点一直存在于Zookeeper上 EPHEMERAL-临时节点临时节点的生命周期与客户端会话绑定，一旦客户端会话失效（客户端与zookeeper连接断开不一定会话失效），那么这个客户端创建的所有临时节点都会被移除。 PERSISTENT_SEQUENTIAL-持久顺序节点基本特性同持久节点，只是增加了顺序属性，节点名后边会追加一个由父节点维护的自增整型数字。 EPHEMERAL_SEQUENTIAL-临时顺序节点基本特性同临时节点，增加了顺序属性，节点名后边会追加一个由父节点维护的自增整型数字。 服务端宕机后 zk 发生的变化zookeeper本身也是集群，推荐配置不少于3个服务器。Zookeeper自身也要保证当一个节点宕机时，其他节点会继续提供服务。如果是一个Follower宕机，还有2台服务器提供访问，因为Zookeeper上的数据是有多个副本的，数据并不会丢失；如果是一个Leader宕机，Zookeeper会选举出新的Leader。ZK集群的机制是只要超过半数的节点正常，集群就能正常提供服务。只有在ZK节点挂得太多，只剩一半或不到一半节点能工作，集群才失效。所以3个节点的cluster可以挂掉1个节点(leader可以得到2票&gt;1.5)；2个节点的cluster就不能挂掉任何1个节点了(leader可以得到1票&lt;=1) 问题一：已经被处理的事务请求（proposal）不能丢（commit的） 当 leader 收到合法数量 follower 的 ACKs 后，就向各个 follower 广播 COMMIT 命令，同时也会在本地执行 COMMIT 并向连接的客户端返回「成功」。但是如果在各个 follower 在收到 COMMIT 命令前 leader 就挂了，导致剩下的服务器并没有执行都这条消息。 如何解决已经被处理的事务请求（proposal）不能丢（commit的）呢？ 选择拥有 proposal 最大值（即 zxid 最大） 的节点作为新的 Leader。 由于所有提案被COMMIT 之前必须有大多数量的 Follower ACK，即大多数服务器已经将该 proposal写入日志文件。因此，新选出的Leader如果满足是大多数节点中proposal最多的，它就必然存有所有被COMMIT消息的proposal。 新Leader与Follower 建立先进先出的队列， 先将自身有而Follower缺失的proposal发送给 它，再将这些 proposal的COMMIT命令发送给 Follower，这便保证了所有的Follower都保存了所有的 proposal、所有的Follower 都处理了所有的消息。 问题二：没被处理的事务请求（proposal）不能再次出现什么时候会出现事务请求被丢失呢？ 当 leader 接收到消息请求生成 proposal 后就挂了，其他 follower 并没有收到此 proposal，因此经过恢复模式重新选了 leader 后，这条消息是被跳过的。 此时，之前挂了的 leader 重新启动并注册成了 follower，他保留了被跳过消息的 proposal 状态，与整个系统的状态是不一致的，需要将其删除。 如何解决呢？ Zab 通过巧妙的设计 zxid 来实现这一目的。一个 zxid 是64位，高 32 是纪元（epoch）编号，每经过一次 leader 选举产生一个新的 leader，新 leader 会将 epoch 号 +1。低 32 位是消息计数器，每接收到一条消息这个值 +1，新 leader 选举后这个值重置为 0。这样一来旧的 leader 挂了后重启，它不会被选举为 leader，因为此时它的 zxid 肯定小于当前的新 leader。当旧的 leader 作为 follower 接入新的 leader 后，新的 leader 会让它将所有的拥有旧的 epoch 号的未被 COMMIT 的 proposal 清除。 Zookeeper有几种角色？Leader 事务请求的唯一调度和处理者，保证集群事务处理的顺序性 集群内部各服务的调度者 Follower 处理客户端的非事务请求，转发事务请求给Leader服务器 参与事务请求Proposal的投票 参与Leader选举投票 Observer 3.3.0版本以后引入的一个服务器角色，在不影响集群事务处理能力的基础上提升集群的非事务处理能力 处理客户端的非事务请求，转发事务请求给Leader服务器 不参与任何形式的投票 Zookeeper 下 Server工作状态服务器具有四种状态，分别是LOOKING、FOLLOWING、LEADING、OBSERVING。 LOOKING：寻找Leader状态。当服务器处于该状态时，它会认为当前集群中没有Leader，因此需要进入Leader选举状态。 FOLLOWING：跟随者状态。表明当前服务器角色是Follower。 LEADING：领导者状态。表明当前服务器角色是Leader。 OBSERVING：观察者状态。表明当前服务器角色是Observer。 ZooKeeper集群节点为什么要部署成奇数？ZooKeeper容错指的是：当宕掉几个ZooKeeper节点服务器之后，剩下的个数必须大于宕掉的个数，也就是剩下的节点服务数必须大于n/2，这样ZooKeeper集群才可以继续使用。对于节点数为5和6，他们允许宕机的节点数目是2，从节约资源的角度看，没必要部署6个也就是偶数个ZooKeeper服务节点。 Zookeeper集群节点宕机了怎么发现剔除的？ 为了保持客户端会话的有效性，在ZooKeeper的运行过程中，客户端会在会话超时时间过期范围内向服务端发送PING请求来保持会话的有效性，我们俗称“心跳检测”。 ZooKeeper通过内部心跳机制来确定Leader的状态，一旦Leader出现意外Zookeeper能很快获悉并且通知其他的follower。 Zookeeper的脑裂是什么？ 由于心跳超时（网络原因导致的）认为Leader死了，但其实Leader还存活着。由于假死会发起新的Leader选举，选举出一个新的Leader，但旧的Leader网络又通了，导致出现了两个Leader ，有的客户端连接到老的Leader，而有的客户端则连接到新的Leader。 怎么解决脑裂 ZooKeeper默认采用了Quorums这种方式来防止“脑裂”现象。也就是只有集群中超过半数节点投票才能选举出Leader。假设某个Leader假死，其余的followers选举出了一个新的Leader。这时，旧的Leader复活并且仍然认为自己是Leader，这个时候它向其他followers发出写请求也是会被拒绝的。因为每当新Leader产生时，会生成一个epoch标号（标识当前属于那个Leader的统治时期），这个epoch是递增的，followers如果确认了新的Leader存在，知道其epoch，就会拒绝epoch小于现任leader epoch的所有请求。 为什么用Zookeeper 做注册中心(优点，与其他选型对比下)Watcher 机制。客户端注册监听它关心的节点的变化，当节点发生变化时（如图中被删除的时候），Zookeeper会通知客户端。该机制是Zookeeper实现分布式协调的重要特性。我们可以通过get，exists，getchildren三种方式对某个节点进行监听。但是该事件只会通知一次。 在微服务中，服务提供方把服务注册到Zookeeper中心去如图中的Member服务，但是每个应用可能拆分成多个服务对应不同的Ip地址，Zookeeper注册中心可以动态感知到服务节点的变化。 服务消费方（Order 服务）需要调用提供方（Member 服务）提供的服务时，从Zookeeper中获取提供方的调用地址列表，然后进行调用。这个过程称为服务的订阅。 Zookeeper Watcher 机制 – 数据变更通知Zookeeper负载均衡和nginx负载均衡区别Zookeeper集群支持动态添加机器吗？其实就是水平扩容了，Zookeeper在这方面不太好。两种方式： 全部重启：关闭所有Zookeeper服务，修改配置之后启动。不影响之前客户端的会话。 逐个重启：在过半存活即可用的原则下，一台机器重启不影响整个集群对外提供服务。这是比较常用的方式。 3.5版本开始支持动态扩容。 先修改node1、2、3配置，重启 1ps: 最后在重启leader节点，会触发选举，默认会选择myid最大的为新leader，即node5 轮序重启后，原来node2：Mode: leader——&gt;Mode: follower；最后添加的node5：Mode: follower——&gt;Mode: leader 至此新增2个节点完成，现在集群一共5个节点 为什么使用zookeeper作为注册中心？在微服务中，服务提供方把服务注册到Zookeeper中心去，但是每个应用可能拆分成多个服务对应不同的Ip地址，Zookeeper注册中心可以动态感知到服务节点的变化。 服务消费方（Order 服务）需要调用提供方（Member 服务）提供的服务时，从Zookeeper中获取提供方的调用地址列表，然后进行调用。这个过程称为服务的订阅。 主要就是watcher机制。由于服务消费方向Zookeeper订阅了（监听）服务提供方，一旦服务提供方有变动的时候（增加服务或者减少服务），Zookeeper就会把最新的服务提供方列表（member list）推送给服务消费方，这就是服务动态发现的原理。 Zookeeper的典型应用场景 数据发布/订阅系统，即所谓的配置中心 使用发布/订阅模式，让多个服务器共同订阅一个目标，然后当目标将最新的配置文件发布的时候，所有的订阅者都能够收到最新的消息，进而达到动态获取数据的目的，实现配置信息的集中式管理和数据的动态更新。 假如有多个服务器都需要连接同一个数据库，我们可以让这多个数据库读取同一个配置文件，这样如果想要更换数据库的话只需要更改配置文件，然后让服务器使用新的配置信息重新连接就好。具体先在ZooKeeper中创建一个节点，然后将数据库的配置信息放在这个节点中，让其它的服务器监听这个节点的数据变化，当该节点的内容发生变化时就重新读取。 命名服务 命名服务是指通过指定的名字来获取资源或者服务的地址，提供者的信息。利用Zookeeper很容易创建一个全局的路径，而这个路径就可以作为一个名字，它可以指向集群中的集群，提供的服务的地址，远程对象等。简单来说使用Zookeeper做命名服务就是用路径作为名字，路径上的数据就是其名字指向的实体。 集群管理 集群机器监控：这通常用于那种对集群中机器状态，机器在线率有较高要求的场景，能够快速对集群中机器变化作出响应。这样的场景中，往往有一个监控系统，实时检测集群机器是否存活。过去的做法通常是：监控系统通过某种手段（比如ping）定时检测每个机器，或者每个机器自己定时向监控系统汇报“我还活着”。 这种做法可行，但是存在两个比较明显的问题： 集群中机器有变动的时候，牵连修改的东西比较多。 有一定的延时。 利用ZooKeeper有两个特性，就可以实时另一种集群机器存活性监控系统：a. 客户端在节点 x 上注册一个Watcher，那么如果 x 的子节点变化了，会通知该客户端。b. 创建EPHEMERAL类型的节点，一旦客户端和服务器的会话结束或过期，那么该节点就会消失。 Master选举 在分布式环境中，相同的业务应用分布在不同的机器上，有些业务逻辑（例如一些耗时的计算，网络I/O处理），往往只需要让整个集群中的某一台机器进行执行， 其余机器可以共享这个结果，这样可以大大减少重复劳动，提高性能，于是这个master选举便是这种场景下的碰到的主要问题。 利用ZooKeeper的强一致性，能够保证在分布式高并发情况下节点创建的全局唯一性，即：同时有多个客户端请求创建 /currentMaster 节点，最终一定只有一个客户端请求能够创建成功。 分布式协调/通知 ZooKeeper 中特有watcher注册与异步通知机制，能够很好的实现分布式环境下不同系统之间的通知与协调，实现对数据变更的实时处理。使用方法通常是不同系统都对 ZK上同一个znode进行注册，监听znode的变化（包括znode本身内容及子节点的），其中一个系统update了znode，那么另一个系统能 够收到通知，并作出相应处理。 分布式锁 分布式队列 它的实现方式也有两种，一种是FIFO（先进先出）的队列，另一种是等待队列元素聚集之后才统一安排的Barrier模型。 FIFO的队列 通过调用getChildren()接口来获取某一节点下的所有节点，即获取队列中的所有元素。 确定自己的节点序号在所有子节点中的顺序。 如果自己不是序号最小的子节点，那么就需要进入等待，同时向比自己序号小的最后一个节点注册Watcher监听。 接收到Watcher通知后，重复步骤1。 Barrier模型 创建完节点以后,根据如下5个步骤来确定执行顺序. 通过调用getData()接口获取/queue_babarrier节点的数据内容:10 通过调用getchildren()接口获取/queue_babarrier节点下的所有子节点,即获取队列中的所有元素,同时注册对子节点变更的watcher监听. 统计子节点的个数. 如果子节点的个数还不足10个,那么就需要进入等待. 接到watcher通知后,重复步骤2 Dubbo用的什么RPC框架，原理，协议序列化协议：fastjson、也是用HessionSerializer 传输协议：dubbo 接口调用如何保证幂等SELECT col1 FROM tab1 WHER col2=2，无论执行多少次都不会改变状态，是天然的幂等。UPDATE tab1 SET col1=1 WHERE col2=2，无论执行成功多少次状态都是一致的，因此也是幂等操作。UPDATE tab1 SET col1=col1+1 WHERE col2=2，每次执行的结果都会发生变化，这种不是幂等的。insert into user(userid,name) values(1,’a’) 如userid为唯一主键，即重复操作上面的业务，只会插入一条用户数据，具备幂等性。如userid不是主键，可以重复，那上面业务多次操作，数据都会新增多条，不具备幂等性。 接口幂等性是指用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生了副作用。 使用乐观锁，版本号。也就是在调用接口之前查询版本号，然后调用接口的时候进行校验。 下游传递一个唯一的序列号。每次向服务端请求的时候附带一个短时间内唯一的不重复的序列号。通过setnx设置，如果设置成功，就能正常执行对应的业务。否则的话，说明已经处理该业务了。 具体的设置信息：使用redis缓存，通过将当前账户的ID、当前订单的ID以及操作的类型组成一个唯一的key，通过setnx设置以及过期时间60ms。value就是是”lock” 状态机：调用接口之前判断页面的状态，比如说如果是审核状态就不能再审核了。 setex key seconds value：将key值设置为value，并将设置key的生存周期 属于原子操作，作用和set key value、expire key seconds作用一致。 如果key值存在，使用setex将覆盖原有值 setnx key value：只有当key不存在的情况下，将key设置为value；若key存在，不做任何操作，结果成功返回1，失败返回0 RPC与HTTP的区别 HTTP 指的是通信协议。而 RPC 则是远程调用，其对应的是本地调用。RPC 的通信可以用 HTTP 协议，也可以自定义协议，是不做约束的。 第一层：物理层 第二层：数据链路层 802.2、802.3ATM、HDLC、FRAME RELAY 第三层：网络层 IP、IPX、APPLETALK、ICMP 第四层：传输层 TCP、UDP、SPX 第五层：会话层 RPC、SQL、NFS 、X WINDOWS、ASP 第六层：表示层 ASCLL、PICT、TIFF、JPEG、 MIDI、MPEG 第七层：应用层 HTTP、FTP、SNMP等 Dubbo的主要角色dubbo是一款高性能、轻量级的开源RPC框架，它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。 服务提供者（Provider）：暴露服务的服务提供方，服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者（Consumer）: 调用远程服务的服务消费方，服务消费者在启动时，向注册中心订阅自己所需的服务，服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 注册中心（Registry）：注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者 监控中心（Monitor）：服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心 dubbo的负载均衡策略？负载均衡的权重是什么 一致性Hash均衡算法、随机调用法、轮询法、最少活动调用法 随机调用负载均衡，加权随机算法负载均衡策略（RandomLoadBalance）是 dubbo 负载均衡的默认实现方式，根据权重分配各个 Invoker 随机选中的比例。也就是将Invoker 列表中的权重进行求和，然后求出单个 Invoker 权重在总权重中的占比，随机数就在总权重值的范围内生成。 在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。 加权轮询调用，对服务器轮询过程进行加权，以调控每台服务器的负载。经过加权后，每台服务器能够得到的请求数比例，接近或等于他们的权重比。比如服务器 A、B、C 权重比为 5:2:1。那么在8次请求中，服务器 A 将收到其中的5次请求，服务器 B 会收到其中的2次请求，服务器 C 则收到其中的1次请求。 一个map存储invoker以及对应的weight。 在循环里面获取最大权重值、最小权重值以及计算权重值之和 计算方法调用的次数(每调用一次+1) ==&gt; sequence 每次对方法的sequence对权重和取余得到一个(0, weightSum)范围内的值 ==&gt; mod 如果mod等于0并且当前invoker的权重大于0，那么就返回这个invoker；否则的话，mod–并且将当前这个invoker的权重减一，并且切换到下一个invoker 12345678910111213141516171819202122232425262728// 获取当前的调用编号int currentSequence = sequence.getAndIncrement();// 如果最小权重小于最大权重，表明服务提供者之间的权重是不相等的if (maxWeight &gt; 0 &amp;&amp; minWeight &lt; maxWeight) &#123; // 使用调用编号对权重总和进行取余操作 int mod = currentSequence % weightSum; // 进行 maxWeight 次遍历 for (int i = 0; i &lt; maxWeight; i++) &#123; // 遍历 invokerToWeightMap for (Map.Entry&lt;Invoker&lt;T&gt;, IntegerWrapper&gt; each : invokerToWeightMap.entrySet()) &#123; // 获取 Invoker final Invoker&lt;T&gt; k = each.getKey(); // 获取权重包装类 IntegerWrapper final IntegerWrapper v = each.getValue(); // 如果 mod = 0，且权重大于0，此时返回相应的 Invoker if (mod == 0 &amp;&amp; v.getValue() &gt; 0) &#123; return k; &#125; // mod != 0，且权重大于0，此时对权重和 mod 分别进行自减操作 if (v.getValue() &gt; 0) &#123; v.decrement(); mod--; &#125; &#125; &#125;&#125; 这个方法存在一个问题，主要是在**mod == 0 &amp;&amp; v.getValue() &gt; 0**这句话，如果总的权重很大，导致mod很大。比如说mod等于10W，那么需要循环很多次才能将mod减为0。 假设每个服务器有两个权重，一个是配置的weight,不会变化，一个是currentWeight会动态调整，初始值为0。当有新的请求进来时，遍历服务器列表，让它的currentWeight加上自身权重。遍历完成后，找到最大的currentWeight，并将其减去权重总和，然后返回相应的服务器即可。 少活跃数调用法：每个服务提供者对应一个活跃数 active，活跃调用数越小，表明该服务提供者效率越高，单位时间内可处理更多的请求。此时应优先将请求分配给该服务提供者。 遍历 invokers 列表，寻找活跃数最小的 Invoker ( 如果当前的活跃数更小，重新设置leastCount、leastIndex、sameWeight；如果相同的话，记下这些 Invoker 在 invokers 集合中的下标、累加上这个invoker的权重，并且判断一下权重是不是和之前的相同 ) 如果只有一个 Invoker 具有最小的活跃数，此时直接返回该 Invoker 即可 如果有多个 Invoker 具有最小活跃数，且它们的权重不相等，此时处理方式和 RandomLoadBalance 一致 如果有多个 Invoker 具有最小活跃数，但它们的权重相等，此时随机返回一个即可 一致性Hash算法，Dubbo 会对 URL 中的参数进行 hash ， Dubbo 默认除了对我们接口所有参数进行 hash 外，还会加上这些额外参数，比如 timestamp 、 loadbalance 、 pid 和 application 等 先看看服务器的数量有没有变 到TreeMap中查找第一个节点值大于或等于当前hash的Invoker dubbo的负载均衡策略最终都是在哪里实现的？dubbo的注册中心挂了还能消费吗因为当启动dubbo容器时，消费者会去zookeeper拉取注册的生产者地址等信息，注册一个监听器，并将其缓存在本地。每次发起调用时，都会按照本地的地址列表，以负载均衡的策略去进行调用。但是如果zookeeper挂掉了，则后续新的生产者无法被消费者发现。 谈谈Dubbo底层的通信原理 Dubbo底层通信，默认采用高性能的Netty通信框架，实现网络通信，Netty是一个高性能的NIO通信框架，所以聊到底层原理，就要从NIO的特点来聊起。 NIO是如何做到多路复用机制的，以及Netty在此基础之后，又增加了主从线程池来进行优化，结合这两方面进行讲解即可。 http 和 dubbo 协议的区别 dubbo 协议适合大并发小数据量(每次请求在 100kb 以内)的服务调用 http1.0协议不支持传文件，只适用于同时给应用程序和浏览器JS调用 dubbo默认使用socket长连接，即首次访问建立连接以后，后续网络请求使用相同的网络通道 http1.0协议默认使用短连接，每次请求均需要进行三次握手，而http2.0协议开始将默认socket连接改为了长连接(keep-alive) dubbo容错机制为了避免单点故障，现在的应用通常至少会部署在两台服务器上。对于一些负载比较高的服务，会部署更多的服务器。这样，在同一环境下的服务提供者数量会大于1。对于服务消费者来说，同一环境下出现了多个服务提供者。这时会出现一个问题，服务消费者需要决定选择哪个服务提供者进行调用。另外服务调用失败时的处理措施也是需要考虑的，是重试呢，还是抛出异常，亦或是只打印异常等。为了处理这些问题，Dubbo 定义了集群接口 Cluster 以及 Cluster Invoker。 集群 Cluster 用途是将多个服务提供者合并为一个 Cluster Invoker，并将这个 Invoker 暴露给服务消费者。这样一来，服务消费者只需通过这个 Invoker 进行远程调用即可，至于具体调用哪个服务提供者，以及调用失败后如何处理等问题，现在都交给集群模块去处理。集群模块是服务提供者和服务消费者的中间层，为服务消费者屏蔽了服务提供者的情况，这样服务消费者就可以专心处理远程调用相关事宜。比如发请求，接受服务提供者返回的数据等。这就是集群的作用。 失败自动切换机制是由 FailoverClusterInvoker 类控制。在调用失败时，会自动切换服务提供者信息进行重试。通常用于读操作，但重试会带来更长延迟。默认配置下，Dubbo 会使用这种机制作为缺省集群容错机制。首先是获取重试次数，然后根据重试次数进行循环调用，失败后进行重试。在循环内，首先是通过负载均衡组件选择一个 Invoker，然后再通过这个 Invoker 的 invoke 方法进行远程调用。如果失败了，记录下异常，并进行重试。 失败自动恢复会在调用失败后，返回一个空结果给服务消费者。并通过定时任务对失败的调用进行重试，适合执行消息通知等操作。定时任务是一个循环。 快速失败只会进行一次调用，失败后立即抛出异常。适用于幂等操作，比如新增记录。 失败安全当调用过程中出现异常时，仅会打印异常，而不会抛出异常。适用于写入审计日志等操作。 并行调用多个服务提供者会在运行时通过线程池创建多个线程，并发调用多个服务提供者。只要有一个服务提供者成功返回了结果，doInvoke 方法就会立即结束运行。ForkingClusterInvoker 的应用场景是在一些对实时性要求比较高读操作（注意是读操作，并行写操作可能不安全）下使用，但这将会耗费更多的资源。 广播调用的处理操作会逐个调用每个服务提供者，如果其中一台报错，再循环调用结束后，BroadcastClusterInvoker 会抛出异常。该类通常用于通知所有提供者更新缓存或日志等本地资源信息。 Dubbo为什么推荐基于随机的负载均衡？ 在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。 dubbo 的基本工作原理 service层，接口层，给服务提供者和消费者来实现的 config层，配置层，主要是对dubbo进行各种配置的 registry层，服务注册层，负责框架的服务注册与发现 proxy层，代理层，Dubbo的生产者和消费者都会生成proxy，用来调用远程接口。使得调用透明 protocol层，远程调用层，封装具体 rpc 调用的过程 exchange层，信息交换层，封装请求响应模式，同步转异步 serialize层，数据序列化层，负责网络传输的序列化和反序列化 transport层，网络传输层，抽象mina和netty为统一接口 cluster层，集群层，主要负责远程调用的容错策略、负载均衡策略以及路由策略 monitor层，监控层，对rpc接口的调用次数和调用时间进行监控 消费者启动的步骤(服务的引入) 检查并更新配置,初始化接口代理类 加载各种配置到map,基于map中的配置构建proxy。在这里获取了消费者的ip地址、调用的方法、保存注册到注册表中的IP地址，创建代理对象等操作。 创建服务代理类，如果是远程调用的话，加载注册表配置并将其转换为registryURL，如果只有一个registryUrl，使用对应的Protocol SPI的refer方法拓展点实现得到invoker 在refer方法中，根据url#protocol找到对应的的spi实现，获取注册中心实例,先从缓存中获取,如果没有则创建注册中心实例 然后调用doRefer方法。生成consumerURL，将consumerURL注册到注册中心，因为使用的Zookeeper，所以zkClient.create(toUrlPath(url), url.getParameter(DYNAMIC_KEY, true))其实就是创建一个临时节点 接着构建路由规则链 监听providers,routers.configurators节点数据，当节点数据发生变化动态更新invoker列表 获取url中的configurators信息，configurators是外部化配置信息，包含服务者动态配置 URL 元数据信息； 获取url中的routers信息， routers是路由配置信息，包含消费者路由策略 URL 元数据信息； 获取url中的providers信息，providers是服务提供者注册信息，包含服务者 URL 元数据信息,对overrideDirectoryUrl配置信息覆盖,刷新invoker列表 刷新incoker方法中，将providerUrl转换成invoker，并将刷新后的invoker列表设置到routerChain中 创建invoker，如果可用,创建InvokerDelegate包裹Invoker。在创建invoker的过程中，是通过netty与注册中心连接，用集群容错策略包装invoker，默认集群容错策略:FailoverCluster(失败重试)，MockClusterWrapper会对Cluster SPI实现增强,提供服务降级能力 首先检查、加载、更新配置，在配置中主要获取了消费者的IP地址、调用的类、方法以及保存到注册表中的IP地址。然后根据配置中的信息构建代理对象。在具体创建代理对象的时候，是先根据注册表的配置获取registryUrl，使用对应的Protocol实现类得到相应的invoker。在创建invoker的时候，会根据配置信息生成consumerURL并注册到注册中心(创建一个临时节点)，监听相应的provider节点。默认具体是使用FailOverCluster创建invoker，将创建的invoker保存到invoker列表中。 消费者怎么知道调用的服务者是哪个节点的 命名服务+负载均衡。 获取具体的URL。在进行服务引用的时候，发送消费者url到注册中心进行订阅服务、设置对提供者的监视器的时候，先获取订阅的服务的providers、routers、configurators的路径(/dubbo/org.apache.dubbo.demo.RemoteService/providers)，这个路径是消费者url上的属性。然后通过这些url创建监听器监听数据变化，在notify方法里获取到providerURL，并刷新invoke列表 命名服务 命名服务是指通过指定的名字来获取资源或者服务的地址，提供者的信息。利用Zookeeper很容易创建一个全局的路径，而这个路径就可以作为一个名字，它可以指向集群中的提供的服务的地址，远程对象等。简单来说使用Zookeeper做命名服务就是用路径作为名字，路径上的数据就是其名字指向的实体。 阿里巴巴集团开源的分布式服务框架Dubbo中使用ZooKeeper来作为其命名服务，维护全局的服务地址列表。在Dubbo实现中：服务提供者在启动的时候，向ZK上的指定节点/dubbo/${serviceName}/providers目录下写入自己的URL地址，这个操作就完成了服务的发布。服务消费者启动的时候，订阅/dubbo/&#123;serviceName&#125;/providers目录下的提供者URL地址， 并向/dubbo/&#123;serviceName&#125;/consumers目录下写入自己的URL地址。 注意，所有向ZK上注册的地址都是临时节点，这样就能够保证服务提供者和消费者能够自动感应资源的变化。另外，Dubbo还有针对服务粒度的监控，方法是订阅/dubbo/&#123;serviceName&#125;目录下所有提供者和消费者的信息。 之后发起rpc请求的步骤(服务请求的过程) 首先消费者启动的时候，有一个服务引用的步骤，根据配置信息生成consumerURL并注册到注册中心(创建一个临时节点)，监听相应的provider节点，创建invoker并保存到invoker列表中。之后在进行服务调用时生成一个代理类，代理类会先生成一个 RpcInvocation 对象然后调用invoke方法。在默认情况下， 实际上是FailoverClusterInvoker 拿到 Directory 返回的 Invoker 列表，并且经过路由过滤，负载均衡从 Invoker 列表中选择一个 Invoker 实例去执行invoke方法，参数为Invocation(Invocation就是之前调用的方法的信息)。接着信息交换层会包装生成一个Request请求，在这里面有一个全局唯一 ID，然后 defaultFuture 将自己和 ID 存储到一个 ConcurrentHashMap。通过netty发送到服务端之后，服务端也会把这个 ID 返回来，这样通过这个 ID 再ConcurrentHashMap 里面就可以找到对应的 future，并返回了。 调用的方式oneway还是很常见的，就是当你不关心你的请求是否发送成功的情况下，就用 oneway 的方式发送，这种方式消耗最小，啥都不用记，啥都不用管。 异步调用，其实 Dubbo 天然就是异步的，可以看到 client 发送请求之后会得到一个 ResponseFuture，然后把 future 包装一下塞到上下文中，这样用户就可以从上下文中拿到这个 future，然后用户可以做了一波操作之后再调用 future.get 等待结果。 同步调用，这是我们最常用的，也就是 Dubbo 框架帮助我们异步转同步了，从代码可以看到在 Dubbo 源码中就调用了 future.get，所以给用户的感觉就是我调用了这个接口的方法之后就阻塞住了，必须要等待结果到了之后才能返回，所以就是同步的。 Dubbo 本质上就是异步的，为什么有同步就是因为框架帮我们转了一下，而同步和异步的区别其实就是future.get 在用户代码被调用还是在框架代码被调用。 生产者启动的步骤(服务暴露的过程) 真正进行服务暴露的入口点在ServiceConfig的doExport方法中，在这个方法里面，首先获取当前服务对应的注册中心实例，将暴露服务的信息(比如服务的接口、实现类、方法)封装到ServiceDescription中，调用doExportUrlsFor1Protocol，在这个方法里首先根据配置构建生产者的url。这个URL表示它是一个dubbo协议(DubboProtocol)的服务提供者url，地址是当前服务器的ip，端口是要暴露的服务的端口号。先经过一次本地暴露，本地暴露使用的是injvm 协议，重新创建一个新的 URL。然后创建invoke，调用InjvmProtocol进行 export，将生成的exporter加入到缓存中。最后进行远程暴露，首先将registry协议的url和dubbo协议的url组装在一起，然后封装成Invoker(invoker就是将服务类指向了一个url)。接着调用具体的protocol的export方法将服务暴露出来。在这个方法里面，首先调用具体的protocol的export方法将服务暴露，然后获取注册中心的url，根据url加载Registry的实现类，将export注册到服务提供者和消费者的注册表中向注册中心注册服务，再封装成DestoryableExporter返回。 生产者处理请求的步骤 NettyServer 收到客户端连接，完成TCP 三次握手 客户端发送 请求(RECEIVED) 经由NettyServer 中构建的处理器责任链处理, 复合消息拆分处理 心跳事件处理 根据线程模型分发请求 对消息进行解码, 解码，对消息进行分发处理后交由后续处理器处理 从inv中获取 服务端口 等信息,从channel中获取调用的端口,组装serviceKey ,找到缓存在exporterMap的DubboExporter,进而获取服务实现类代理对象Invoker 调用代理类,处理请求,根据返回类型组装相应的Result 如果请求需要返回值,回写响应 消费者自己宕机了, 没法自己通知configserver和zookeeper,，只能通过心跳机制 那是你必须知道的，至少要知道 dubbo 分成哪些层，然后平时怎么发起 rpc 请求的，注册、发现、调用，这些是基本的。 dubbo 支持哪些通信协议？ dubbo协议：单一TCP长连接和NIO异步通讯，适合大并发小数据量(每次请求在 100kb 以内)的服务调用，以及服务消费者远大于提供者的情况，使用Hessian二进制序列化。 缺点是不适合传送大数据包的服务 rmi协议：采用JDK标准的rmi协议实现，传输参数和返回参数对象需要实现Serializable接口。使用java标准序列化机制，使用阻塞式短连接，传输数据包不限，适用于消费者和提供者个数相当的情况。多个短连接，TCP协议传输，同步传输，适用常规的远程服务调用和rmi互操作。 缺点是在依赖低版本的Common-Collections包，java反序列化存在安全漏洞，需升级commons-collections3 到3.2.2版本或commons-collections4到4.1版本。 webservice协议：基于WebService的远程调用协议(Apache CXF的frontend-simple和transports-http)实现，提供和原生WebService的互操作使用的是 SOAP 文本序列化，并且是多个短连接，基于HTTP传输，同步传输，适用系统集成和跨语言调用。 http协议：基于Http表单提交的远程调用协议，使用Spring的HttpInvoke实现。对传输数据包不限，传入参数大小混合，提供者个数多于消费者 缺点是不支持传文件，只适用于同时给应用程序和浏览器JS调用 hessian协议：使用的是hessian 序列化协议，多个短连接，适用于提供者数量比消费者数量还多的情况，适用于传输大数据包(可传文件) 缺点是参数及返回值需实现Serializable接口，多连接的，自定义实现List、Map、Number、Date、Calendar等接口 thrift协议：对thrift原生协议的扩展添加了额外的头信息使用较少，不支持传null值 memcache：基于memcached实现的RPC协议 redis：基于redis实现的RPC协议 为啥默认使用单一连接因为服务的现状大都是服务提供者少，通常只有几台机器，而服务的消费者多，可能整个网站都在访问该服务，比如 Morgan 的提供者只有 6 台提供者，却有上百台消费者，每天有 1.5 亿次调用，如果采用常规的 hessian 服务，服务提供者很容易就被压跨，通过单一连接，保证单一消费者不会压死提供者，并且一般使用长连接，能够减少连接握手验证，再加上使用异步 IO，复用线程池，提高系统的吞吐量防止 C10K 问题。 支持哪些序列化协议 hessian2：hessian是一种跨语言的高效二进制序列化方式。但这里实际不是原生的hessian2序列化，而是阿里修改过的hessian lite，它是dubbo RPC默认启用的序列化方式 相比hessian1，hessian2中增加了压缩编码，其**序列化二进制流大小是Java序列化的50%，序列化耗时是Java序列化的30%，反序列化耗时是Java序列化的20%。** jdk：JDK自带的Java序列化实现。 fastjson：依赖阿里的fastjson库，功能强大(支持普通JDK类包括任意Java Bean Class、Collection、Map、Date或enum) fst：完全兼容JDK序列化协议的系列化框架，序列化速度大概是JDK的4-10倍，大小是JDK大小的1/3左右。 kryo：是一个快速序列化/反序列化工具，其使用了字节码生成机制（底层依赖了 ASM 库），因此具有比较好的运行速度，速度快，序列化后体积小，跨语言支持较复杂 为啥不用jdk自带的java序列化实现？ 无法跨语言，Java序列化目前只支持Java语言实现的框架 易被攻击， 序列化后的字节流大，影响整个系统的吞吐量 序列化性能太差，影响网络通信的效率 说一下 Hessian 的数据结构？Hessian 的对象序列化机制有 8 种原始类型： 原始二进制数据 boolean 64-bit date（64 位毫秒值的日期） 64-bit double 32-bit int 64-bit long null UTF-8 编码的 string 另外还包括 3 种递归类型： list for lists and arrays map for maps and dictionaries object for objects还有一种特殊的类型： ref：用来表示对共享对象的引用。 spi相关java实现spi的原理 当服务的提供者提供了一种接口的实现之后，需要在 Classpath 下的 META-INF/services/ 目录里创建一个以服务接口命名的文件，此文件记录了该 jar 包提供的服务接口的具体实现类。当某个应用引入了该 jar 包且需要使用该服务时，JDK SPI 机制就可以通过查找这个 jar 包的 META-INF/services/ 中的配置文件来获得具体的实现类名，进行实现类的加载和实例化，最终使用该实现类完成业务功能。 spi是懒加载的，首先通过serviceLoader获取到接口相应的classsloader。然后创建一个迭代器，通过遍历迭代器，在调用hasNext() 方法的时候，按照行读取出配置文件中的内容，并保存到lazyIterator的一个iterator中。调用iterator的next方法，实际调用了nextService方法，在这里使用了反射的技术，根据前面从文件中读取到的实现类全路径获取到该实现类的对象 dubbo实现spi的原理 Dubbo的传输协议 dubbo默认的传输协议是Dubbo，它是基于TCP传输，单一长连接的，并且是NIO异步通信的，使用Hession二进制序列化。 适合于小数据量（每次请求在100kb以内）大并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况。比较常见的dubbo协议是使用netty完成报文发送和接收的。 netty的通信 Dubbo缺省协议不适合传送大数据量的服务，比如传文件，传视频等，除非请求量很低。 RpcContextRpcContext 是一个 ThreadLocal 的临时状态记录器，当接收到 RPC 请求，或发起 RPC 请求时，RpcContext 的状态都会变化。比如：A 调 B，B 再调 C，则 B 机器上，在 B 调 C 之前，RpcContext 记录的是 A 调 B 的信息，在 B 调 C 之后，RpcContext 记录的是 B 调 C 的信息。 服务熔断服务熔断：当下游的服务因为某种原因突然变得不可用或响应过慢，上游服务为了保证自己整体服务的可用性，不再继续调用目标服务，直接返回，快速释放资源。如果目标服务情况好转则恢复调用。 需要说明的是熔断其实是一个框架级的处理，那么这套熔断机制的设计，基本上业内用的是断路器模式，如Martin Fowler提供的状态转换图如下所示 最开始处于closed状态，一旦检测到错误到达一定阈值，便转为open状态； 这时候会有个 reset timeout，到了这个时间了，会转移到half open状态； 尝试放行一部分请求到后端，一旦检测成功便回归到closed状态，即恢复服务； dubbo是怎么处理服务熔断的？ 服务降级Dubbo 本身就提供了服务降级的机制；而 Dubbo 的服务降级机制主要是利用服务消费者的 mock 属性。 Dubbo的线程池面试题：服务端和消费端都各有一个线程池。一个服务消费方可能会并发调用多个服务提供者，每个用户线程发送请求后，会进行超时时间内的等待。多个服务提供者可能同时做完业务，然后返回，服务消费方的线程池会收到多个响应对象。这个时候要考虑一个问题，如何将线程池里面的每个响应对象传递给相应等待的用户线程，且不出错呢？ 具体在invoker调用invoke方法里，构建请求，发出请求之前构建了一个DefaultFuture。DefaultFuture 被创建时，会传入一个 Request 对象。此时 DefaultFuture 可从 Request 对象中获取调用id，并将 &lt;调用id, DefaultFuture 对象&gt; 映射关系存入到ConcurrentHashMap 中。 用户线程发出请求后，因为是同步的，所以会在AsyncRpcResult对象的get方法中阻塞进行等待。 当接收到服务端的响应后，响应事件会被放到线程池里面。执行的任务就是往队列中添加任务。当队列中有任务后，就可以从队列中获取任务，用户线程执行任务。具体是获取到相应的id，从 DefaultFuture 对象中获取调用结果了。 集合集合遍历的几种方式总结及比较 for需要知道集合或数组的大小，而且需要是有序的，不然无法遍历； foreach和iterator都不需要知道集合或数组的大小，他们都是得到集合内的每个元素然后进行处理； for和foreach都需要先知道集合的类型，甚至是集合内元素的类型，即需要访问内部的成员，不能实现多态； iterator是一个接口类型，他不关心集合或者数组的类型，而且他还能随时修改和删除集合的元素，举个例子： 12345public void display（Iterator&lt;object&gt; it）&#123; while(it.hasNext())&#123; system.out.print(it.next()+&quot;&quot;); &#125;&#125; 当我们需要遍历不同的集合时，我们只需要传递集合的iterator（如arr.iterator()）看懂了吧，这就是iterator的好处，他不包含任何有关他所遍历的序列的类型信息，能够将遍历序列的操作与序列底层的结构分离。迭代器统一了对容器的访问方式。这也是接口的解耦的最好体现。 iterator在遍历过程中能删除，for和foreach不能删除 遍历集合的方式集合类的通用遍历方式， 用迭代器迭代 1234Iterator it = list.iterator();while(it.hasNext()) &#123; Object obj = it.next();&#125; Map遍历方式： 1.通过获取所有的key按照key来遍历 1234//Set&lt;Integer&gt; set = map.keySet(); //得到所有key的集合for (Integer in : map.keySet()) &#123; String str = map.get(in);//得到每个key多对用value的值&#125; 2.通过Map.entrySet使用iterator遍历key和value 12345Iterator&lt;Map.Entry&lt;Integer， String&gt;&gt; it = map.entrySet().iterator();while (it.hasNext()) &#123; Map.Entry&lt;Integer， String&gt; entry = it.next(); System.out.println(&quot;key= &quot; + entry.getKey() + &quot; and value= &quot; + entry.getValue());&#125; 3.通过Map.entrySet遍历key和value，推荐，尤其是容量大时 123456for (Map.Entry&lt;Integer， String&gt; entry : map.entrySet()) &#123; //Map.entry&lt;Integer，String&gt; 映射项（键-值对） 有几个方法：用上面的名字entry //entry.getKey() ;entry.getValue(); entry.setValue(); //map.entrySet() 返回此映射中包含的映射关系的 Set视图。 System.out.println(&quot;key= &quot; + entry.getKey() + &quot; and value= &quot; + entry.getValue());&#125; 4.通过Map.values()遍历所有的value，但不能遍历key 123for (String v : map.values()) &#123; System.out.println(&quot;value= &quot; + v);&#125; List遍历方式： 第一种：通过list.iterator() 12345Iterator iterator = list.iterator();while(iterator.hasNext())&#123; int i = (Integer) iterator.next(); System.out.println(i);&#125; 第二种：通过foreach 123for (Object object : list) &#123; System.out.println(object); &#125; 第三种：通过for循环遍历 1234for(int i = 0 ;i&lt;list.size();i++) &#123; int j= (Integer) list.get(i); System.out.println(j); &#125; foreach 遍历ArrayList 和LinkedList 的效率iterator最慢，那两个类似 ConcurrentHashMap实现 当执行put方法插入数据时，根据key的hash值，在Node数组中找到相应的位置 根据 key 计算出 hashcode 判断是否需要进行初始化 f 即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功 如果当前位置的 hashcode == MOVED == -1，则需要进行扩容 如果都不满足，则利用 synchronized 锁写入数据 如果数量大于 TREEIFY_THRESHOLD 则要转换为红黑树 整个扩容操作分为两个部分 第一部分是构建一个nextTable，它的容量是原来的两倍，这个操作是单线程完成的。这个单线程的保证是通过RESIZE_STAMP_SHIFT这个常量经过一次运算来保证的，这个地方在后面会有提到； 第二个部分就是将原来table中的元素复制到nextTable中，这里允许多线程进行操作。 ConcurrentHashMap里的size()方法是如何获得Map的长度的 ConcurrentHashMap的size()方法都是线程安全的，都是准确的计算出实际的数量，但是这个数据在并发场景下是随时都在变的。使用size()获取ConcurrentHashMap的长度，最大值是Integer.MAX_VALUE；使用mappingCount，返回的是long类型，最大值更大。 无论是size()方法还是mappingCount()方法，核心方法都是sumCount()方法。当counterCells为空时直接返回baseCount，当counterCells不为空时遍历它并垒加到baseCount中。baseCount是一个volatile变量，那么我们来看在put()方法执行时是如何使用baseCount的，在put方法的最后一段代码中会调用addCount()方法。首先对baseCount做CAS自增操作。如果并发导致了baseCount的CAS失败了，则使用counterCells进行CAS。如果counterCells的CAS也失败了，那么则进入fullAddCount()方法，fullAddCount()方法中会进入死循环，直到counterCells的CAS添加成功为止。 1234567891011final long sumCount() &#123; CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value; &#125; &#125; return sum; &#125; counterCells是一个使用了 @sun.misc.Contended 标记的类，内部一个 volatile 变量。@sun.misc.Contended 这个注解是为了防止“伪共享”。 那么什么是伪共享呢？ 缓存系统中是以缓存行（cache line）为单位存储 的。缓存行是2的整数幂个连续字节，一般为32-256个字节。最常见的缓存行大小是64个字节。当多线程修改互相独立的变量时，如果这些变量共享同一个缓存行，就会无意中影响彼此的性能，这就是伪共享。所以伪共享对性能危害极大。 原理是在使用此注解的对象或字段的前后各增加128字节大小的padding，使用2倍于大多数硬件缓存行的大小来避免相邻扇区预取导致的伪共享冲突。 两个属性 ： baseCount 和 counterCells。 baseCount是一个 volatile 的变量，在 addCount 方法中会使用它，而 addCount 方法在 put 结束后调用。 在 addCount 方法中，首先判断countCells是否已经被初始化，如果没有被初始化，那么将尝试在size的更新操作放在baseCount上。如果此时没有冲突，那么CAS修改baseCount就能成功，size的更新就落在了baseCount上。如果此时已经有countCells了，那么会根据线程的探针随机落到countCells的某个下标上。对size的更新就是CAS更新对应CountCells的value值。如果并发导致 CAS 失败了，使用 counterCells。具体是在fullAddCount方法中，自旋重试直到更新成功。 这里说一下我个人对ConcurrentHashMap采用这么复杂的方式进行计数的理解？因为ConcurrenthHashMap是出于吞吐量最大的目的设计的，因此，如果单纯的用一个size直接记录元素的个数，那么每次增删操作都需要同步size，这会让ConcurrentHashMap的吞吐量大大降低。因为，将size分散成多个部分，每次修改只需要对其中的一部分进行修改，可以有效的减少竞争，从而增加吞吐量。 concurrentHashmap的resize 判断是否需要进行resize。具体是通过size是否大于等于sizeCtl进行判断。在进行resize的过程中，首先，最先发起扩容的线程需要对数组进行翻倍，并设置一个transferIndex变量，初始值为旧数组的容量n。设置一个步长stride，线程从后往前依次领取一个部分，当线程认领的一个步长的任务完成后，继续去认领下一个步长，直到transferIndex &lt; 0。针对每个桶的rehash，首先，如果桶上没有元素或是桶上的元素是ForwardingNode，说明不用处理该桶，继续处理上一个桶。对于桶上存放正常的节点而言，为了线程安全，需要对桶的头节点进行上锁，以链表为例，如果旧数组容量为oldCap，且节点之前在旧数组的下标为i，那么rehash链表中的所有节点将放在nextTable[i]或者nextTable[i+oldCap]的桶上。当一个桶的元素被transfer完成后，旧数组相关位置上会被放上ForwardingNode的特殊节点表示该桶已经被迁移过。 如何重写hashcode？HashMap Student， String。键：Student 包括name和age。要求：如果两个对象的成员变量值都相同，则为同一个对象 123456789101112131415161718192021222324252627@Overridepublic int hashCode() &#123; final int prime = 31; int result = 1; result = prime * result + age; result = prime * result + ((name == null) ? 0 : name.hashCode()); return result;&#125;@Overridepublic boolean equals(Object obj) &#123; if (this == obj) return true; if (obj == null) return false; if (getClass() != obj.getClass()) return false; Student other = (Student) obj; if (age != other.age) return false; if (name == null) &#123; if (other.name != null) return false; &#125; else if (!name.equals(other.name)) return false; return true;&#125; HashSet实现 HashMap扩容为什么要扩2倍 StringBuilder和StringBuffer的差别 红黑树的特点二叉查找树（BST）具备什么特性呢？ 1.左子树上所有结点的值均小于或等于它的根结点的值。 2.右子树上所有结点的值均大于或等于它的根结点的值。 3.左、右子树也分别为二叉排序树。 红黑树的特点 节点是红色或黑色。 根节点是黑色。 每个叶子节点都是黑色的空节点（NIL节点）。 每个红色节点的两个子节点都是黑色。(从每个叶子到根的所有路径上不能有两个连续的红色节点) 从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。 保证了从根到叶子的最长路径不会超过最短路径的两倍 LinkedHashMap怎么保证有序？默认是按照插入顺序进行排序的。还有一种是按照访问顺序进行排序。LinkedHashMap的Entry是除了保存了hash值、键、值以及next节点以外，还保存了before和after两个Entry。 put方法： LinkedHashMap没有重写put方法，所以还是调用HashMap得到put方法。只不过重写了put里面的一些方法。如果通过hashcode计算哈希表的位置为空，newNode创建一个节点存在这里。在这里创建的是LinkedHashMap的Entry节点，并调用一个linkNodeLast方法，将这个节点连接在last节点后面。如果键存在了，那么会进行更新value，并调用afterNodeAccess(e)，如果accessOrder为true，也就是按访问顺序进行排序，那么把节点移动到last节点的尾部。 get方法： get方法和HashMap的一样。只不过获取到entry之后，调用accessOrder，将节点移动到双向链表的尾部 TreeMap怎么保证有序？底层是红黑树，红黑树天然支持排序。默认情况下通过Key值的自然顺序进行排序。 HashMap的get迭代了一个链表，那怎么保证HashMap的时间复杂度O(1)?链表的查找的时间复杂度又是多少？根据键值找到哈希桶的位置时间复杂度为O(1)，使用的就是数组的高效查询。但是仅仅有这个是无法满足整个hashmap查询时间复杂度为O(1)的。hashmap在处理哈希冲突的方式是拉链法，在冲突数据没有达到8个时该哈希桶内部存储使用的是链表的方式，当某个哈希桶的数据超过8个的情况下，有下面两种处理方式： 哈希桶的数量没有超过64个，进行resize，然后数据迁移 哈希桶的数量超过了64个，将该哈希桶内部数据进行红黑树化处理 在hashmap的键值正常（不同对象的hash值不同的情况）的情况下，哈希桶数量超过8个概率低于千万分之一，所以我们通常认为hashmap的查询时间复杂度为O(1) 在极端情况下，对象的hash值为相同的，那么查询的时间复杂度为O( logn) 为什么 Map 桶中超过 8 个才转为红黑树？ 因为 TreeNode 的大小大约是常规节点的两倍，所以优先使用链表，当树的节点小于或等于6个以后，又会将红黑树转为list。在理想的情况下，hashcode是随机码时，**哈希表中节点的频率遵循泊松分布，而根据统计，忽略方差，list长度为8的情况概率很低(1亿分之6)**，再往后调整没有很大意义了。 copyOnWriteListCopyOnWriteArrayList是ArrayList的线程安全版本，从他的名字可以推测，CopyOnWriteArrayList是在有写操作的时候会copy一份数据，然后写完再设置成新的数据。 123456789101112131415161718public boolean add(E e) &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; // 获得当前数组对象 Object[] elements = getArray(); int len = elements.length; //拷贝到一个新的数组中 Object[] newElements = Arrays.copyOf(elements, len + 1); //插入数据元素 newElements[len] = e; //将新的数组对象设置回去 setArray(newElements); return true; &#125; finally &#123; lock.unlock(); &#125;&#125; CopyOnWriteArrayList适用于读多写少的并发场景，CopyOnWriteArraySet是线程安全版本的Set实现，它的内部通过一个CopyOnWriteArrayList来代理读写等操作，使得CopyOnWriteArraySet表现出了和CopyOnWriteArrayList一致的并发行为，他们的区别在于数据结构模型的不同，set不允许多个相同的元素插入容器中。 TreeMap怎么按照自己想要的顺序排序一个非常多元素的HashMap，rehash非常耗时，所以需要在它rehash过程中还能get、put，你有什么解决方案或者思路，谈谈你的理解这题我一开始没了解面试官说的什么意思，然后扯了很多相关的东西，比如：针对大容量，带参初始化，避免频繁扩容ConcurrentHashMap是fail-safe容器，遍历时会在副本上遍历，后续的写操作对当前遍历不可见 然后面试官又说那这个数据量特别大，rehash很耗时怎么办，还在rehash后面的get、put怎么办呢？我说可以借鉴Redis的渐进式hash，hash操作分摊开 然后面试官说正在rehash，这时get元素怎么取值、put元素怎么插入？我说在ConcurrentHashMap中当另一个线程put元素时，发现当前状态为扩容时会协助其扩容，这样提高性能 面试官说你不一定要去想已有的实现，可以自己想想有什么方案我说由于正在rehash，比如正在从16扩容到32，那么对输入key进行两次hash，分别定位扩容前和扩容后的table索引位置，然后再在链表上遍历 IO水平触发 如果文件描述符上可以非阻塞地执行I/O系统调用，此时认为它已经就绪，触发通知.在网络服务器的开发中，假如我们以水平触发模式的io多路复用来同时监控多个已连接描述符，当某一描述符上有数据可读的时候，就会触发通知(一般来说，就是相应调用返回)。应用程序在收到通知后可以选择读取所有可读的数据，读取部分数据，甚至是不读取任何数据。对于读取部分数据和不读取任何数据这种处理方式中，当下一次检查的时候，该文件描述符还是被返回(尽管在这一段时间内，该文件描述符上没有任何新的时间发生)，被返回的原因则是该文件描述符上仍然有数据可读，即仍然可以在其上非阻塞的执行read()。也就是说，在水平触发模式中，当前事件的消息不一定必须要当前读取完毕，留到之后读取也没有关系。 边缘触发 如果文件描述符自上次状态检查以来有了新的I/O活动(比如新的输入，用于写入的缓冲区腾出了空间等)，此时需要触发通知。仍然以网络服务器开发为例，这次我们使用边缘触发，当某一文件描述符上有数据可读，假如我们仍然是部分读取数据，那么在下一次检查的时候，我们将得不到该文件描述符已经ready的通知(除非在这一段时间内，该描述符上面有新的数据到来)。边缘触发只有在有新的i/o活动的时候才会触发通知，及时当前套接字上有数据，可以非阻塞的执行I/O系统调用，应用程序也得不到通知。也就是说，在边缘触发模式中，在一次通知中，我们应该尽可能的多的读取数据，直到不能再读为止。具体的做法是这样的:文件描述符应该提前设置问非阻塞(O_NONBLOCK),在已就绪文件描述符上循环的读取数据，知道read()返回-1，并且erron为EAGAIN或EWOULDBLOCK. 在上面我们对水平触发和边缘触发的分析中，我们可以知道边缘触发相比于水平触发来讲，降低了同一个事件被重复触发的概率，减少了多路复用系统调用的次数，因此边缘触发的性能由于水平触发。 在我们常用的io多路复用系统调用中，select和poll只支持水平触发，epoll则既支持水平触发也支持边缘触发。 零拷贝零拷贝技术实现的方式通常有 2 种： mmap + write 12buf = mmap(file, len);write(sockfd, buf, len); 具体过程：（三次拷贝） 应用进程调用了 mmap() 后，DMA 会把磁盘的数据拷贝到内核的缓冲区里。接着，应用进程跟操作系统内核「共享」这个缓冲区； 应用进程再调用 write()，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 CPU 来搬运数据； 最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 DMA 搬运的。 系统调用函数 sendfile() 具体过程：（二次拷贝） 通过 DMA 将磁盘上的数据拷贝到内核缓冲区里； 缓冲区描述符和数据长度传到 socket 缓冲区，这样网卡的 SG-DMA 控制器就可以直接将内核缓存中的数据拷贝到网卡的缓冲区里，此过程不需要将数据从操作系统内核缓冲区拷贝到 socket 缓冲区中，这样就减少了一次数据拷贝； 零拷贝（Zero-copy）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。 netty原理介绍Reactor模型Reactor模式是基于事件驱动开发的，服务端程序处理传入多路请求，并将它们同步分派给请求对应的处理线程，Reactor模式也叫Dispatcher模式，即I/O多路复用统一监听事件，收到事件后分发（Dispatch给某进程），这是编写高性能网络服务器的必备技术之一。 Reactor模式以NIO为底层支持，核心组成部分包括Reactor和Handler： Reactor：Reactor在一个单独的线程中运行，负责监听和分发事件，分发给适当的处理程序来对I/O事件做出反应。它就像公司的电话接线员，它接听来自客户的电话并将线路转移到适当的联系人。 Handlers：处理程序执行I/O事件要完成的实际事件，Reactor通过调度适当的处理程序来响应 I/O 事件，处理程序执行非阻塞操作。类似于客户想要与之交谈的公司中的实际员工。 根据Reactor的数量和Handler线程数量，可以将Reactor分为三种模型: 单线程模型 (单Reactor单线程) 多线程模型 (单Reactor多线程) 主从多线程模型 (多Reactor多线程) Netty的IO原理Reactor反应模型，Linux那边叫做IO多路复用。一个线程用来接收请求，将读写事件交给背后的worker线程。Redis、Nginx、Netty都是用到了这种模型。Redis其实也是多线程，只不过是用单线程来接收请求，在客户端看起来是串行接收执行，所以效果上就是单线程。但是IO多路复用才是Redis能高并发的底层保证。 一个IO怎么实现多路复用一旦某个文件句柄就绪，就能够通知应用程序进行相应的读写操作；没有文件句柄就绪时会阻塞应用程序，交出cpu。多路是指网络连接，复用指的是同一个线程 了解NIO吗 NIO（Non-blocking I/O，在Java领域，也称为New I/O），是一种同步非阻塞的I/O模型，也是I/O多路复用的基础，已经被越来越多地应用到大型应用服务器，成为解决高并发与大量连接、I/O处理问题的有效方式。 read读取数据时，无论是堵塞模式还是非堵塞模式，当缓存区有数据的时候就会直接返回，而不是等到应用程序所需的数据完整才返回。如果此时缓冲区是空的，那么阻塞模式会等待，非阻塞则会返回-1并有EWOULDBLOCK或EAGAIN错误 write写入数据时，当内核缓冲区空间足够时，无论是堵塞模式还是非堵塞模式，都会将数据一次性拷贝到缓冲区中。如果缓冲区空间不够的话，阻塞模式下会一直等待发送区空间足够，非阻塞模式能写多少就写多少，并返回写入值的大小，这种情况需要循环的方式将数据写入到缓冲区中。如果缓冲区已满，返回-1，并设置error值为EAGAIN 或 EWOULDBLOCK。 BIO、NIO、AIO首先一个IO操作(read/write系统调用)其实分成了两个步骤： 发起IO请求 实际的IO读写(内核态与用户态的数据拷贝) 阻塞IO和非阻塞IO的区别在于第一步，发起IO请求的进程是否会被阻塞，如果阻塞直到IO操作完成才返回那么就是传统的阻塞IO，如果不阻塞，那么就是非阻塞IO。 同步IO和异步IO的区别就在于第二步，实际的IO读写(内核态与用户态的数据拷贝)是否需要进程参与，如果需要进程参与则是同步IO，如果不需要进程参与就是异步IO。 如果实际的IO读写需要请求进程参与，那么就是同步IO。因此阻塞IO、非阻塞IO、IO复用、信号驱动IO都是同步IO， 同步阻塞IO（Blocking IO） BIO，同步阻塞模式。用户发出请求，一直堵塞等待数据准备好，数据返回成功之后用户才能继续发送下一个请求。 同步非阻塞IO（Non-blocking IO） NIO，同步非阻塞模式。用户发出请求后，一直等待数据返回结果；但是不会堵塞而是会直接发送下一个请求。 异步IO（Asynchronous IO） AIO，异步非阻塞模式。用户进程只需要发起一个IO操作便立即返回，等 IO 操作真正完成以后，应用程序会得到IO操作完成的通知，此时用户进程只需要对数据处理就好了。 Java的NIO 缓冲区 Buffer。Buffer 是一个对象，它包含一些要写入或者要读出的数据，在 NIO 库中，所有数据都是用缓冲区处理的。在读取数据时，它是直接读到缓冲区中的；在写入数据时，写入到缓冲区中，任何时候访问 NIO 中的数据，都是通过缓冲区进行操作。缓冲区实质上是一个数组。通常它是一个字节数组(ByteBuffer)，也可以使用其他种类的数组，但是一个缓冲区不仅仅是一个数组，缓冲区提供了对数据的结构化访问以及维护读写位置(limit)等信息。常用的有ByteBuffer，其它还有CharBuffer、ShortBuffer、IntBuffer、LongBuffer、FloatBuffer、DoubleBuffer。 通道 Channel。Channel 是一个通道，可以通过它读取和写入数据，它就像自来水管一样，网络数据通过 Channel 读取和写入。通道与流的不同之处在于通道是双向的，流只是一个方向上移动(一个流必须是 InputStream 或者 OutputStream 的子类)，而且通道可以用于读、写或者用于读写。同时Channel 是全双工的，因此它可以比流更好的映射底层操作系统的API。特别是在Unix网络编程中，底层操作系统的通道都是全双工的，同时支持读写操作。 多路复用器Selector。多路复用器 Selector 是 Java NIO 编程的基础，多路复用器提供选择已经就绪的任务的能力，简单的说，Selector 会不断的轮询注册在其上的 Channel，如果某个 Channel 上面有新的 TCP 连接接入、读和写事件，这个 Channel 就处于就绪状态，会被 Selector 轮询出来，然后通过 SelectionKey 可以获取就绪 Channel 的集合，进行后续的 I/O 操作。一个多用复用器 Selector 可以同时轮询多个 Channel，由于 JDK 使用了 epoll() 代替传统的 select() 实现，所以它并没有最大连接句柄 1024/2048 的限制，这也意味着只需要一个线程负责 Selector 的轮询，就可以接入成千上万的客户端。 实现步骤：置和映射原生信息，将 POJO 映射成数 据库中的记录，避免了几乎所有的 JDBC 代码和手 创建ServerSocketChannel实例，并绑定指定端口； 创建Selector实例； 将serverSocketChannel注册到selector，并指定事件OP_ACCEPT，最底层的socket通过channel和selector建立关联； 如果没有准备好的socket，select方法会被阻塞一段时间并返回0； 如果底层有socket已经准备好，selector的select方法会返回socket的个数，而且selectedKeys方法会返回socket对应的事件（connect、accept、read or write）； 根据事件类型，进行不同的处理逻辑(在步骤3中，selector只注册了serverSocketChannel的OP_ACCEPT事件)： 如果有客户端A连接服务，执行select方法时，可以通过serverSocketChannel获取客户端A的socketChannel，并在selector上注册socketChannel的OP_READ事件 如果客户端A发送数据，会触发read事件，这样下次轮询调用select方法时，就能通过socketChannel读取数据，同时在selector上注册该socketChannel的OP_WRITE事件，实现服务器往客户端写数据。 尽管 NIO 编程难度确实比同步阻塞 BIO 大很多，但是我们要考虑到它的优点： 客户端发起的连接操作是异步的，可以通过在多路复用器注册 OP_CONNECT 等后续结果，不需要像之前的客户端那样被同步阻塞。 SocketChannel 的读写操作都是异步的，如果没有可读写的数据它不会同步等待，直接返回，这样IO通信线程就可以处理其它的链路，不需要同步等待这个链路可用。 线程模型的优化：由于 JDK 的 Selector 在 Linux 等主流操作系统上通过 epoll 实现，它没有连接句柄数的限制(只受限于操作系统的最大句柄数或者对单个进程的句柄限制)，这意味着一个 Selector 线程可以同时处理成千上万个客户端连接，而且性能不会随着客户端的增加而线性下降，因此，它非常适合做高性能、高负载的网络服务器。 java的NIO是一种同步非堵塞IO模型，也是I/O多路复用的基础，已经被越来越多地应用到大型应用服务器，成为解决高并发与大量连接、I/O处理问题的有效方式。ava中的Selector和Linux中的epoll都是这种模型。 IO模型 select和epoll的区别文件描述符fd 有三个数组，readFds：保存发生read的FD；writeFds：保存发生write的FD；exceptFds：存发生except的FD fd_set是一个文件描述符fd的集合，由于每个进程可打开的文件描述符默认值为1024，fd_set可记录的fd个数上限也是为1024个。 select实现 select 的实现思路很直接，假如程序同时监视如 sock1、sock2 和 sock3 三个 socket，那么在调用 select 之后，操作系统把进程 A 分别加入这三个 socket 的等待队列中。当任何一个 socket 收到数据后，中断程序将唤起进程。所谓唤起进程，就是将进程从所有的等待队列中移除，加入到工作队列里面。经由这些步骤，当进程 A 被唤醒后，它知道至少有一个 socket 接收了数据。程序只需遍历一遍 socket 列表，就可以得到就绪的 socket 调用 select 之后，调用函数将fd_set从用户空间拷贝到内核空间 接着调用do_select方法，do_select是整个select的核心过程，主要工作流程如下： 初始化等待队列，将等待队列的成员变量poll_queue_proc设置为__pollwait函数；同时记录当前进程task_struct记在pwq结构体的polling_task。 循环遍历所有的文件描述符fd，调用文件系统*f_op-&gt;poll函数，调用回调函数__pollwait，把current（当前进程）挂到设备的等待队列中。当存在被监控的fd触发目标事件，并将其fd相对应的结果写入到fd_set，退出循环，将结果返回用户空间；当没有找到目标事件，如果已超时或者有待处理的信号，也会退出循环体，返回给用户空间。 当以上两种情况都不满足，则会让当前进程进入休眠状态，以等待fd或者超时定时器来唤醒自己，再走一遍循环 __pollwait的主要工作就是把current（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列，对于tcp_poll来说，其等待队列是sk-&gt;sk_sleep（注意把进程挂到等待队列中并不代表进程已经睡眠了）。在设备收到一条消息（网络设备）或填写完文件数据（磁盘设备）后，会唤醒设备等待队列上睡眠的进程，这时current便被唤醒了,每次会轮询所有的fd的f_op-&gt;poll()，这是缺陷。 select的缺点： 每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大 每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大 select支持的文件描述符数量太小了，默认是1024 poll实现 poll和select的区别 select使用的是定长数组，而poll是通过用户自定义数组长度的形式（pollfd[]） select只支持最大fd为1024，如果单个进程的文件句柄数超过1024，select就不能用了。poll在接口上无限制 还有就是一些时间不同、精度不同等等 其他的都差不多 epoll epoll的流程 epoll_create 当某个进程调用 epoll_create 方法时，内核会创建一个eventpoll对象，在这个eventpoll里面会初始化一个等待队列和一个就绪链表。 从slab缓存中创建一个eventpoll对象，并且创建一个匿名的fd跟fd对应的file对象，而eventpoll对象保存在struct file结构的private指针中，并且返回，该fd对应的file operations只是实现了poll跟release操作。创建eventpoll对象的初始化操作。获取当前用户信息，是不是root，最大监听fd数目等并且保存到eventpoll对象中。初始化等待队列，初始化就绪链表，初始化红黑树的头结点 epoll_ctl 创建 eventpoll 对象后，调用 epoll_ctl 添加或删除所要监听的 socket。首先会创建一个与fd对应的epitem结构，并且初始化相关成员，比如保存监听的fd跟file结构之类的，指定调用poll_wait时的回调函数，在其内部主要是将 eventpoll 添加到socket的等待队列。epitem就跟这个socket关联起来了，当socket有状态变化时，会通过ep_poll_callback()来通知。 将epoll_event结构拷贝到内核空间中，并且判断加入的fd是否支持poll结构(epoll、poll、selectI/O多路复用必须支持poll操作)。并且从epfd-&gt;file-&gt;privatedata获取event_poll对象，根据op区分是添加删除还是修改，首先在eventpoll结构中的红黑树查找是否已经存在了相对应的fd，没找到就支持插入操作，否则报重复的错误。相对应的修改、删除比较简单就不啰嗦了。插入操作时，会创建一个与fd对应的epitem结构，并且初始化相关成员，比如保存监听的fd跟file结构之类的。重要的是指定了调用poll_wait时的回调函数，用于数据就绪时唤醒进程，(其内部初始化设备的等待队列，将该进程注册到等待队列)完成这一步，我们的epitem就跟这个socket关联起来了，当它有状态变化时，会通过ep_poll_callback()来通知。最后调用加入的fd的file operation-&gt;poll函数(最后会调用poll_wait操作)用于完成注册操作。最后将epitem结构添加到红黑树中。 有一个set保存了我们要监听的fd epoll_wait操作 循环调用epoll_wait，当 socket 收到数据后，中断程序会给 eventpoll 的就绪列表添加 socket 引用，当前线程放到等待队列中，然后循环判断是否有信号到来，如果超时或者被唤醒，首先从自己初始化的等待队列删除，然后扫描整个链表，开始拷贝资源给用户空间了。(因为 rdlist 的存在，进程 A 可以知道哪些 socket 发生了变化) 计算睡眠时间(如果有)，判断eventpoll对象的链表是否为空，不为空那就干活。并且初始化一个等待队列，把自己挂上去，设置自己的进程状态为可睡眠状态。判断是否有信号到来(有的话直接被中断醒来)，如果啥事都没有那就调用schedule_timeout进行睡眠，如果超时或者被唤醒，首先从自己初始化的等待队列删除，然后开始拷贝资源给用户空间了。拷贝资源则是先把就绪事件链表转移到中间链表，然后挨个遍历拷贝到用户空间， 并且挨个判断其是否为水平触发，是的话再次插入到就绪链表 当某个进程调用 epoll_create 方法时，内核会创建一个 eventpoll 对象（也就是程序中 epfd 所代表的对象）。eventpoll 对象也是文件系统中的一员，和 socket 一样，它也会有等待队列。创建一个代表该 epoll 的 eventpoll 对象是必须的，因为内核要维护“就绪列表”等数据，“就绪列表”可以作为 eventpoll 的成员。创建 eventpoll 对象后，可以用 epoll_ctl 添加或删除所要监听的 socket。以添加 socket 为例，如果通过 epoll_ctl 向eventpoll 添加 sock1、sock2 和 sock3 的监视，内核会将 eventpoll 添加到这三个 socket 的等待队列中。当 socket 收到数据后，中断程序会给 eventpoll 的“就绪列表 rdlist ”添加 socket 引用。eventpoll 对象相当于 socket 和进程之间的中介，socket 的数据接收并不直接影响进程，而是通过改变 eventpoll 的就绪列表来改变进程状态。当程序执行到 epoll_wait 时，如果 rdlist 已经引用了 socket，那么 epoll_wait 直接返回，如果 rdlist 为空，阻塞进程。假设计算机中正在运行进程 A 和进程 B，在某时刻进程 A 运行到了 epoll_wait 语句。如下图所示，内核会将进程 A 放入 eventpoll 的等待队列中，阻塞进程。 当 socket 接收到数据，中断程序一方面修改 rdlist，另一方面唤醒 eventpoll 等待队列中的进程，进程 A 再次进入运行状态（如下图）。也因为 rdlist 的存在，进程 A 可以知道哪些 socket 发生了变化。 epoll的优势 epoll解决了上文提到的select中存在的所有问题 对于第一个缺点，epoll的解决方案在epoll_ctl函数中。每次注册新的事件到epoll句柄中时会把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。保证了每个fd在整个过程中只会拷贝一次，后面还会进行动态扩容。 对于需要监听的fd，只需要在初始化的时候调用一次epoll_ctl将fd与epfd相关联，后续就能循环调用epoll_wait监听事件了。无需像select一样，每次调用select方法的时候都要重复设置并传入待监听的fd集合。这样可以减少重复设置fd_set、以及将fd_set在用户空间与kernel之间来回拷贝带来的开销 epoll事先通过epoll_ctl()来注册一个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait() 时便得到通知。 对于第二个缺点，epoll的解决方案不像select或poll一样每次都把当前进程轮流加入fd对应的设备等待队列中，而只在epoll_ctl时把current挂一遍（这一遍必不可少）并为每个fd指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调函数，而这个回调函数会把就绪的fd加入一个就绪链表）。epoll_wait的工作实际上就是在这个就绪链表中查看有没有就绪的fd（利用schedule_timeout()实现睡一会，判断一会的效果，和select实现中的第7步是类似的）。 在调用epoll_create1时，会在kernel中建立一颗fd红黑树与一个就绪fd链表，后续调用epoll_ctl中放入的fd会被挂载到这棵树上，同时也会在kernel的中断处理函数中注册一个回调函数。一旦某个正在监听的fd上有数据可读，kernel在把数据拷贝到内核缓存区中之后，还会将这个fd插入到就绪fd链表中。这样kernel就不用在有fd就绪的时候遍历整个fd集合，从而减少开销。 对于第三个缺点，epoll没有这个限制。举个例子，在1GB内存的机器上大约是10万左右，具体数目可以cat /proc/sys/fs/file-max察看，一般来说这个数目和系统内存关系很大。 拦截器，监听器和过滤器区别 RESTful 和SOAP NIO有哪些特性，相对于普通IO的不同之处 多线程并发juc了解么，有哪些线程安全的list CopyOnWriteArrayList、CopyOnWriteArraySetmute lock怎么实现的锁的本质。所谓的锁，在计算机里本质上就是一块内存空间。当这个空间被赋值为1的时候表示加锁了，被赋值为0的时候表示解锁了，仅此而已。在硬件上实现多线程中只有一个线程抢到锁是通过CPU的缓存一致性。如果其中一个cpu修改了数据，会通过总线立即回写到主内存中，其他cpu会通过总线嗅探机制感知到缓存中数据的变化并将工作内存中的数据失效，再去读取主内存中的数据。 多个线程抢一个锁，就是抢着要把这块内存赋值为1。在一个多核环境里，内存空间是共享的。每个核上各跑一个线程，那如何保证一次只有一个线程成功抢到锁呢？另外，需要在一个单一的、不可中断的指令中对内存进行读和写操作。 硬件。CPU如果提供一些用来构建锁的atomic指令，譬如x86的CMPXCHG（加上LOCK 前缀），能够完成atomic的（CAS），用这样的硬件指令就能实现spin lock。本质上LOCK前缀的作用是锁定系统总线（或者锁定某一块cache line）来实现atomicity，可以了解下基础的缓存一致协议譬如MSEI。 操作系统。一个spin lock就是让没有抢到锁的线程不断在while里面循环进行compare-and-swap，燃烧CPU，直到前面的线程放手（对应的内存被赋值0）。这个过程不需要操作系统的介入，这是运行程序和硬件之间的故事。如果需要长时间的等待，这样反复CAS轮询就比较浪费资源，这个时候程序可以向操作系统申请被挂起，然后持锁的线程解锁了以后再通知它。但是OS切换线程也需要一些开销，所以是否选择被挂起，取决于大概是否需要等很长时间，如果需要，则适合挂起切换为别的线程。线程向操作系统请求被挂起是通过一个系统调用，在linux上的实现就是futex，宏观来讲，OS需要一些全局的数据结构来记录一个被挂起线程和对应的锁的映射关系，这样一个数据结构天然是全局的，因为多个OS线程可能同时操作它。 线程池里面的阻塞队列是干什么的首先线程池的作用，就是线程的复用。如果新任务的到达速率超过了线程池的处理速率，那么新到来的请求将累加起来，这样的话将耗尽资源。另外，线程池创建线程需要获取mainlock这个全局锁，影响并发效率，阻塞队列可以很好的缓冲。 为什么需要拒绝策略线程池工作中，当任务量很大，超过系统实际承载能力时，如果不去搭理它，系统很可能崩溃 FutureTask123FutureTask&lt;String&gt; task = new FutureTask&lt;&gt;(new MyCallable());new Thread(task).start();System.out.println(&quot;task return &quot; + task.get()); Mycallable实现了Callable的call的方法，然后让Thread执行这个方法。 Callable和Future的了解 Runable适用于完全异步的任务，不用操心执行情况，异常出错的。 Callable适用于需要由返回结果的，对执行中的异常要知晓的，需要提交到线程池中。 Future主要是线程池执行Callable任务，返回的结果。它能够中断任务的执行，一直等待结果，或者等待一段时间获取结果。 123456789MyCallable submitTask = new MyCallable(&quot;submit&quot;);// submit会返回一个FutureFuture&lt;?&gt; submit = threadPool.submit(submitTask);try &#123; // 可能会抛出异常 submit.get();&#125; catch (ExecutionException e) &#123; e.printStackTrace();&#125; Callable和Runnable的区别 Callable规定的方法是call(),Runnable规定的方法是run(). Callable的任务执行后可返回值，而Runnable的任务是不能返回值得 call方法可以抛出异常，run方法不可以 sleep()和wait()的区别，追问wait()为什么要在同步代码块或同步方法里面wait()、notify()和notifyAll() wait()、notify() 和 notifyAll()方法是本地方法，并且为 final 方法，无法被重写。 调用某个对象的 wait() 方法能让当前线程阻塞，并且当前线程必须拥有此对象的 monitor（即锁，或者叫管程）。 调用某个对象的 notify() 方法能够唤醒一个正在等待这个对象的 monitor 的线程，如果有多个线程都在等待这个对象的 monitor，则只能唤醒其中一个线程。 调用 notifyAll() 方法能够唤醒所有正在等待这个对象的monitor的线程 为了避免出现 lost wake up 问题，也就是消费者在检查count到调用wait()之间，count就可能被改掉了。 那我们如何解决呢？让消费者和生产者竞争一把锁，竞争到了的，才能够修改count的值。 Wait/Notify的原理 BLOCKED状态的线程是在竞争对象时，发现Monitor的Owner已经是别的线程了，此时就会进入EntryList中，并处于BLOCKED状态 WAITING状态的线程是获得了对象的锁，但是自身因为某些原因需要进入阻塞状态时，锁对象调用了wait方法而进入了WaitSet中，处于WAITING状态 BLOCKED状态的线程会在锁被释放的时候被唤醒，但是处于WAITING状态的线程只有被锁对象调用了notify方法(obj.notify/obj.notifyAll)，才会被唤醒。 Thread.sleep()和Object.wait()的区别 Thread.sleep()不会释放占有的锁，Object.wait()会释放占有的锁； Thread.sleep()必须传入时间，Object.wait()可传可不传，不传表示一直阻塞下去； Thread.sleep()到时间了会自动唤醒，然后继续执行；Object.wait()不带时间的，需要另一个线程使用Object.notify()唤醒； Object.wait()带时间的，假如没有被notify，到时间了会自动唤醒，这时又分好两种情况，一是立即获取到了锁，线程自然会继续执行；二是没有立即获取锁，线程进入同步队列等待获取锁； 其实，他们俩最大的区别就是Thread.sleep()不会释放锁资源，Object.wait()会释放锁资源。 Thread.sleep()和Condition.await()的区别这个题目的回答思路跟Object.wait()是基本一致的，不同的是Condition.await()底层是调用LockSupport.park()来实现阻塞当前线程的。实际上，它在阻塞当前线程之前还干了两件事，一是把当前线程添加到条件队列中，二是“完全”释放锁，也就是让state状态变量变为0，然后才是调用LockSupport.park()阻塞当前线程 Thread.sleep()和LockSupport.park()的区别LockSupport.park()还有几个兄弟方法——parkNanos()、parkUtil()等，我们这里说的park()方法统称这一类方法。 从功能上来说，Thread.sleep()和LockSupport.park()方法类似，都是阻塞当前线程的执行，且都不会释放当前线程占有的锁资源； Thread.sleep()没法从外部唤醒，只能自己醒过来；LockSupport.park()方法可以被另一个线程调用LockSupport.unpark()方法唤醒； Thread.sleep()方法声明上抛出了InterruptedException中断异常，所以调用者需要捕获这个异常或者再抛出；LockSupport.park()方法不需要捕获中断异常 Thread.sleep()本身就是一个native方法；LockSupport.park()底层是调用的Unsafe的native方法； Object.wait()和LockSupport.park()的区别 Object.wait()方法需要在synchronized块中执行；LockSupport.park()可以在任意地方执行； Object.wait()方法声明抛出了中断异常，调用者需要捕获或者再抛出；LockSupport.park()不需要捕获中断异常； Object.wait()不带超时的，需要另一个线程执行notify()来唤醒，但不一定继续执行后续内容；LockSupport.park()不带超时的，需要另一个线程执行unpark()来唤醒，一定会继续执行后续内容； 如果在wait()之前执行了notify()会怎样？抛出IllegalMonitorStateException异常；如果在park()之前执行了unpark()会怎样？线程不会被阻塞，直接跳过park()，继续执行后续内容； Synchronized如果对象虽然被多个线程访问，但是线程间不存在竞争，会进行一个重偏向的操作。当撤销偏向锁的阈值超过40以后，就会将整个类的对象都改为不可偏向的 创建锁记录（Lock Record）对象，每个线程的栈帧都会包含一个锁记录对象，内部可以存储锁定对象的mark word（不再一开始就使用Monitor） 让锁记录中的Object reference指向锁对象（Object），并尝试用cas去替换Object中的mark word，将此mark word放入lock record中保存 如果cas替换成功，则将Object的对象头替换为锁记录的地址和状态 00（轻量级锁状态），并由该线程给对象加锁 锁膨胀 如果一个线程在给一个对象加轻量级锁时，cas替换操作失败（因为此时其他线程已经给对象加了轻量级锁），此时该线程就会进入锁膨胀过程 此时便会给对象加上重量级锁（使用Monitor） 将对象头的Mark Word改为Monitor的地址，并且状态改为01(重量级锁) 并且该线程放入EntryList中，并进入阻塞状态(blocked) 什么情况下用ReentrantLock而不用synchronized 锁的实现。synchronized 是 JVM 实现的，而 ReentrantLock 是 JDK 实现的。 等待可中断。当持有锁的线程长期不释放锁的时候，正在等待的线程可以选择放弃等待(需要使用lock.lockInterruptibly()方法获取锁，并且长时间获取不到锁的时候调用方法Thread.interrupt()，捕获异常进行相应操作)，改为处理其他事情。ReentrantLock 可中断，而 synchronized 不行。 公平锁是指多个线程在等待同一个锁时，必须按照申请锁的时间顺序来依次获得锁。synchronized 中的锁是非公平的，ReentrantLock 默认情况下也是非公平的，但是也可以是公平的。 锁绑定多个条件。一个 ReentrantLock 可以同时绑定多个 Condition 对象。用来实现分组需要唤醒的线程们，可以精确唤醒，而不像synchronized那样随便唤醒一个线程或者全部线程(消费者生产者模式、顺序执行)。 Lock是显示锁（手动开启和关闭锁）, synchronized时隐式锁，出来作用域自动释放Lock只有代码块锁，synchronized有代码块锁和方法锁。 在性能上来说，如果竞争资源不激烈，两者的性能是差不多的，而当竞争资源非常激烈时（即有大量线程同时竞争），此时Lock的性能要远远优于synchronized。所以说，在具体使用时要根据适当情况选择。 除非需要使用 ReentrantLock 的高级功能，否则优先使用 synchronized。这是因为 synchronized 是 JVM 实现的一种锁机制，JVM 原生地支持它，而 ReentrantLock 不是所有的 JDK 版本都支持。并且使用 synchronized 不用担心没有释放锁而导致死锁问题，因为 JVM 会确保锁的释放。 Java的中断怎么实现，为什么synchronized不能中断，ReentrantLock可以中断中断操作是Thread类调用interrupt方法实现的。 Synchronized只有获取到锁之后才能中断，等待锁时不可中断。Thread.interrupt()只是做了修改一个中断状态值为true，并没有显式声明抛出InterruptedException异常。如果该线程处于阻塞状态(调用了wait,sleep,join方法)，那么该线程将会立即从阻塞状态中退出，并抛出一个InterruptedException异常。 必须使用ReentrantLock.lockInterruptibly()来获取锁，使用ReentrantLock.lock()方法不可中断。ReentrantLock.lockInterruptibly()首次尝试获取锁之前就会判断是否应该中断，如果没有获取到锁，在自旋等待的时候也会继续判断中断状态。这时lockInterruptibly底层再显式抛错 发生异常时 synchronized和lock锁的占用情况？ synchronized：线程执行发生异常，此时JVM会让线程自动释放锁 Lock，必须主动去释放锁，并且在发生异常时，不会自动释放锁 Java 中的多线程和操作系统中的多线程有什么区别现在Java线程与操作系统线程一对一映射，现在的Java线程，就是操作系统中的线程。 操作系统存在用户态的线程和内核态的线程。在jdk1.1中，JVM使用的是绿色线程，是由虚拟机调度而不是由操作系统调用的线程。所以不经过用户态到内核态的转换，这种线程对于操作系统内核看来就一个进程。 java1.2之后，java线程就是依赖操作系统实现的了，是1:1的关系。另外，在Linux2.6之后线程的实现方式为NPTL，JVM线程跟内核轻量级进程有一一对应的关系。线程的调度完全交给了操作系统内核。举个例子，在linux下，只要一个Thread.run就会调用一个fork产生一个线程。 线程有自己的独立的上下文，由操作系统调度，但是也有一个缺点，那就是线程消耗资源太大了，例如在linux上，一个线程默认的栈大小是1M，单机创建几万个线程就有点吃力了。所以后来在编程语言的层面上，就出现了协程这个东西。协程的模式有点类似结合了上面二种方式，即是在用户态做线程资源切换，也让操作系统在内核层做线程调度。协程跟操作系统的线程是有映射关系的，例如我们建了m个协程，需要在N个线程上执行，这就是m: n的方案，这n个线程也是靠操作系统调度实现。另外协程是按需使用栈内存的，所以理论上可以轻轻松松创建百万级的协程。 POSIX标准兼容 线程切换的过程？进程切换的过程？进程切换和线程切换的区别 最主要的一个区别在于进程切换涉及虚拟地址空间的切换而线程不会。因为每个进程都有自己的虚拟地址空间，而线程是共享所在进程的虚拟地址空间的，因此同一个进程中的线程进行线程切换时不涉及虚拟地址空间的转换。 为什么虚拟地址空间切换会比较耗时呢 现在我们已经知道了进程都有自己的虚拟地址空间，把虚拟地址转换为物理地址需要查找页表，页表查找是一个很慢的过程，因此通常使用Cache来缓存常用的地址映射，这样可以加速页表查找，这个cache就是TLB。由于每个进程都有自己的虚拟地址空间，那么显然每个进程都有自己的页表，那么当进程切换后页表也要进行切换，页表切换后TLB可能会失效，cache失效导致命中率降低，那么虚拟地址转换为物理地址就会变慢，表现出来的就是程序运行会变慢，而线程切换则不会导致TLB失效，因为线程线程无需切换地址空间，因此我们通常说线程切换要比较进程切换块，原因就在这里。 中断？谁去执行中断呢？在计算机科学中，中断指计算机CPU获知某些事，暂停正在执行的程序，转而去执行处理该事件的程序，当这段程序执行完毕后再继续执行之前的程序。整个过程称为中断处理，简称中断，而引起这一过程的事件称为中断事件。中断是计算机实现并发执行的关键，也是操作系统工作的根本。 什么是线程安全？线程安全如何保证？我答：对临界区资源加锁，在操作系统层面是P/V操作。 任务队列一般选用阻塞队列还是非阻塞队列？为什么使用阻塞队列？一般是阻塞队列，为了保证线程安全 成员变量和静态变量是否线程安全？成员变量 如果它们没有共享，则线程安全 如果它们被共享了，根据它们的状态是否能够改变，又分两种情况 如果只有读操作，则线程安全 如果有读写操作，则这段代码是临界区，需要考虑线程安全 局部变量 局部变量是线程安全的 但局部变量引用的对象则未必 （要看该对象是否被共享且被执行了读写操作） 如果该对象没有逃离方法的作用范围，它是线程安全的 如果该对象逃离方法的作用范围，需要考虑线程安全 不可变类线程安全性(String、Integer是线程安全的吗)String、Integer 等都是不可变类，因为其内部的状态不可以改变，因此它们的方法都是线程安全的 有同学或许有疑问，String 有 replace，substring 等方法【可以】改变值啊，那么这些方法又是如何保证线程安全的呢？ 这是因为这些方法的返回值都创建了一个新的对象，而不是直接改变String、Integer对象本身。 mutex硬件上，CPU如果提供一些用来构建锁的atomic指令，譬如x86的CMPXCHG（加上LOCK prefix），能够完成atomic的compare-and-swap （CAS），用这样的硬件指令就能实现spin lock。本质上CPU使用LOCK前缀来锁定系统总线（或者锁定某一块cache line）来实现atomicity，可以了解下基础的缓存一致协议譬如MSEI。 操作系统层面来讲，一个spin lock就是让没有抢到锁的线程不断在while里面循环进行compare-and-swap，燃烧CPU，浪费青春，直到前面的线程放手（对应的内存被赋值0）。这个过程不需要操作系统的介入，这是运行程序和硬件之间的故事。如果需要长时间的等待，这样反复CAS轮询就比较浪费资源，这个时候程序可以向操作系统申请被挂起，然后持锁的线程解锁了以后再通知它。这样CPU就可以用来做别的事情，而不是反复轮询。但是OS切换线程也需要一些开销，所以是否选择被挂起，取决于大概是否需要等很长时间，如果需要，则适合挂起切换为别的线程。 线程向操作系统请求被挂起是通过一个系统调用，在linux上的实现就是futex，宏观来讲，OS需要一些全局的数据结构来记录一个被挂起线程和对应的锁的映射关系，这样一个数据结构天然是全局的，因为多个OS线程可能同时操作它。所以，实现高效的锁本身也需要锁。有没有一环套一环的感觉？futex的巧妙之处就在于，它知道访问这个全局数据结构不会太耗时，于是futex里面的锁就是spin lock。linux上pthread mutex的实现就是用的futex。更多精彩内存参考talk：https://www.infoq.com/presentations 线程池参数的设置默认使用的是这个，队列长度设置为10000，空闲时间为0。核心线程数一般设置为5，最大的为10，如果设定的大于1000的话，设置为1000)。线程池的拒绝测量是实现了Reject的ExecutionHandler接口的方法。当触发拒绝策略时，将任务信息存储到一个ThreadLocal里面，当线程池的活跃线程数小于最大线程数的90%，让线程池去执行这个任务。一直尝试100次。如果还是不行，抛出异常。 还解决了一些问题： 使用ThreadLocal时，如果使用完毕后没有及时清理ThreadLocal，可能会因为连接池复用线程，线程长时间未释放等原因造成内存泄漏。（ThreadPools接口创建的线程会在线程结束后自动清理该线程的ThreadLocal） lockSupport.parkNanos–阻塞当前线程，最长不超过nanos纳秒，返回条件在park()的基础上增加了超时返回。 CPU密集型 CPU密集型也叫计算密集型，指的是系统的硬盘、内存性能相对CPU要好很多，此时，系统运作大部分的状况是CPU Loading 100%，CPU要读/写I/O(硬盘/内存)，I/O在很短的时间就可以完成，而CPU还有许多运算要处理，CPU Loading 很高。在多重程序系统中，大部分时间用来做计算、逻辑判断等CPU动作的程序称之CPU bound。例如一个计算圆周率至小数点一千位以下的程序，在执行的过程当中绝大部分时间用在三角函数和开根号的计算，便是属于CPU bound的程序。CPU bound的程序一般而言CPU占用率相当高。这可能是因为任务本身不太需要访问I/O设备，也可能是因为程序是多线程实现因此屏蔽掉了等待I/O的时间。 IO密集型 IO密集型指的是系统的CPU性能相对硬盘、内存要好很多，此时，系统运作，大部分的状况是CPU在等I/O (硬盘/内存) 的读/写操作，此时CPU Loading并不高。I/O bound的程序一般在达到性能极限时，CPU占用率仍然较低。这可能是因为任务本身需要大量I/O操作，而pipeline做得不是很好，没有充分利用处理器能力。 先看下机器的CPU核数，然后在设定具体参数：即CPU核数 = Runtime.getRuntime().availableProcessors() CPU密集型：corePoolSize = CPU核数 + 1；IO密集型：corePoolSize = CPU核数 * 2 对于计算密集型的任务，一个有Ncpu个处理器的系统通常通过使用一个Ncpu + 1个线程的线程池来获得最优的利用率（计算密集型的线程恰好在某时因为发生一个页错误或者因其他原因而暂停，刚好有一个“额外”的线程，可以确保在这种情况下CPU周期不会中断工作）。 1000 多个并发线程，10 台机器，每台机器 4 核，设计线程池大小。 如果是 CPU 密集型的任务，我们应该尽量的减少上下文切换，所以核心线程数可以设置为 5，队列的长度可以设置为 100，最大线程数保持和核心线程数一致。 如果是 IO 密集型的任务，我们可以适当的多分配一点核心线程数，更好的利用 CPU，所以核心线程数可以设置为 8，队列长度还是 100，最大线程池设置为 10。 再通过压测一下，得到合理的参数配置。是不是可以设置的极限一点，比如核心线程数和最大线程数都是 4，队列长度为 96，刚好可以承担这 100 个请求，多一个都不行了。 另外，因为是10台机器，我就假设用的dubbo。假设我们用的是随机负载均衡，我们就不能保证每台机器各自承担 100 个请求了。 协程，英文Coroutines，是一种比线程更加轻量级的存在。正如一个进程可以拥有多个线程一样，一个线程也可以拥有多个协程。 最重要的是，协程不是被操作系统内核所管理，而完全是由程序所控制（也就是在用户态执行）。这样带来的好处就是性能得到了很大的提升，不会像线程切换那样消耗资源。既然协程这么好，它到底是怎么来使用的呢？ 由于Java的原生语法中并没有实现协程（某些开源框架实现了协程，但是很少被使用），所以我们来看一看python当中对协程的实现案例，同样以生产者消费者模式为例： 这段代码十分简单，即使没用过python的小伙伴应该也能基本看懂。代码中创建了一个叫做consumer的协程，并且在主线程中生产数据，协程中消费数据。其中 yield 是python当中的语法。当协程执行到yield关键字时，会暂停在那一行，等到主线程调用send方法发送了数据，协程才会接到数据继续执行。但是，yield让协程暂停，和线程的阻塞是有本质区别的。协程的暂停完全由程序控制，线程的阻塞状态是由操作系统内核来进行切换。因此，协程的开销远远小于线程的开销。 如何确定线程数设置多少？ newFixedThreadPool有什么缺点 里面存放超出固定长度数的线程的队列是什么样的 ArrayBlockingQueueArrayBlockingQueue是一个有边界的阻塞队列，它的内部实现是一个数组。 初始化时必须制定容量的大小。默认的是非公平的。 使用ReentrantLock保证添加的线程安全。 可以通过add、offer、put添加元素。add-如果满了抛出异常；offer-如果满了返回false；put-如果满了，让当前线程等待。 通过peek、take、poll取出元素。peek-取出takeIndex位置的值；take-判断当前队列的元素数量为0，则当前线程进行等待，等待notEmpty.singal()，否则取出takeIndex位置的元素，把数组中此位置的引用置为null，判断takeIndex和数组的长度，如果相等证明，已经取到了最后一个元素，下次再取元素需要从位置0开始，为此把takeIndex置为0；poll-判断当前队列中的元素个数如果为0返回null，否则执行dequeue操作。 ArrayBlockingQueue是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。 LinkedBlockingQueue 默认是采用了默认大小为Integer.MAX_VALUE的容量 。它的内部实现是一个链表 不是双向队列，就是普通的 使用原子类AtomicInteger的count表示队列中元素的个数 定义了两把锁，一把take锁takeLock，另一把put锁putLock。通过两把可重入锁实现并发 put的步骤：判断要put的元素e是否为null，如果为null直接抛出空指针异常； ​ e不为null，则使用e创建一个Node节点，获得put锁； ​ 判断当前队列中的元素数量和队列的容量，如果相等，则阻塞当前线程；如果不相等，把生成的node节点插入队列。 ​ 唤醒notFull的操作，即可以进行继续添加，执行put等添加操作。 和ArrayBlockingQueue一样，LinkedBlockingQueue 也是以先进先出的方式存储数据，最新插入的对象是尾部，最新移出的对象是头部。 take的步骤：获得take锁，表示执行take操作； ​ 获得当前队列的元素数量，如果数量为0，则阻塞当前线程，直到被中断或者被唤醒；否则执行出队操作； offer的步骤：判断当前队列中的元素数量和队列容量，如果相等，直接返回false；否则执行入队操作。入队操作之后，判断队列中元素数量如果仍小于队列容量，唤醒其他的阻塞线程。 poll的步骤：如果当前队列元素数量为0，直接返回null；如果当前队列元素数量大于0，执行出队操作； ​ 如果c&gt;1，即c最小为2，则出队成功后，仍有1个元素，可以唤醒阻塞在take锁的线程； ​ 如果c=capacity，则出队成功后，队列中的元素为capacity-1，这时队列为满，可以唤醒阻塞在put锁上的其他线程，即可以添加； DelayQueue DelayQueue是一个无界的BlockingQueue，用于放置实现了Delayed接口的对象，其中的对象只能在其到期时才能从队列中取走。这种队列是有序的，即队头对象的延迟到期时间最长。注意：不能将null元素放置到这种队列中。 DelayQueue应用场景：定时关闭连接、缓存对象，超时处理等 PriorityBlockingQueue PriorityBlockingQueue是带优先级的无界阻塞队列，每次出队都返回优先级最高或者最低的元素(这里规则可以自己制定)，内部是使用平衡二叉树实现的，遍历不保证有序；需要注意，PriorityBlockingQueue中允许插入null对象。 底层实现还是数组，默认的大小为11 当前数组中实际数据总数&gt;=数组容量，就进行扩容。如果当前数组容量小于64，新数组容量就是2n+2，大于64，新的容量就是3n/2 否则的话，插入到优先堆中。获取的时候，就是获取数组的第一个值 SynchronousQueueSynchronousQueue队列内部仅允许容纳一个元素。 当一个线程插入一个元素后会被阻塞，除非这个元素被另一个线程消费。 线程通讯 volatile修饰的共享变量 互斥锁、条件变量、读写锁。 互斥锁提供了以排他方式防止数据结构被并发修改的方法。读写锁允许多个线程同时读共享数据，而对写操作是互斥的。条件变量可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件的测试是在互斥锁的保护下进行的。条件变量始终与互斥锁一起使用。 信号量机制(Semaphore)。包括无名线程信号量和命名线程信号量。Semaphore Semaphore(信号量)-允许多个线程同时访问。Semaphore（信号量）是用来控制同时访问特定资源的线程数量，它通过协调各个线程，以保证合理的使用公共资源。 执行 acquire 方法阻塞，直到有一个许可证可以获得然后拿走一个许可证；每个 release 方法增加一个许可证，这可能会释放一个阻塞的 acquire 方法。然而，其实并没有实际的许可证这个对象，Semaphore 只是维持了一个可获得许可证的数量。 Semaphore 经常用于限制获取某种资源的线程数量。 信号机制(Signal)。类似进程间的信号处理；线程间的通信目的主要是用于线程同步，所以线程没有像进程通信中的用于数据交换的通信机制。 多线程的创建，怎么退出线程 线程属于一次性消耗品，在执行完run()方法之后线程便会正常结束了，线程结束后便会销毁，不能再次start，只能重新建立新的线程对象，但有时run()方法是永远不会结束的。例如在程序中使用线程进行Socket监听请求，或是其他的需要循环处理的任务。在这种情况下，一般是将这些任务放在一个循环中，如while循环。当需要结束线程时，如何退出线程呢？ 有三种方法可以结束线程： 1.设置退出标志，使线程正常退出，也就是当run()方法完成后线程终止 2.使用interrupt()方法中断线程 Thread.interrupt()，设置当前中断标记为true（类似属性的set方法）。interrupt机制是一个中断状态位的变化和检测，还可以进行中断异常的处理。 如果没有必须捕获InterruptedException异常的代码(比如Thread.sleep())，则isInterrupted()会返回true，此时可以在isInterrupted的判断中处理中断变化。 如果有必须捕获InterruptedException异常的代码(比如Thread.sleep())，则会抛出InterruptedException异常，并进行捕获，同时重置isInterrupted为false，此时得在异常捕获中处理中断变化。 3.使用stop方法强行终止线程（不推荐使用，Thread.stop， Thread.suspend， Thread.resume 和Runtime.runFinalizersOnExit 这些终止线程运行的方法已经被废弃，使用它们是极端不安全的！） 线程池的参数 线程池的状态 RUNNING 状态说明：线程池处在RUNNING状态时，能够接收新任务，以及对已添加的任务进行处理。状态切换：线程池的初始化状态是RUNNING。换句话说，线程池被一旦被创建，就处于RUNNING状态，并且线程池中的任务数为0！ 1private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING， 0));1 SHUTDOWN 状态说明：线程池处在SHUTDOWN状态时，不接收新任务，但能处理已添加的任务。状态切换：调用线程池的shutdown()接口时，线程池由RUNNING -&gt; SHUTDOWN。 STOP 状态说明：线程池处在STOP状态时，不接收新任务，不处理已添加的任务，并且会中断正在处理的任务。状态切换：调用线程池的shutdownNow()接口时，线程池由(RUNNING or SHUTDOWN ) -&gt; STOP。 TIDYING 状态说明：当所有的任务已终止，ctl记录的”任务数量”为0，线程池会变为TIDYING状态。当线程池变为TIDYING状态时，会执行钩子函数terminated()。terminated()在ThreadPoolExecutor类中是空的，若用户想在线程池变为TIDYING时，进行相应的处理；可以通过重载terminated()函数来实现。状态切换：当线程池在SHUTDOWN状态下，阻塞队列为空并且线程池中执行的任务也为空时，就会由 SHUTDOWN -&gt; TIDYING。当线程池在STOP状态下，线程池中执行的任务为空时，就会由STOP -&gt; TIDYING。 TERMINATED 状态说明：线程池彻底终止，就变成TERMINATED状态。状态切换：线程池处在TIDYING状态时，执行完terminated()之后，就会由 TIDYING -&gt; TERMINATED。 死锁怎么预防 出现了死锁怎么办 银行家怎么实现 进程调度CFS 时钟调度算法 慢查询优化 ThreadLocal场景应用，spring里有用到吗 ThreadLocal自身内部是如何解决内存泄漏的 虚引用弱引用的区别 rabbitmq组建 Spring Boot启动流程以及生命周期 https://blog.csdn.net/u011277123/article/details/104476683 出了个sql题，怎么建索引。有一个订单表，有用户的属性和日期。现在有三个sql查询， 查某个用户的所有订单 查某个date的所有订单 查某个用户最近一个月的所有订单当时我说建三个索引，后来反问的时候，他提醒了我一下最左前缀，我才反应过来，两个就够。(date和id-date) 为什么线程切换上下文开销大当从一个线程切换到另一个线程时，不仅会发生线程上下文切换，还会发生特权模式切换。 首先，既然是线程切换，那么一定涉及线程状态的保存和恢复，包括寄存器、栈等私有数据。另外，线程的调度是需要内核级别的权限的（操作CPU和内存），也就是说线程的调度工作是在内核态完成的，因此会有一个从用户态到内核态的切换。而且，不管是线程本身的切换还是特权模式的切换，都要进行CPU的上下文切换。本质上都是从“一段程序”切换到了“另一段程序”，都要设置相应的CPU上下文。要明确一个问题，那就是内核也是有代码的，只是这些代码的机密性比较高，我们用户态无法访问。（要理清这几个概念的关系） cpu上下文包括寄存器和程序计数器 总结来说，线程切换过程包括：线程上下文的保存和恢复，用户态和内核态的转换，CPU上下文的切换，这些工作都需要CPU去完成，是一笔不小的开销 什么是伪共享 因为 CPU 与 内存的速度差异很大，需要靠预读数据至缓存来提升效率。而缓存以缓存行为单位，每个缓存行对应着一块内存，一般是 64 byte（8 个 long）缓存的加入会造成数据副本的产生，即同一份数据会缓存在不同核心的缓存行中。CPU 要保证数据的一致性，如果某个 CPU 核心更改了数据，其它 CPU 核心对应的整个缓存行必须失效。 因为 Cell 是数组形式，在内存中是连续存储的，一个 Cell 为 24 字节（16 字节的对象头和 8 字节的 value），因 此缓存行可以存下 2 个的 Cell 对象。这样问题来了： Core-0 要修改 Cell[0] Core-1 要修改 Cell[1] 无论谁修改成功，都会导致对方 Core 的缓存行失效，比如 Core-0 中 Cell[0]=6000, Cell[1]=8000 要累加 Cell[0]=6001, Cell[1]=8000 ，这时会让 Core-1 的缓存行失效。 @sun.misc.Contended 用来解决这个问题，它的原理是在使用此注解的对象或字段的前后各增加 128 字节大小的 padding（空白），从而让 CPU 将对象预读至缓存时占用不同的缓存行，这样，不会造成对方缓存行的失效 操作系统父进程执行fork()命令，假如父进程拥有1G的资源，那么子进程会携带这些资源吗？ 给你10000个IP，你如何快速查找这个IP是否存在，使用什么数据结构?顺嘴一提，我想了一会儿，面试官给我解释数据结构有哪些？哈哈哈。 默认虚拟机类加载完成时，类的信息保存在哪里？怎么找到的？ 举个例子，student表name， subject， scorely， yuwen， 95 —-&gt; ly， yuwen， 95， shuxue， 98ly， shuxue， 98讲了思路，真写不出来。。 为什么使用分布式锁？ 多线程锁只能是在单一的jvm上可用的。对于分布式系统，它是运行在不同的JVM里面的，如果是使用的多线程锁，每一个子系统中加的锁只对属于自己JVM里面的线程有效，对于其他JVM的线程是无效的。所以Java提供的原生锁机制在多机部署场景下会失效。所以这个时候就需要使用分布式锁。 为了保证一个方法在高并发情况下的同一时间只能被同一个线程执行，在传统单体应用单机部署的情况下，可以使用Java并发处理相关的API(如ReentrantLcok或synchronized)进行互斥控制。但是，随着业务发展的需要，原单体单机部署的系统被演化成分布式系统后，由于分布式系统多线程、多进程并且分布在不同机器上，这将使原单机部署情况下的并发控制锁策略失效，为了解决这个问题就需要一种跨JVM的互斥机制来控制共享资源的访问，这就是分布式锁要解决的问题。 为什么不用数据库的锁？ 这把锁强依赖数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。 这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁。 这把锁只能是非阻塞的，因为数据的insert操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作。 这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据中数据已经存在了。 传参： 主要有客户的ID、渠道、来源单据Id、更新总金额 RabbitMq通信过程了解了RabbitMQ的基础构成之后，我们再通过一张图片来解释一下RabbitMQ的通信过程： 是不是画的很好看，而且一下子就能把之前学到的给用上了~ 流程解析： 1、消息生产者连接到RabbitMQ Broker，创建connection，开启channel。 2、生产者声明交换机类型、名称、是否持久化等（其实这个说法不是很准确，其实大部分是在管理页面或者消费者端先声明的）。 3、生产者发送消息，并指定消息是否持久化等属性和routing key。 4、exchange收到消息之后，根据routing key路由到跟当前交换机绑定的相匹配的队列里面。 5、消费者监听接收到消息之后开始业务处理，然后发送一个ack确认告知消息已经被消费（手动自动都有可能）。 6、RabbitMQ Broker收到ack之后将对应的消息从队列里面删除掉。 rabbitMq和rocketMq的区别JVM讲一讲类加载器工作机制？你知道强引用、弱引用和软引用吗?为什么要有这些东西？他们有什么作用？你在项目中用过吗？假如现在在同一台机器上开两个java项目，有几个java虚拟机？每一个java程序对应一个jvm实例，都有自己的程序执行空间，每一个都有一套jvm虚拟机机制。 如果web服务器突然出现频率很高的FullGC，可能是什么原因?你会怎么去排查呢？频繁full gc的常见原因 full gc 触发条件是老年代空间不足，所以追因的方向就是导致老年代空间不足的原因： 大量对象频繁进入老年代 + 老年代空间释放不掉 系统并发高、执行耗时过长，或者数据量过大，导致 young gc频繁，且gc后存活对象太多，但是survivor 区存放不下（太小 或 动态年龄判断） 导致对象快速进入老年代，老年代迅速堆满。或者直接放到老年代中 存在内存泄漏的情况，老年代驻留了大量释放不掉的对象， 只要有一点点对象进入老年代就达到 full gc的阈值了 设置的问题，比如说元数据区加载了太多类 ，满了也会发生 full gc；堆外内存 direct buffer memory 使用不当导致 你看到老年代内存不高重启也没用还在频繁发生full gc， 那么可能有人作妖，在代码里搞执行了 System.gc(); 定位思路 如果有监控，那么通过图形能比较直观、快速的了解gc情况；如果没有监控，那么只能看gc日志或jstat来分析 观察年轻代 gc的情况，多久执行一次、每次gc后存活对象有多少 survivor区多大存活对象比较多 超过survivor区大小或触发动态年龄判断 =&gt; 调整内存分配比例 观察老年代的内存情况阈值情况，多久执行一次、执行耗时多少、回收掉多少内存。如果在持续的上涨，而且full gc后回收效果不好，那么很有可能是内存溢出了 =&gt; dump 排查具体是什么玩意 如果年轻代和老年代的内存都比较低，而且频率低，那么又可能是元数据区加载太多东西了 其实如果是自己负责的系统，可能要看是不是发版改了什么配置、代码 怎么在OOM中定位问题使用MAT这个工具。 确定最大（多）对象。 看那一个对象的深堆最大，持有的对象最多。 往下查，持有什么对象。看他持有了哪一个对象，为什么这么大。 往上追，直到GC root。 查线程堆栈。 通过GC root的堆栈，看什么原因导致大对象的生成 内存泄漏的地方/出现OOM的原因及解决方法 Java堆。 原因： 无法在 Java 堆中分配对象 使用内存的速度多于当前回收内存的速度 应用程序无意中持有对对象的引用，这会阻止对象被垃圾收集 应用程序过度使用finalize方法。如果重写了finalize方法，jvm会把该对象放到F-queue队列中。由一低优先级线程执行该队列中对象的finalize方法，有时GC的线程跟不上队列的速度就会出现内存溢出的情况。 方法：增大堆内存的大小；修复代码中的内存泄漏； GC次数太多。 原因：花了98%的时间进行full GC，这样连续超过5次就报错了 方法：关闭GCOverheadLimit这个参数；增大堆内存的大小；修复应用程序中的内存泄漏； 原因：数组大小超过 VM 限制。数组太大，堆放不下了。 方法：增大堆内存的大小-Xmx；将数组转为几个小数组存储 元空间。 原因：类的元数据是存放在本地内存中的，本地内存不够就会溢出 方法：增大元空间的大小-XX:MaxMetaSpaceSize；减小java堆的大小增大元空间的大小；硬件上增加内存；可能是代码的问题 原因：无法创建本地线程。 方法：减小堆内存的大小；硬件上增加内存条；修复代码中的内存泄漏；操作系统对进程数量做了一个限制，增大这个值：max user processes Java程序占用cpu100%如何排查解决CPU占用100则表示当前有计算密集类型线程正在执行并处于无法退出/执行计算难度很大的状态(比如死循环和大量对象未死亡时的GC扫描阶段) 主要思路：查看GC是否正常，当有大量小对象存在，则GC会非常占用资源；查看占用资源的线程信息，根据线程执行信息定位代码问题 查看是不是频繁在full gc： 看一下苍穹Monitor，看一下堆内存增长趋势与GC持续时间很长、GC频率是不是很高 使用top方法，查看应用的内存占用量 查看堆内存是不是一直在full GC：tail –f gc.log | grep –i full 查看是不是陷入在死循环里了： 查看占用最高进程: top查看占用最高PID 查看该进程的可疑线程: top -Hp PID 查看当前进程中占用最高线程 打印程序线程并查询对应线程信息: jstack pid | grep -A num 16进制PID （使用printf %x pid这个命令转为16进制） 查看线程栈信息的调用路径以及当前所处方法，查看代码逻辑 一般是在java.lang.Thread.State: RUNNABLE下面可以看到是哪一个类在运行，并且能看到哪一行代码 从类加载器到常说的五个再到解释器和编译器和本地方法库及接口 Jmap指令可以查看内存中对象的信息 jmap -heap 可以查看JVM内存使用总体情况 jmap -histo |more 可以查看内存中每个class的实例数和占用的内存量jmap -histo:live |more 可以查看所有活跃的实例对象 jmap -dump:format=b,file=dump_tomcat.dat 将内存使用情况dump到文件中 使用MAT工具对dump文件进行分析看到主界面，Problem区域就是提示可能有问题的类的信息了，我们可以看到，有一个类的实例占了700M左右，占用了内存总量90%以上了，点击Details可以查看详情，点击See stacktrace可以查看跟踪堆栈找到问题代码所在位置 为什么要有gc root？new一个对象的生命周期 答：讲了jmm，答了很久 老年代除了空间不够，还有什么时候会触发fgc 答：System.gc()也会触发，生产环境一般加DisableExplicitGC禁用System.gc() 新生代的对象年龄达到15的时候，会晋升到老年代，为什么是15？ 答：应该是经验值，如果定的太小，可能对象到了老年代之后很容易死亡，如果太大，可能对象在新生代一直复制，增加开销 正确答案，应该是因为对象头的markword部分，存分代年龄的age字段的二进制长度是4，所以最大值是15 有什么方法来避免Full GC 方法区空间增大 老年代空间增大 新生代空间减小 禁止使用System.gc()方法（或者少使用） 使用标记-整理算法，尽量让连续空间保持最大 排查代码中的无用大对象(内存泄漏) 可以用堆代替栈吗(区别) 功能：栈是描述该线程当前执行中方法的内存模型，堆存放程序运行时创建的对象 生命周期：堆生命周期是和程序一致，线程共享。栈生命周期和执行线程一致，且只用于当前线程，所以内存大小比堆小得多 内存地址：堆内存地址不一定连续，并且内存回收由GC回收非实时回收。栈内存地址要求连续，统一分配好。 栈内存放入堆中由于大部分线程执行周期短会造成大量垃圾对象或者长期对象也会有内存泄露的问题，一直跟随垃圾回收进行迁移 所以堆和栈的内存管理方式不一样，用堆代替栈会增加堆内存回收的复杂性，分开更容易管理各自内存 两者内存作用不同，栈中溢出会提示栈溢出错误，堆中溢出会提示内存溢出错误，如果放在一起则无法明确定位错误 分开的意义在于两个内存释放时机要求不同导致的内存管理方式不一样,如果放到一起会增加现有垃圾回收复杂度.并且堆栈内存的存取速率以及隔离性要求不一致,使用堆来存放也会增加堆内存管理复杂度以及降低线程执行效率 JVM的堆并不是数据结构意义上的堆（一种有序的树），而是动态内存分配意义上的堆——用于管理动态生命周期的内存区域。 JVM为啥要有垃圾回收器JAVA语言未提供手动释放对象资源的方式,内存空间释放由JVM虚拟机自己判断与执行,所以提供垃圾回收器帮我们进行内存资源释放.这样也避免了我们申请完内存后如果忘了释放内存而造成的内存泄漏. java为什么采用解释器和编译器并行的架构。垃圾回收器 CMS和G1的过程 初始标记(Initial Marking)：这阶段仅仅只是标记GC Roots能直接关联到的对象并修改TAMS(Next Top at Mark Start)的值，让下一阶段用户程序并发运行时，能在正确的可用的Region中创建新对象，这阶段需要停顿线程，但是耗时很短。而且是借用进行Minor GC的时候同步完成的，所以G1收集器在这个阶段实际并没有额外的停顿。 **并发标记(Concurrent Marking)**：从GC Roots开始对堆的对象进行可达性分析，递归扫描整个堆里的对象图，找出存活的对象，这阶段耗时较长，但是可以与用户程序并发执行。当对象图扫描完成以后，还要重新处理SATB记录下的在并发时有引用变动的对象。 **最终标记(Final Marking)**：对用户线程做另一个短暂的暂停，用于处理并发阶段结束后仍遗留下来的最后那少量的 SATB 记录。 在处理 SATB 记录的数据的时候，由于用户线程可能还是在继续修改对象图，继续在产生 SATB 数据，所以还是会有一小部分的 SATB 数据，所以才需要一个短暂的暂停。 **筛选回收(Live Data Counting and Evacuation)**：负责更新 Region 的统计数据，对各个 Region 的回收价值和成本进行排序，根据用户所期望的停顿时间来制定回收计划。可以自由选择任意多个 Region 构成回收集，然后把决定回收的那一部分 Region 的存活对象复制到空的 Region 中，再清理掉整个旧 Region 的全部空间。这里的操作涉及存活对象的移动，是必须暂停用户线程，由多条收集器线程并行完成的。 增量和原始快照算法G1要解决的问题以及具体是如何解决的 java程序堆的内存越来越大，从而导致程序中可存活的活对象越来越多，因此GC的STW时间越来越长。这是G1要解决的主要问题STW带来的停顿时间太长了。 CMS在GC后，没有整理内存。 G1的另一个显著特点他能够让用户设置应用的暂停时间，为什么G1能做到这一点呢？G1回收的第4步，它是“选择一些内存块”，而不是整代内存来回收，这是G1跟其它GC非常不同的一点，其它GC每次回收都会回收整个Generation的内存(Eden, Old)，这样回收内存所需的时间就取决于内存的大小，以及实际垃圾的多少，所以垃圾回收时间是不可控的。因为G1的内存划分为成很多小块，G1每次并不会回收整代内存，到底回收多少内存就看用户配置的暂停时间，配置的时间短就少回收点，配置的时间长就多回收点，伸缩自如。 主要的类加载器 引导类加载器（bootstrap class loader）：它用来加载 Java 的核心库，是用原生代码来实现的，并不继承自 java.lang.ClassLoader。 扩展类加载器（extensions class loader）：它用来加载 Java 的扩展库。Java 虚拟机的实现会提供一个扩展库目录。该类加载器在此目录里面查找并加载 Java 类。 系统类加载器（system class loader）：它根据 Java 应用的类路径（CLASSPATH）来加载 Java 类。一般来说，Java 应用的类都是由它来完成加载的。可以通过 ClassLoader.getSystemClassLoader()来获取它。 类加载的过程 加载。将外部的Class文件加载到Java虚拟机并且存储到方法区中 数组类通过Java虚拟机直接创建，不通过类加载器创建。 验证。确保加载进来的Class文件包含的信息符合Java虚拟机的要求 准备。为类变量分配内存，设置类变量的初始值。类变量就是静态变量 这些变量所使用的内存都在方法去中分配。实例变量不在该阶段分配内存。若类变量为final修饰，则直接赋予开发者定义的值 解析。将常量池中的符号引用转为直接引用 符号引用就是一组符号来描述目标，可以是任何字面量；直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。 初始化。为类的静态变量赋予正确的初始值 类的构造器和构造函数是不同的： 不需要显式调用父类的构造器 子类的构造器执行之前，父类的构造器一定会被执行 因此虚拟机第一个执行的构造器是java.lang.Object 静态语句块只可别赋值，不可被访问 接口与类不同，执行子接口的构造器不需要执行父接口的构造器 如果多个线程同时去初始化一个类，那么只会有一个线程去执行这个类的&lt;clinit&gt;()方法，其他类都会堵塞等待 双亲委派​ 双亲委派的意思是如果一个类加载器需要加载类，那么首先它会把这个类请求委派给父类加载器去完成，每一层都是如此。一直递归到顶层，当父加载器无法完成这个请求时，子类才会尝试去加载。这里的双亲其实就指的是父类，没有mother。父类也不是我们平日所说的那种继承关系，比如类A中引用了类B，Java虚拟机将使用加载类A的类加载器去加载类B。 ​ 父亲委托机制的优点是能够提高软件系统的安全性。因为在此机制下，用户自定义的类加载器不可能加载应该由父加载器加载的可靠类，从而防止不可靠甚至恶意的代码代替由父加载器加载的可靠代码。例如，java.lang.Object类总是由根类加载器加载，其他任何用户自定义的类加载器都不可能加载含有恶意代码的java.lang.Object类。 什么时候需要自定义类加载器？ 主流的Java Web服务器，比如Tomcat，都实现了自定义的类加载器（一般都不止一个）。因为一个功能健全的Web服务器，要解决如下几个问题： 部署在同一个服务器上的两个Web应用程序所使用的Java类库可以实现相互隔离。这是最基本的要求，两个不同的应用程序可能会依赖同一个第三方类库的不同版本，不能要求一个类库在一个服务器中只有一份，服务器应当保证两个应用程序的类库可以互相使用 部署在同一个服务器上的两个Web应用程序所使用的Java类库可以相互共享。这个需求也很常见，比如相同的Spring类库10个应用程序在用不可能分别存放在各个应用程序的隔离目录中 支持热替换，我们知道JSP文件最终要编译成.class文件才能由虚拟机执行，但JSP文件由于其纯文本存储特性，运行时修改的概率远远大于第三方类库或自身.class文件，而且JSP这种网页应用也把修改后无须重启作为一个很大的优势看待 由于存在上述问题，因此Java提供给用户使用的ClassLoader就无法满足需求了。Tomcat服务器就有自己的ClassLoader架构， 智力题7天 金条 每次掰为两半，掰两次，然后实现工资日结 两个人，n个小球，自己先手拿球，一次最多拿三个，至少拿一个，怎么保证自己一定赢（拿到最后一个球为赢家） 分布式事务CAP理论 C-一致性：外部访问任意一个节点，获取的数据都应该是一样的；（额外需要说明是，CAP的中C区别于数据库事务ACID中C，指代的是线性一致性，数据库事务中的C适用于理解为逻辑一致性） 对某个指定的客户端来说，读操作能返回最新的写操作。对于数据分布在不同节点上的数据上来说，如果在某个节点更新了数据，那么在其他节点如果都能读取到这个最新的数据，那么就称为强一致，如果有某个节点没有读取到，那就是分布式不一致。 A-可用性：挂掉任何一个节点，系统还是可以正常的响应所有外部的访问，不要出现大面积的失败和超时； 非故障的节点在合理的时间内返回合理的响应(不是错误和超时的响应)。可用性的两个关键一个是合理的时间，一个是合理的响应。合理的时间指的是请求不能无限被阻塞，应该在合理的时间给出返回。合理的响应指的是系统应该明确返回结果并且结果是正确的，这里的正确指的是比如应该返回50，而不是返回40。 P-分区容错性：节点之间的网络通信断了，整个系统还是可以继续提供服务，而不是直接崩溃。 当出现网络分区后，系统能够继续工作。打个比方，这里个集群有多台机器，有台机器网络出现了问题，但是这个集群仍然可以正常工作。 取舍策略 CAP三个特性只能满足其中两个，那么取舍的策略就共有三种： CA without P：如果不要求P（不允许分区），则C（强一致性）和A（可用性）是可以保证的。但放弃P的同时也就意味着放弃了系统的扩展性，也就是分布式节点受限，没办法部署子节点，其实就是网络出现大量的延迟，系统就不能用了。传统的关系型数据库：**Oracle、MySQL就是CA**。 CP without A：如果不要求A（可用，也就是可能会返回错误），相当于每个请求都需要在服务器之间保持强一致，而P（分区）会导致同步时间无限延长(也就是等待数据同步完才能正常访问服务)，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统(返回失败)。设计成CP的系统其实不少，最典型的就是分布式数据库，如Redis、HBase等。 AP wihtout C：要高可用并允许分区，则需**放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。典型的应用就如某米的抢购手机场景**，可能前几秒你浏览商品的时候页面提示是有库存的，当你选择完商品准备下单的时候，系统提示你下单失败，商品已售完。这其实就是先在 A（可用性）方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，虽然多少会影响一些用户体验，但也不至于造成用户购物流程的严重阻塞。 多个数据库的事务了解过没？分布式事务管理分布式数据库，怎么保证转账一致性？分布式事务 XA协议 XA 规范 使用两阶段提交（2PC，Two-Phase Commit）来保证所有资源同时提交或回滚任何特定的事务。 第一阶段，事务协调器向事务参与者发送 prepare 请求，事务参与者收到请求后，如果可以提交事务，回复 yes，否则回复 no。 第二阶段，如果所有事务参与者都回复了 yes，事务协调者向所有事务参与者发送 commit 请求，否则发送 rollback 请求。 XA和TCC原理上都是两阶段提交的，但是他俩是有本质区别的。XA预提交时并不会提交本地事务，有全局锁，性能很差。TCC预提交时会提交本地事务，只是预留一些字段，在confirm和cancel第二阶段进行业务上确认或者回滚，它是以全局事务唯一标识xid来确保一致性的 TCC（Try-Confirm-Cancel） T-try：资源检查/锁定。库存锁定；余额冻结… C-commit：资源提交，解锁资源。执行库存扣减；执行余额扣减… C-cancel：回滚，解锁资源。库存解锁，余额解冻… 首先定义事务的边界，事务开始以及事务提交 然后服务调用链路依次执行 Try 逻辑(这个操作就是通过业务代码锁定某个资源，设置一个预备类的状态，冻结部分数据。比如余额表里面，设置一个冻结余额的字段) 如果前一个操作都正常的话，进行数据库的提交。 如果某个服务的 Try 逻辑有问题，TCC 的事务协调器感知到之后就会推进执行各个服务的 Cancel 逻辑(业务代码)，撤销之前执行的各种操作。 资源需要回滚。比如，整体事务失败，被扣减库存需要增加回去实现强一致性。主要主动实现doTry()、doConfirm()、doCancel()方法。并且要保障幂等性！ 可能会出现，提交过程中服务器因为网络堵塞等问题宕机，这样框架会有三种处理操作按设定的重复次数重试、按设定的重试时间重试、还不行的话转为人工，报出日志。让运维人员检查服务器出现什么问题。 MQ事务 基本流程如下: 第一阶段Prepared消息，会拿到消息的地址。 第二阶段执行本地事务。 第三阶段通过第一阶段拿到的地址去访问消息，并修改状态。消息接受者就能使用这个消息。 如果确认消息失败，在RocketMq Broker中提供了定时扫描没有更新状态的消息，如果有消息没有得到确认，会向消息发送者发送消息，来判断是否提交，在rocketmq中是以listener的形式给发送者，用来处理。 如果消费超时，则需要一直重试，消息接收端需要保证幂等。如果消息消费失败，这个就需要人工进行处理，因为这个概率较低，如果为了这种小概率时间而设计这个复杂的流程反而得不偿失 Saga事务 Saga是30年前一篇数据库伦理提到的一个概念。其核心思想是将长事务拆分为多个本地短事务，由Saga事务协调器协调，如果正常结束那就正常完成，如果某个步骤失败，则根据相反顺序一次调用补偿操作(不会进行回滚，只是保证数据最终一致即可)。 Saga的组成： 每个Saga由一系列sub-transaction Ti 组成 每个Ti 都有对应的补偿动作Ci，补偿动作用于撤销Ti造成的结果，这里的每个T，都是一个本地事务。 可以看到，和TCC相比，Saga没有“预留 try”动作，它的Ti就是直接提交到库。 还是拿100元买一瓶水的例子来说，这里定义 T1=扣100元 T2=给用户加一瓶水 T3=减库存一瓶水 C1=加100元 C2=给用户减一瓶水 C3=给库存加一瓶水 我们一次进行T1,T2T3如果发生问题，就执行发生问题的C操作的反向。 隔离性的问题会出现在，如果执行到T3这个时候需要执行回滚，但是这个用户已经把水喝了(另外一个事务)，回滚的时候就会发现，无法给用户减一瓶水了。这就是事务之间没有隔离性的问题 可以看见saga模式没有隔离性的影响还是较大，可以参照华为的解决方案:从业务层面入手加入一 Session 以及锁的机制来保证能够串行化操作资源。也可以在业务层面通过预先冻结资金的方式隔离这部分资源， 最后在业务操作的过程中可以通过及时读取当前状态的方式获取到最新的更新。 分布式事务的控制https://www.cnblogs.com/jajian/p/10014145.html 分库分表其它 Nginx语法和配置 Nginx反代(如何配置) Nginx涉及到网络模型哪几层 应用层、表示层、会话层、传输层 nginx的upstream是干什么的 nginx有哪些负载均衡策略 dubbo怎么实现熔断和降级？ 项目怎么压测和部署的， MySQL抗压能力(单纯数据库多少QPS) 压测: Jemeter使用及结果报告 JVM中的数组对象 Maven 使用过吧，讲一下 Maven 与远程仓库的登录校验要在哪里完成 在repositories元素下，可以使用repository子元素声明一个或者多个远程仓库。上例中声明了一个id为jboss，名称为JBoss Repository的仓库。任何一个仓库声明的id必须是唯一的，尤其需要注意的是，Maven自带的中央仓库使用的id是central，如果其他的仓库声明也使用该id，就会覆盖中央仓库的配置。配置中的url指向了仓库的地址，一般来说，该地址都基于http协议，用户可以在浏览器中打开仓库地址浏览构件。 为什么用MyBatis（从与JDBC、Hibernate的区别聊） 而Hibernate把数据库和你隔离了，如果使用Hibernate，虽然能对生成的查询进行一定程度的定制，但开发起来付出的代价更大。 Hibernate在你项目开始时候省事，但是会导致业务逻辑模型和数据库模型互相依赖的程度太高，一旦项目变迁，随便改一处数据库的schema，整个java项目可能要改几十处！而MyBatis的自动mapping做的也不差，开发起来也没多花多少时间。等项目进入中后期，你需要大量定制和优化查询的时候，MyBatis的开发效率就胜出了。= 很多地方Hibernate可以强大到只用一行代码解决很多问题，但比如说一个update()或者save()到底做了什么，这里既有Hibernate本身的逻辑，也有你应用的逻辑，如果这一行产生了问题，你该如何去做？而MyBatis的开发和调优更加方便简单。尤其是需要处理大量数据或者大并发情况的网站服务，这也阿里选择MyBatis的原因。 ArrayList扩容机制，为什么是1.5 因为1.5 可以充分利用移位操作，减少浮点数或者运算时间和运算次数。int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); 什么是restfull风格，那几个方法 spring bean的生命周期 接口设计的原则 单一职责原则 接口的职责应该尽量单一。 里氏替换原则 所有引用基类的地方必须能透明地使用其子类的对象 里氏替换原则为良好的继承定义了一个规范： 1.子类必须完全实现父类的方法 2.子类可以有自己的个性（属性和方法）。 3.覆盖或实现父类的方法时输入参数可以被放大。 4.覆写或实现父类的方法时输出结果可以被缩小。 依赖倒置原则 包含三层含义： 1.高层模块不应该依赖低层模块，两者都应该依赖其抽象。 2.抽象不应该依赖细节。 3.细节应该依赖抽象。 精简的定义： 面向接口编程。 接口隔离原则：接口尽量细化，同时接口中的方法尽量少。 如何细化？细化到什么程序？没有统一的标准，应根据业务合理细分，适合业务才是重点。 迪米特法则：又称最少知识原则（Least Knowledge Principle, LKP）。通俗来讲：一个类应该对自己需要耦合或调用的类知道得最少，你（被耦合或调用的类）的内部是如何复杂都和我没有关系，那是你的事情，我就调用你提供的public方法，其他一概不关心。 开闭原则 一个软件实体如类、模块和函数应该对扩展开放，对修改关闭） 设计一个系统，每天有 100 亿条数据，需要在后台做实时展示和查找。 nginx 负载均衡，消息队列存储，多线程读取，批量插入，数据库 分库分表。 面试官根据我的回答又衍生出了很多问题，如消息队列存满了怎么办？（也就是消费跟不上 生产）批量插入时某一条失败了有什么影响？怎么解决？分库分表应该怎么分？怎么解决数 据迁移的问题？ 10 亿数据找出只出现一次的数字（回答布隆过滤器，bitMap，其实还可以Hash分组） 10亿数据找出最小的一百万个（回答维护一个堆用来存最小的一百万个数据） 和restful的http有什么区别，什么场景下使用，socket和他们有什么关系，是那部分的封装。 数据库万级变成亿级，怎么处理。分库分表，分片规则hash和取余数。使用mycat中间件实现。 现在有一大堆用户的订单数据存放在好几个数据库中，这些数据完全是混乱的，如何统计每一个用户的订单数？并找出订单数TopN的用户？用哈希表将同一个用户的订单划分到一个数据库中，之后用堆将每个用户的订单数进行排序。 子类的修饰符要大于父类：里氏替换原则(Liskov Substitution Principle, LSP)：所有引用基类（父类）的地方必须能透明地使用其子类的对象。 #和$的区别 #将传入的数据都当成一个字符串，会对自动传入的数据加一个双引号。如：where username=#&#123;username&#125;，如果传入的值是111，那么解析成sql时的值为where username=&quot;111&quot;，如果传入的值是id，则解析成的sql为where username=&quot;id&quot; $将传入的数据直接显示生成在sql中。如：where username=$&#123;username&#125;，如果传入的值是111，那么解析成sql时的值为where username=111如果传入的值是：drop table user;，则解析成的sql为：select id, username, password, role from user where username=; drop table user; #方式能够很大程度防止sql注入，$方式无法防止Sql注入。 $方式一般用于传入数据库对象，例如传入表名 一般能用#的就别用$，若不得不使用“${xxx}”这样的参数，要手工地做好过滤工作，来防止sql注入攻击。 在MyBatis中，“${xxx}”这样格式的参数会直接参与SQL编译，从而不能避免注入攻击。但涉及到动态表名和列名时，只能使用“${xxx}”这样的参数格式。所以，这样的参数需要我们在代码中手工进行处理来防止注入。 所以，在编写MyBatis的映射语句时，尽量采用“#{xxx}”这样的格式。若不得不使用“${xxx}”这样的参数，要手工地做好过滤工作，来防止SQL注入攻击。 微服务架构是什么？它的优缺点？ 传统的单体应用比较适合于小项目，如果应用在大型项目里，代码功能耦合在一起，代码维护难，另外，稳定性不高(一个微不足道的小问题，可以导致整个应用挂掉)，扩展性不高(无法满足高并发情况下的业务需求)。现在主流的设计一般会采用微服务架构。其思路不是开发一个巨大的单体式应用，而是将应用分解为小的、互相连接的微服务。一个微服务完成某个特定功能。每个微服务都有自己的业务逻辑。微服务提供API接口给其他微服务和应用客户端使用。 优点。首先，它将单体应用分解为一组服务。虽然功能总量不变，但应用程序已被分解为可管理的模块或服务。这些服务定义了明确的RPC或消息驱动的API边界。微服务架构强化了应用模块化的水平，而这通过单体代码库很难实现。因此，微服务开发的速度要快很多，更容易理解和维护。 其次，这种体系结构使得每个服务都可以由专注于此服务的团队独立开发。只要符合服务API契约，开发人员可以自由选择开发技术。这就意味着开发人员可以采用新技术编写或重构服务，由于服务相对较小，所以这并不会对整体应用造成太大影响。 第三，微服务架构可以使每个微服务独立部署。开发人员无需协调对服务升级或更改的部署。这些更改可以在测试通过后立即部署。所以微服务架构也使得CI／CD成为可能。 缺点。服务间调用、服务发现、服务容错、服务部署、数据调用 总线嗅探机制 每个处理器会通过嗅探器来监控总线上的数据来检查自己缓存内的数据是否过期，如果发现自己缓存行对应的地址被修改了，就会将此缓存行置为无效。当处理器对此数据进行操作时，就会重新从主内存中读取数据到缓存行。 如果在短时间内产生大量的cas操作在加上 volatile的嗅探机制则会不断地占用总线带宽，导致总线流量激增，就会产生总线风暴。 总之，就是因为volatile 和CAS 的操作导致BUS总线缓存一致性流量激增所造成的影响。 Linux管道用什么来实现的 一个管道实际上就是个只存在于内存中的文件，对这个文件的操作要通过两个已经打开文件进行，它们分别代表管道的两端。管道是一种特殊的文件，它不属于某一种文件系统，而是一种独立的文件系统，有其自己的数据结构。根据管道的适用范围将其分为：无名管道和命名管道。 高并发情况下，如何使用线程池，用哪个，问了一下线程结束要多久，是否在下一个线程结束前完成（我想的是cachethreadpool，其实思路错了）。 表示并发量比较大，所以我说可以考虑并发量是否大于队列长度加上最大线程数量和，如果不超过的话可以是用fixthreadpool。问题4 应该是想问在IO密集或者cpu密集下设置参数吧。IO密集，核心线程数高2N cpu密集是使用N+1 排序算法和适用场景 快排：nlogn 坏n2一般情况时排序速度最块，但是不稳定，当有序时，反而不好； 归并排序：nlogn稳定，适合大规模的排序，Array.sort()使用的就是归并排序； 堆排序：nlogn适合数据量很大的情况，例如：大数据处理的一个例子：找出一千万个数中最小的前一百个数； log4j的日志级别 ALL 最低等级的，用于打开所有日志记录。 TRACE designates finer-grained informational events than the DEBUG.Since:1.2.12，很低的日志级别，一般不会使用。 DEBUG 指出细粒度信息事件对调试应用程序是非常有帮助的，主要用于开发过程中打印一些运行信息。 INFO 消息在粗粒度级别上突出强调应用程序的运行过程。打印一些你感兴趣的或者重要的信息，这个可以用于生产环境中输出程序运行的一些重要信息，但是不能滥用，避免打印过多的日志。 WARN 表明会出现潜在错误的情形，有些信息不是错误信息，但是也要给程序员的一些提示。 ERROR 指出虽然发生错误事件，但仍然不影响系统的继续运行。打印错误和异常信息，如果不想输出太多的日志，可以使用这个级别。 FATAL 指出每个严重的错误事件将会导致应用程序的退出。这个级别比较高了。重大错误，这种级别你可以直接停止程序了。 OFF 最高等级的，用于关闭所有日志记录。 二叉树的查找效率？B+ 树和 B 树的区别点？二叉搜索树和红黑树的效率 logN 设计模式了解过哪些？简单工厂模式和工厂方法模式有什么区别？你刚才提到了开闭原则，还有哪些设计原则？ MyBatis框架（dao接口没有实现类、xml解析过程、延迟加载） JDBC连接数据库执行sql过程 广度优先遍历。广度优先遍历，指的是从图的一个未遍历的节点出发，先遍历这个节点的相邻节点，再依遍次历每个相邻节点的相邻节点。 深度优先遍历主要思路是从图中一个未访问的顶点 V 开始，沿着一条路一直走到底，然后从这条路尽头的节点回退到上一个节点，再从另一条路开始走到底…，不断递归重复此过程，直到所有的顶点都遍历完成。先走完一条路，再换一条路继续走。 git操作。切换回某一个版本并新建一个分支：git checkout commitID, git checkout -b branchName 互斥量的实现原理 后端如何处理重复无效的恶意请求（说了IP限制；或者有登录情况下可以使用SessionID进行限制） 文件系统有没有了解 计算机从快到慢的结构是哪些 set 里面只有一个元素，怎么取出来（这里重点说一下，因为平时都用list 和map，set 的一下子忘记了，当时脑抽没有用迭代器.next()的方式取出，而是用.get(0)， 用id做主键和用手机号做主键，怎么选择 水平分表依据什么分？时间还是数据 分库分表之后我想查询近期的订单，怎么查 限流的算法有哪些？ 我现在要做一个限流功能,怎么做？ 这个限流要做成分布式的,怎么做？ 怎么抢锁？锁怎么释放？ 加了超时之后有没有可能在没有释放的情况下,被人抢走锁？怎么解决？ 不用zk的心跳，可以怎么解决这个问题呢？ 假如这个限流希望做成可配置的,需要有一个后台管理系统随意对某个api配置全局流量,怎么做？ 对CAS的理解，java中的CAS，如何不用unsafe实现CAS SpringBoot的自动配置是怎么实现的 微信朋友圈该如何设计；如果换做是微博呢？闲鱼 扫码登录原理（没回答好） 前端发起请求，请求如何到达后端（具体问法忘记了） 设计一个短路径的网址 设计一个朋友圈，读QPS 1000w，写QPS 10w https://developer.aliyun.com/article/706808 考虑一个业务场景:头条的文章的评论量非常大,比如说一篇热门文章就有几百万的评论,设计一个后端服务,实现评论的时序展示与分页 尝试用 java 编写一个转账服务，传入交易流水号、转出账号，转入账号，转账金额，完成转出和转入账号的资金处理，该服务要确保在资金处理时转出账户的余额不会透支，金额计算准确，能够支撑每天 10 万笔的个人用户之间转账， 描述思路即可 用户有普通用户和vip用户，怎么高效的处理vip用户过期的那一瞬间使用vip的功能失败呢？ 用户进行权限验证时 Shiro会去缓存中找，如果查不到数据，会执行doGetAuthorizationInfo这个方法去查权限,并放入缓存中， 因此我们在前端页面分配用户权限时，执行清除shiro缓存的方法即可实现动态分配用户权限 SpringBoot 整合Shiro实现动态权限加载更新+Session共享+单点登录： shiro动态加载权限处理方法，shiro的几个方法 loadFilterChainDefinitionMap：初始化权限ex: 在上面Shiro配置类ShiroConfig中的Shiro基础配置shiroFilterFactory方法中我们就需要调用此方法将数据库中配置的所有uri权限全部加载进去，以及放行接口和配置权限过滤器等【注：过滤器配置顺序不能颠倒，多个过滤器用,分割】ex: filterChainDefinitionMap.put(“/api/system/user/list”, “authc,token,zqPerms[user1]”) updatePermission：动态刷新加载数据库中的uri权限 -&gt; 页面在新增uri路径到数据库中，也就是配置新的权限时就可以调用此方法实现动态加载uri权限 updatePermissionByRoleId：shiro动态权限加载 -&gt; 即分配指定用户权限时可调用此方法删除shiro缓存，重新执行doGetAuthorizationInfo方法授权角色和权限 设计一个排行榜 考察的就是对于zset的理解与使用 添加 member 命令格式：zadd key score member [score member …] zadd sport:ranking:20210227 10158 mx 30169 les 48858 skr 66079 jay 增加 member score 命令格式：zincrby key increment member zincrby sport:ranking:20210227 5000 why 获取 member 排名命令格式：zrank/zrevrank key member zrank 是按照分数从低到高返回 member 排名。 zrevrank 是按照分数从高到低返回 member 排名。 返回指定排名范围内的 member 命令格式：zrange/zrevrange key start end [withscores] zrange 是按照 score 从低到高返回指定排名范围内的 member。 zrevrange 是按照 score 从高到低返回指定排名范围内的 member。 当用户请求查看排行榜的时候，再去根据用户的好友关系，循环获取好友的步数，生成排行榜。","categories":[{"name":"总结","slug":"总结","permalink":"http://example.com/categories/%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"name":"总结","slug":"总结","permalink":"http://example.com/tags/%E6%80%BB%E7%BB%93/"}]},{"title":"Hello World","slug":"2019-11-21-Hello-World","date":"2019-11-21T09:16:54.000Z","updated":"2019-11-21T15:28:17.000Z","comments":true,"path":"2019/11/21/2019-11-21-Hello-World/","link":"","permalink":"http://example.com/2019/11/21/2019-11-21-Hello-World/","excerpt":"写一写本博客主要的内容有什么～","text":"写一写本博客主要的内容有什么～ 前言这个文章就是为了留个纪念然后一些闲聊~ 写博客的初衷我写这个博客就是想把自己折腾过的东西都记一记，然后顺便把经验和其他东西分享给各位！ 我会往博客放的内容有 Java语言的各种心得、总结 搭建各种东西 如: 这个博客、各种开发环境等等 采坑经历分享 各种有趣的东西最后…希望大家多多评论，如有不足的地方各位可以提意见哦-v-","categories":[{"name":"杂文","slug":"杂文","permalink":"http://example.com/categories/%E6%9D%82%E6%96%87/"}],"tags":[{"name":"闲聊","slug":"闲聊","permalink":"http://example.com/tags/%E9%97%B2%E8%81%8A/"}]}],"categories":[{"name":"总结","slug":"总结","permalink":"http://example.com/categories/%E6%80%BB%E7%BB%93/"},{"name":"杂文","slug":"杂文","permalink":"http://example.com/categories/%E6%9D%82%E6%96%87/"}],"tags":[{"name":"面试题","slug":"面试题","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"name":"总结","slug":"总结","permalink":"http://example.com/tags/%E6%80%BB%E7%BB%93/"},{"name":"闲聊","slug":"闲聊","permalink":"http://example.com/tags/%E9%97%B2%E8%81%8A/"}]}